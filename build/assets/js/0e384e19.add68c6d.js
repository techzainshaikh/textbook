"use strict";(globalThis.webpackChunkphysical_ai_book_humanoid=globalThis.webpackChunkphysical_ai_book_humanoid||[]).push([[3976],{2053(e,n,i){i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>r,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"intro","title":"Introduction","description":"Introduction to Physical AI and Humanoid Robotics","source":"@site/docs/intro.md","sourceDirName":".","slug":"/intro","permalink":"/textbook/docs/intro","draft":false,"unlisted":false,"editUrl":"https://github.com/Panaversity/physical_ai_book_humanoid/tree/main/docs/intro.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Introduction","description":"Introduction to Physical AI and Humanoid Robotics","keywords":["physical ai","humanoid robotics","robotics","ai","textbook"]},"sidebar":"tutorialSidebar","next":{"title":"Glossary","permalink":"/textbook/docs/glossary"}}');var o=i(4848),s=i(8453);const a={sidebar_position:1,title:"Introduction",description:"Introduction to Physical AI and Humanoid Robotics",keywords:["physical ai","humanoid robotics","robotics","ai","textbook"]},r="Introduction to Physical AI and Humanoid Robotics",d={},l=[{value:"About This Textbook",id:"about-this-textbook",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Textbook Structure",id:"textbook-structure",level:2},{value:"Academic Rigor",id:"academic-rigor",level:2},{value:"Getting Started",id:"getting-started",level:2},{value:"Navigate the Textbook",id:"navigate-the-textbook",level:2}];function c(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"introduction-to-physical-ai-and-humanoid-robotics",children:"Introduction to Physical AI and Humanoid Robotics"})}),"\n",(0,o.jsx)(n.p,{children:"Welcome to the Physical AI and Humanoid Robotics textbook. This comprehensive resource bridges AI software intelligence with physical robotic embodiment, enabling students to design, simulate, and deploy humanoid robots using ROS 2, Gazebo/Unity, NVIDIA Isaac, and Vision-Language-Action (VLA) systems."}),"\n",(0,o.jsx)(n.h2,{id:"about-this-textbook",children:"About This Textbook"}),"\n",(0,o.jsx)(n.p,{children:"This textbook is designed for:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Senior CS / Software Engineering students"}),"\n",(0,o.jsx)(n.li,{children:"Robotics and AI practitioners"}),"\n",(0,o.jsx)(n.li,{children:"Panaversity Physical AI learners"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Prerequisite knowledge: Python, basic AI/ML, linear algebra, and operating systems fundamentals."}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(n.p,{children:"By the end of this textbook, students will be able to:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Understand and implement ROS 2 fundamentals for robotic systems"}),"\n",(0,o.jsx)(n.li,{children:"Create digital twins using Gazebo and Unity simulations"}),"\n",(0,o.jsx)(n.li,{children:"Develop AI-powered robot brains using NVIDIA Isaac platform"}),"\n",(0,o.jsx)(n.li,{children:"Implement Vision-Language-Action systems for cognitive robotics"}),"\n",(0,o.jsx)(n.li,{children:"Build end-to-end humanoid robot systems that respond to natural language commands"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"textbook-structure",children:"Textbook Structure"}),"\n",(0,o.jsx)(n.p,{children:"This textbook is organized into four main modules:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"The Robotic Nervous System (ROS 2)"})," - Nodes, Topics, Services, Actions, rclpy-based Python agents, URDF humanoid modeling, and ROS 2 control architecture"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"The Digital Twin (Gazebo & Unity)"})," - Physics simulation, sensor simulation, environment modeling, and Unity-based human-robot interaction visualization"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"The AI-Robot Brain (NVIDIA Isaac\u2122)"})," - Isaac Sim photorealistic simulation, synthetic data generation, Isaac ROS perception pipelines, Nav2-based humanoid path planning, reinforcement learning for control, and sim-to-real transfer"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Vision-Language-Action (VLA)"})," - Voice-to-action pipelines, LLM-based task decomposition, ROS 2 action planning, and multimodal perception"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"academic-rigor",children:"Academic Rigor"}),"\n",(0,o.jsx)(n.p,{children:"All mathematical equations include formal derivations or proper citations in APA format. All code examples are complete, executable, and include comments explaining both WHAT the code does and WHY it's implemented that way, with version-pinned dependencies."}),"\n",(0,o.jsx)(n.h2,{id:"getting-started",children:"Getting Started"}),"\n",(0,o.jsx)(n.p,{children:"Begin with Module 1: The Robotic Nervous System (ROS 2) to establish the foundational concepts necessary for understanding the more advanced topics covered in subsequent modules."}),"\n",(0,o.jsx)(n.h2,{id:"navigate-the-textbook",children:"Navigate the Textbook"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"./module-1-ros2/intro",children:"Module 1: The Robotic Nervous System (ROS 2)"})," - Foundation concepts, nodes, topics, services, and agents"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"./module-2-digital-twin/intro",children:"Module 2: The Digital Twin (Gazebo & Unity)"})," - Physics simulation, sensor simulation, environment modeling, and Unity-based human-robot interaction visualization"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"./module-3-ai-brain/intro",children:"Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)"})," - Isaac Sim, synthetic data, perception pipelines, Nav2 navigation, reinforcement learning, and sim-to-real transfer"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"./module-4-vla/intro",children:"Module 4: Vision-Language-Action (VLA)"})," - Speech recognition, LLM-based planning, ROS 2 actions, multimodal perception, and end-to-end VLA systems"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},8453(e,n,i){i.d(n,{R:()=>a,x:()=>r});var t=i(6540);const o={},s=t.createContext(o);function a(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);