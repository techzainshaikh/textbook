"use strict";(globalThis.webpackChunkphysical_ai_book_humanoid=globalThis.webpackChunkphysical_ai_book_humanoid||[]).push([[6200],{1189(n,e,t){t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>r,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"module-3-ai-brain/chapter-2-synthetic-data","title":"Synthetic Data Generation for AI Training","description":"Creating synthetic datasets using Isaac Sim for AI model training","source":"@site/docs/module-3-ai-brain/chapter-2-synthetic-data.md","sourceDirName":"module-3-ai-brain","slug":"/module-3-ai-brain/chapter-2-synthetic-data","permalink":"/textbook/docs/module-3-ai-brain/chapter-2-synthetic-data","draft":false,"unlisted":false,"editUrl":"https://github.com/Panaversity/physical_ai_book_humanoid/tree/main/docs/module-3-ai-brain/chapter-2-synthetic-data.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Synthetic Data Generation for AI Training","sidebar_position":3,"description":"Creating synthetic datasets using Isaac Sim for AI model training","keywords":["synthetic data","isaac sim","data generation","ai training","computer vision"]},"sidebar":"tutorialSidebar","previous":{"title":"Isaac Platform and Ecosystem","permalink":"/textbook/docs/module-3-ai-brain/chapter-1-isaac-platform"},"next":{"title":"Perception Pipelines for Robotics AI","permalink":"/textbook/docs/module-3-ai-brain/chapter-3-perception-pipelines"}}');var i=t(4848),s=t(8453);const r={title:"Synthetic Data Generation for AI Training",sidebar_position:3,description:"Creating synthetic datasets using Isaac Sim for AI model training",keywords:["synthetic data","isaac sim","data generation","ai training","computer vision"]},o="Chapter 2: Synthetic Data Generation for AI Training",l={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Core Concepts",id:"core-concepts",level:2},{value:"Synthetic Data Benefits",id:"synthetic-data-benefits",level:3},{value:"Domain Randomization",id:"domain-randomization",level:3},{value:"Implementation",id:"implementation",level:2},{value:"Isaac Sim Synthetic Data Pipeline",id:"isaac-sim-synthetic-data-pipeline",level:3},{value:"Synthetic Dataset Validator",id:"synthetic-dataset-validator",level:3},{value:"Examples",id:"examples",level:2},{value:"Example 1: Domain Randomization Configuration",id:"example-1-domain-randomization-configuration",level:3},{value:"Example 2: Synthetic Dataset Pipeline Integration",id:"example-2-synthetic-dataset-pipeline-integration",level:3},{value:"Summary",id:"summary",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Conceptual",id:"conceptual",level:3},{value:"Logical",id:"logical",level:3},{value:"Implementation",id:"implementation-1",level:3}];function p(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"chapter-2-synthetic-data-generation-for-ai-training",children:"Chapter 2: Synthetic Data Generation for AI Training"})}),"\n",(0,i.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(e.p,{children:"By the end of this chapter, students will be able to:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Generate synthetic datasets using Isaac Sim for AI model training"}),"\n",(0,i.jsx)(e.li,{children:"Configure realistic sensor noise and environmental variations in simulation"}),"\n",(0,i.jsx)(e.li,{children:"Create diverse training scenarios with varying lighting, textures, and objects"}),"\n",(0,i.jsx)(e.li,{children:"Implement domain randomization techniques for robust AI models"}),"\n",(0,i.jsx)(e.li,{children:"Validate synthetic data quality against real-world datasets"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsx)(e.p,{children:"Students should have:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Understanding of computer vision and machine learning fundamentals"}),"\n",(0,i.jsx)(e.li,{children:"Experience with Isaac Sim (covered in Chapter 1)"}),"\n",(0,i.jsx)(e.li,{children:"Knowledge of dataset formats for training AI models"}),"\n",(0,i.jsx)(e.li,{children:"Basic understanding of domain adaptation concepts"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"core-concepts",children:"Core Concepts"}),"\n",(0,i.jsx)(e.p,{children:"Synthetic data generation is a critical component of modern robotics AI development, allowing for rapid dataset creation without the time and cost of real-world data collection. Isaac Sim provides powerful capabilities for generating diverse, labeled datasets that can be used to train perception and control systems."}),"\n",(0,i.jsx)(e.h3,{id:"synthetic-data-benefits",children:"Synthetic Data Benefits"}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Cost and Time Efficiency:"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Generate thousands of labeled images in minutes rather than days"}),"\n",(0,i.jsx)(e.li,{children:"Control environmental conditions (lighting, weather, objects)"}),"\n",(0,i.jsx)(e.li,{children:"Create rare scenarios that are difficult to capture in real life"}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Safety:"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Train in dangerous scenarios without risk to equipment or personnel"}),"\n",(0,i.jsx)(e.li,{children:"Test edge cases without real-world consequences"}),"\n",(0,i.jsx)(e.li,{children:"Generate failure scenarios for robustness testing"}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Label Quality:"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Perfect ground truth for segmentation, depth, poses"}),"\n",(0,i.jsx)(e.li,{children:"Consistent labeling across large datasets"}),"\n",(0,i.jsx)(e.li,{children:"Multiple modalities (RGB, depth, semantic segmentation) synchronized"}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"domain-randomization",children:"Domain Randomization"}),"\n",(0,i.jsx)(e.p,{children:"Domain randomization involves varying environmental parameters to create diverse training data:"}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Visual Properties:"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Lighting conditions (intensity, direction, color temperature)"}),"\n",(0,i.jsx)(e.li,{children:"Texture variations (materials, colors, patterns)"}),"\n",(0,i.jsx)(e.li,{children:"Camera properties (noise, blur, distortion)"}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Physical Properties:"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Object poses and configurations"}),"\n",(0,i.jsx)(e.li,{children:"Environmental layouts"}),"\n",(0,i.jsx)(e.li,{children:"Dynamics parameters (friction, mass, restitution)"}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Sensor Properties:"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Noise characteristics"}),"\n",(0,i.jsx)(e.li,{children:"Resolution variations"}),"\n",(0,i.jsx)(e.li,{children:"Distortion parameters"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"implementation",children:"Implementation"}),"\n",(0,i.jsx)(e.p,{children:"Let's implement synthetic data generation using Isaac Sim capabilities:"}),"\n",(0,i.jsx)(e.h3,{id:"isaac-sim-synthetic-data-pipeline",children:"Isaac Sim Synthetic Data Pipeline"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n# synthetic_data_generator.py\n\nimport omni\nimport carb\nimport numpy as np\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.synthetic_utils import SyntheticDataHelper\nfrom omni.replicator.core import Replicator\nimport omni.synthetic_utils as synth_utils\nimport PIL.Image\nimport json\nimport os\nfrom pathlib import Path\n\nclass IsaacSyntheticDataGenerator:\n    """\n    Class for generating synthetic datasets using Isaac Sim\n    """\n\n    def __init__(self, output_dir="synthetic_dataset"):\n        self.output_dir = Path(output_dir)\n        self.output_dir.mkdir(exist_ok=True)\n\n        # Initialize replicator for synthetic data generation\n        self.replicator = Replicator()\n\n        # Data counters\n        self.image_count = 0\n        self.annotation_count = 0\n\n        print(f"Synthetic data generator initialized. Output directory: {self.output_dir}")\n\n    def setup_replication_graph(self):\n        """Setup replication graph for data generation"""\n        # Create a basic scene with randomizable elements\n        # This would typically include:\n        # - Randomizable objects with domain randomization\n        # - Adjustable lighting\n        # - Camera positioning\n        # - Sensor noise models\n\n        # Example: Create randomizable objects\n        self.setup_randomizable_objects()\n\n        # Example: Configure lighting randomization\n        self.setup_lighting_randomization()\n\n        # Example: Configure camera properties\n        self.setup_camera_properties()\n\n    def setup_randomizable_objects(self):\n        """Setup objects that can be randomized"""\n        # In a real implementation, this would use Isaac Replicator\n        # to define randomizable properties for objects in the scene\n        pass\n\n    def setup_lighting_randomization(self):\n        """Setup lighting that can be randomized"""\n        # In a real implementation, this would define randomizable\n        # lighting parameters such as intensity, color, direction\n        pass\n\n    def setup_camera_properties(self):\n        """Setup camera with configurable properties"""\n        # In a real implementation, this would define camera\n        # properties that can be randomized during data generation\n        pass\n\n    def generate_training_data(self, num_samples=1000, data_types=["rgb", "depth", "seg"]):\n        """Generate synthetic training data"""\n        print(f"Generating {num_samples} synthetic samples with types: {data_types}")\n\n        for i in range(num_samples):\n            # Randomize scene parameters\n            self.randomize_scene()\n\n            # Capture data from scene\n            sample_data = self.capture_sample(data_types)\n\n            # Save sample data\n            self.save_sample(sample_data, i)\n\n            # Progress update\n            if (i + 1) % 100 == 0:\n                print(f"Generated {i + 1}/{num_samples} samples")\n\n        print(f"Completed generation of {num_samples} synthetic samples")\n\n    def randomize_scene(self):\n        """Randomize scene properties for domain randomization"""\n        # This would randomize:\n        # - Object positions and orientations\n        # - Lighting conditions\n        # - Material properties\n        # - Camera parameters\n        # - Environmental settings\n        pass\n\n    def capture_sample(self, data_types):\n        """Capture a single sample with specified data types"""\n        sample = {}\n\n        for data_type in data_types:\n            if data_type == "rgb":\n                # Capture RGB image\n                sample[data_type] = self.capture_rgb_image()\n            elif data_type == "depth":\n                # Capture depth image\n                sample[data_type] = self.capture_depth_image()\n            elif data_type == "seg":\n                # Capture segmentation mask\n                sample[data_type] = self.capture_segmentation()\n            elif data_type == "pose":\n                # Capture object poses\n                sample[data_type] = self.capture_poses()\n\n        return sample\n\n    def capture_rgb_image(self):\n        """Capture RGB image from virtual camera"""\n        # In Isaac Sim, this would use the rendering pipeline\n        # to capture RGB data from a virtual camera\n        width, height = 640, 480\n        rgb_data = np.random.randint(0, 255, (height, width, 3), dtype=np.uint8)\n        return rgb_data\n\n    def capture_depth_image(self):\n        """Capture depth image from virtual depth sensor"""\n        # In Isaac Sim, this would use the depth rendering pipeline\n        width, height = 640, 480\n        depth_data = np.random.uniform(0.1, 10.0, (height, width)).astype(np.float32)\n        return depth_data\n\n    def capture_segmentation(self):\n        """Capture semantic segmentation mask"""\n        # In Isaac Sim, this would use semantic segmentation rendering\n        width, height = 640, 480\n        seg_data = np.random.randint(0, 10, (height, width), dtype=np.uint8)\n        return seg_data\n\n    def capture_poses(self):\n        """Capture object poses in the scene"""\n        # In Isaac Sim, this would capture ground truth poses\n        # of objects in the scene\n        poses = {\n            "object_1": {"position": [1.0, 0.0, 0.5], "rotation": [0, 0, 0, 1]},\n            "object_2": {"position": [0.5, 1.0, 0.3], "rotation": [0, 0, 0, 1]}\n        }\n        return poses\n\n    def save_sample(self, sample_data, sample_idx):\n        """Save a single sample to disk"""\n        sample_dir = self.output_dir / f"sample_{sample_idx:06d}"\n        sample_dir.mkdir(exist_ok=True)\n\n        for data_type, data in sample_data.items():\n            if data_type == "rgb":\n                # Save RGB image\n                img = PIL.Image.fromarray(data)\n                img.save(sample_dir / "rgb.png")\n            elif data_type == "depth":\n                # Save depth image\n                depth_img = PIL.Image.fromarray((data * 256).astype(np.uint16))\n                depth_img.save(sample_dir / "depth.png")\n            elif data_type == "seg":\n                # Save segmentation mask\n                seg_img = PIL.Image.fromarray(data)\n                seg_img.save(sample_dir / "segmentation.png")\n            elif data_type == "pose":\n                # Save pose annotations\n                with open(sample_dir / "poses.json", \'w\') as f:\n                    json.dump(data, f)\n\n        # Create annotation file for this sample\n        annotation = {\n            "sample_id": f"sample_{sample_idx:06d}",\n            "data_types": list(sample_data.keys()),\n            "timestamp": carb.events.acquire_events_interface().get_current_event_time(),\n            "scene_parameters": self.get_current_scene_params()\n        }\n\n        with open(sample_dir / "annotation.json", \'w\') as f:\n            json.dump(annotation, f, indent=2)\n\n        self.image_count += 1\n\n    def get_current_scene_params(self):\n        """Get current scene randomization parameters"""\n        # Return current randomization settings for reproducibility\n        return {\n            "lighting_intensity": np.random.uniform(0.5, 1.5),\n            "lighting_color_temp": np.random.uniform(5000, 8000),\n            "camera_noise_level": np.random.uniform(0.0, 0.05),\n            "object_count": np.random.randint(1, 5)\n        }\n\n    def validate_synthetic_data(self, real_dataset_stats):\n        """Validate synthetic data quality against real dataset statistics"""\n        # Compare statistical properties of synthetic vs real data\n        # This could include:\n        # - Color distribution comparison\n        # - Texture complexity metrics\n        # - Object size distributions\n        # - Depth range distributions\n\n        print("Validating synthetic data quality...")\n\n        # Example validation: compare mean and std of RGB channels\n        synthetic_stats = self.compute_synthetic_stats()\n\n        # Compute similarity metrics\n        similarity_score = self.compute_similarity(real_dataset_stats, synthetic_stats)\n\n        print(f"Synthetic data quality score: {similarity_score:.3f}")\n\n        return similarity_score > 0.7  # Return True if quality is acceptable\n\n    def compute_synthetic_stats(self):\n        """Compute statistics for synthetic dataset"""\n        # This would compute various statistical measures\n        # across the generated synthetic dataset\n        return {\n            "color_mean": [120, 115, 110],\n            "color_std": [50, 45, 40],\n            "depth_mean": 3.5,\n            "depth_std": 1.2\n        }\n\n    def compute_similarity(self, real_stats, synthetic_stats):\n        """Compute similarity between real and synthetic datasets"""\n        # Simplified similarity computation\n        # In practice, this would use more sophisticated statistical tests\n        score = 0.0\n\n        # Compare color statistics\n        for channel in ["color_mean", "color_std"]:\n            real_vals = np.array(real_stats[channel])\n            synth_vals = np.array(synthetic_stats[channel])\n            diff = np.mean(np.abs(real_vals - synth_vals) / (real_vals + 1e-6))\n            score += (1.0 - min(diff, 1.0))\n\n        # Compare depth statistics\n        depth_diff = abs(real_stats["depth_mean"] - synthetic_stats["depth_mean"]) / real_stats["depth_mean"]\n        score += (1.0 - min(depth_diff, 1.0))\n\n        return score / 3.0  # Normalize to 0-1 range\n\ndef main():\n    """Main entry point for synthetic data generation"""\n    generator = IsaacSyntheticDataGenerator(output_dir="synthetic_robot_dataset")\n\n    # Setup the replication graph\n    generator.setup_replication_graph()\n\n    # Generate training data\n    generator.generate_training_data(\n        num_samples=100,\n        data_types=["rgb", "depth", "seg", "pose"]\n    )\n\n    print("Synthetic data generation completed!")\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,i.jsx)(e.h3,{id:"synthetic-dataset-validator",children:"Synthetic Dataset Validator"}),"\n",(0,i.jsx)(e.p,{children:"Let's create a validation tool to assess synthetic dataset quality:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n# synthetic_dataset_validator.py\n\nimport numpy as np\nimport cv2\nimport json\nimport os\nfrom pathlib import Path\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\nclass SyntheticDatasetValidator:\n    """\n    Validates synthetic datasets for quality and consistency\n    """\n\n    def __init__(self, dataset_path):\n        self.dataset_path = Path(dataset_path)\n        self.samples = []\n        self.stats = {}\n\n    def load_dataset(self):\n        """Load dataset samples"""\n        sample_dirs = [d for d in self.dataset_path.iterdir() if d.is_dir()]\n\n        for sample_dir in sample_dirs:\n            sample_data = {}\n\n            # Load RGB image\n            rgb_path = sample_dir / "rgb.png"\n            if rgb_path.exists():\n                sample_data["rgb"] = cv2.imread(str(rgb_path))\n\n            # Load depth image\n            depth_path = sample_dir / "depth.png"\n            if depth_path.exists():\n                sample_data["depth"] = cv2.imread(str(depth_path), cv2.IMREAD_UNCHANGED).astype(np.float32) / 256.0\n\n            # Load segmentation\n            seg_path = sample_dir / "segmentation.png"\n            if seg_path.exists():\n                sample_data["seg"] = cv2.imread(str(seg_path), cv2.IMREAD_UNCHANGED)\n\n            # Load annotations\n            annot_path = sample_dir / "annotation.json"\n            if annot_path.exists():\n                with open(annot_path, \'r\') as f:\n                    sample_data["annotation"] = json.load(f)\n\n            self.samples.append(sample_data)\n\n        print(f"Loaded {len(self.samples)} samples from {self.dataset_path}")\n\n    def validate_rgb_quality(self):\n        """Validate RGB image quality"""\n        rgb_means = []\n        rgb_stds = []\n\n        for sample in self.samples:\n            if "rgb" in sample:\n                img = sample["rgb"]\n                # Convert BGR to RGB for proper analysis\n                img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n                # Calculate mean and std for each channel\n                means = [np.mean(img_rgb[:,:,i]) for i in range(3)]\n                stds = [np.std(img_rgb[:,:,i]) for i in range(3)]\n\n                rgb_means.append(means)\n                rgb_stds.append(stds)\n\n        if rgb_means:\n            mean_rgb_means = np.mean(rgb_means, axis=0)\n            mean_rgb_stds = np.mean(rgb_stds, axis=0)\n\n            print(f"RGB Channel Means: {mean_rgb_means}")\n            print(f"RGB Channel Stds: {mean_rgb_stds}")\n\n            # Check for reasonable ranges\n            if np.any(mean_rgb_means < 20) or np.any(mean_rgb_means > 235):\n                print("WARNING: Some RGB channels have extreme mean values (possible lighting issues)")\n\n            if np.any(mean_rgb_stds < 20):\n                print("WARNING: Some RGB channels have low variance (possible lack of texture)")\n\n    def validate_depth_consistency(self):\n        """Validate depth image consistency"""\n        depth_values = []\n\n        for sample in self.samples:\n            if "depth" in sample:\n                depth_img = sample["depth"]\n                valid_depths = depth_img[depth_img > 0]  # Exclude invalid depths\n                depth_values.extend(valid_depths)\n\n        if depth_values:\n            depth_array = np.array(depth_values)\n\n            print(f"Depth Statistics:")\n            print(f"  Mean: {np.mean(depth_array):.2f}m")\n            print(f"  Std: {np.std(depth_array):.2f}m")\n            print(f"  Min: {np.min(depth_array):.2f}m")\n            print(f"  Max: {np.max(depth_array):.2f}m")\n            print(f"  Median: {np.median(depth_array):.2f}m")\n\n            # Check for reasonable depth ranges\n            if np.max(depth_array) > 50.0:\n                print("WARNING: Maximum depth value is unusually high")\n\n            if np.min(depth_array) < 0.01:\n                print("WARNING: Minimum depth value is unusually low (possible sensor clipping)")\n\n    def validate_annotations(self):\n        """Validate annotation quality and consistency"""\n        annotation_stats = []\n\n        for sample in self.samples:\n            if "annotation" in sample:\n                annot = sample["annotation"]\n                annotation_stats.append(annot)\n\n        if annotation_stats:\n            print(f"Validated {len(annotation_stats)} annotations")\n\n            # Check for consistent scene parameters\n            lighting_intensities = [a["scene_parameters"]["lighting_intensity"] for a in annotation_stats if "scene_parameters" in a]\n            if lighting_intensities:\n                print(f"Lighting intensity range: {min(lighting_intensities):.2f} - {max(lighting_intensities):.2f}")\n\n            object_counts = [a["scene_parameters"]["object_count"] for a in annotation_stats if "scene_parameters" in a]\n            if object_counts:\n                print(f"Object count range: {min(object_counts)} - {max(object_counts)} (avg: {np.mean(object_counts):.1f})")\n\n    def validate_segmentation(self):\n        """Validate segmentation masks"""\n        seg_class_histograms = []\n\n        for sample in self.samples:\n            if "seg" in sample:\n                seg_img = sample["seg"]\n                # Calculate histogram of segmentation classes\n                unique, counts = np.unique(seg_img, return_counts=True)\n                hist = dict(zip(unique, counts))\n                seg_class_histograms.append(hist)\n\n        if seg_class_histograms:\n            print(f"Segmentation validation for {len(seg_class_histograms)} samples")\n\n            # Analyze class distribution\n            all_classes = set()\n            for hist in seg_class_histograms:\n                all_classes.update(hist.keys())\n\n            print(f"Found {len(all_classes)} unique segmentation classes: {sorted(all_classes)}")\n\n            # Check for samples with too few distinct classes (possible rendering issues)\n            for i, hist in enumerate(seg_class_histograms):\n                if len(hist) < 2:\n                    print(f"WARNING: Sample {i} has only {len(hist)} segmentation class(es) - may be invalid")\n\n    def generate_validation_report(self):\n        """Generate comprehensive validation report"""\n        print("\\n=== SYNTHETIC DATASET VALIDATION REPORT ===")\n\n        # RGB quality validation\n        print("\\nRGB Quality Assessment:")\n        self.validate_rgb_quality()\n\n        # Depth consistency validation\n        print("\\nDepth Consistency Assessment:")\n        self.validate_depth_consistency()\n\n        # Annotation validation\n        print("\\nAnnotation Quality Assessment:")\n        self.validate_annotations()\n\n        # Segmentation validation\n        print("\\nSegmentation Quality Assessment:")\n        self.validate_segmentation()\n\n        print("\\n=========================================")\n\ndef main():\n    """Main validation function"""\n    validator = SyntheticDatasetValidator("synthetic_robot_dataset")\n    validator.load_dataset()\n    validator.generate_validation_report()\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,i.jsx)(e.h2,{id:"examples",children:"Examples"}),"\n",(0,i.jsx)(e.h3,{id:"example-1-domain-randomization-configuration",children:"Example 1: Domain Randomization Configuration"}),"\n",(0,i.jsx)(e.p,{children:"Here's an example of how to configure domain randomization in Isaac Sim:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n# domain_randomization_config.py\n\nimport omni.replicator.core as rep\nfrom omni.isaac.core.utils.prims import get_prim_at_path\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nimport numpy as np\n\n# Initialize replicator\nrep.random_seed(42)\n\n# Define randomization functions\n@rep.randomizer\ndef randomize_lighting():\n    """Randomize lighting properties"""\n    lights = rep.get.light()\n\n    with lights.randomize.light():\n        # Randomize intensity between 500 and 1500\n        rep.modify.attribute("inputs:intensity", rep.distribution.normal(1000, 200))\n\n        # Randomize color temperature between 4000K and 8000K\n        rep.modify.attribute("inputs:color", rep.distribution.uniform([0.8, 0.9, 1.0], [1.0, 0.9, 0.8]))\n\n@rep.randomizer\ndef randomize_materials():\n    """Randomize material properties"""\n    prims = rep.get.prims()\n\n    with prims.randomize.prim_type_regex(".*Material.*"):\n        # Randomize roughness\n        rep.modify.attribute("roughness", rep.distribution.uniform(0.1, 0.9))\n\n        # Randomize metallic\n        rep.modify.attribute("metallic", rep.distribution.uniform(0.0, 0.3))\n\n@rep.randomizer\ndef randomize_objects():\n    """Randomize object positions and properties"""\n    cubes = rep.get.cube()\n\n    with cubes.randomize.position():\n        # Randomize positions in a certain area\n        rep.modify.bbox((-2, -2, 0.1), (2, 2, 1.0))\n\n    with cubes.randomize.rotation():\n        # Randomize rotations\n        rep.modify.rotation(rep.distribution.uniform((-180, -180, -180), (180, 180, 180)))\n\n# Create trigger for randomization\ntrigger = rep.trigger.on_frame(num_frames=1)\n\n# Register randomizers with trigger\nwith trigger:\n    randomize_lighting()\n    randomize_materials()\n    randomize_objects()\n\n# Create a function to capture data with randomization\ndef capture_randomized_data(num_samples=100):\n    """Capture data with domain randomization applied"""\n    print(f"Capturing {num_samples} samples with domain randomization...")\n\n    for i in range(num_samples):\n        # Trigger randomization\n        rep.orchestrator.step()\n\n        # Capture RGB, depth, segmentation\n        # This would use Isaac Sim\'s rendering pipelines\n        print(f"Captured sample {i+1}/{num_samples}")\n\n    print("Data capture completed with domain randomization")\n\n# Execute the randomization and capture process\ncapture_randomized_data(50)\n'})}),"\n",(0,i.jsx)(e.h3,{id:"example-2-synthetic-dataset-pipeline-integration",children:"Example 2: Synthetic Dataset Pipeline Integration"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n# synthetic_pipeline_integration.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, PointCloud2\nfrom std_msgs.msg import String\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport json\nfrom pathlib import Path\n\nclass SyntheticPipelineIntegration(Node):\n    """\n    Integrate synthetic data generation with ROS 2 pipeline\n    """\n\n    def __init__(self):\n        super().__init__(\'synthetic_pipeline_integration\')\n\n        # Publishers for synthetic data\n        self.synthetic_rgb_pub = self.create_publisher(Image, \'/synthetic/camera/rgb\', 10)\n        self.synthetic_depth_pub = self.create_publisher(Image, \'/synthetic/camera/depth\', 10)\n        self.synthetic_status_pub = self.create_publisher(String, \'/synthetic/status\', 10)\n\n        # CV Bridge for image conversion\n        self.cv_bridge = CvBridge()\n\n        # Synthetic data parameters\n        self.dataset_path = Path("synthetic_robot_dataset")\n        self.current_sample_idx = 0\n        self.total_samples = 0\n\n        # Timers\n        self.data_gen_timer = self.create_timer(0.1, self.publish_synthetic_data)\n        self.status_timer = self.create_timer(5.0, self.publish_status)\n\n        # Load dataset if available\n        self.load_synthetic_dataset()\n\n        self.get_logger().info(\'Synthetic Pipeline Integration node initialized\')\n\n    def load_synthetic_dataset(self):\n        """Load synthetic dataset for streaming"""\n        if self.dataset_path.exists():\n            sample_dirs = [d for d in self.dataset_path.iterdir() if d.is_dir()]\n            self.total_samples = len(sample_dirs)\n            self.get_logger().info(f\'Loaded {self.total_samples} synthetic samples\')\n        else:\n            self.get_logger().warn(f\'Dataset path {self.dataset_path} does not exist\')\n\n    def publish_synthetic_data(self):\n        """Publish synthetic data to ROS topics"""\n        if self.total_samples == 0:\n            return\n\n        # Load next sample\n        sample_path = self.dataset_path / f"sample_{self.current_sample_idx:06d}"\n\n        if not sample_path.exists():\n            # Loop back to beginning if we reach the end\n            self.current_sample_idx = 0\n            sample_path = self.dataset_path / f"sample_{self.current_sample_idx:06d}"\n\n        # Load RGB image\n        rgb_path = sample_path / "rgb.png"\n        if rgb_path.exists():\n            rgb_img = cv2.imread(str(rgb_path))\n            if rgb_img is not None:\n                # Convert BGR to RGB\n                rgb_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2RGB)\n\n                # Convert to ROS Image message\n                ros_img = self.cv_bridge.cv2_to_imgmsg(rgb_img, encoding="rgb8")\n                ros_img.header.stamp = self.get_clock().now().to_msg()\n                ros_img.header.frame_id = "synthetic_camera_rgb_optical_frame"\n\n                self.synthetic_rgb_pub.publish(ros_img)\n\n        # Load depth image\n        depth_path = sample_path / "depth.png"\n        if depth_path.exists():\n            depth_img = cv2.imread(str(depth_path), cv2.IMREAD_UNCHANGED)\n            if depth_img is not None:\n                # Convert to float32 meters\n                depth_img_float = depth_img.astype(np.float32) / 256.0\n\n                # Convert to ROS Image message\n                ros_depth = self.cv_bridge.cv2_to_imgmsg(depth_img_float, encoding="passthrough")\n                ros_depth.header.stamp = self.get_clock().now().to_msg()\n                ros_depth.header.frame_id = "synthetic_camera_depth_optical_frame"\n\n                self.synthetic_depth_pub.publish(ros_depth)\n\n        # Move to next sample\n        self.current_sample_idx = (self.current_sample_idx + 1) % self.total_samples\n\n    def publish_status(self):\n        """Publish synthetic data generation status"""\n        status_msg = String()\n        status_msg.data = f"Synthetic dataset streaming: {self.current_sample_idx}/{self.total_samples} samples, " \\\n                         f"rate: 10Hz"\n        self.synthetic_status_pub.publish(status_msg)\n\n    def validate_synthetic_data_stream(self):\n        """Validate the quality of synthetic data stream"""\n        # This would validate that the synthetic data stream meets quality requirements\n        # For example, checking that images are properly formatted, depths are reasonable, etc.\n        self.get_logger().info("Validating synthetic data stream...")\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = SyntheticPipelineIntegration()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info(\'Shutting down Synthetic Pipeline Integration...\')\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(e.p,{children:"Synthetic data generation is a cornerstone of modern robotics AI development, enabling rapid training dataset creation with perfect ground truth annotations. Isaac Sim provides powerful capabilities for generating diverse, labeled datasets that can be used to train perception and control systems with domain randomization techniques to ensure robustness."}),"\n",(0,i.jsx)(e.p,{children:"Key aspects include:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Efficiency"}),": Generating thousands of labeled samples in minutes rather than hours"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Safety"}),": Training in dangerous scenarios without risk to equipment or personnel"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Quality"}),": Perfect ground truth annotations across multiple modalities"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Variety"}),": Domain randomization to create robust AI models"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"exercises",children:"Exercises"}),"\n",(0,i.jsx)(e.h3,{id:"conceptual",children:"Conceptual"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Explain the advantages of synthetic data generation over real-world data collection for robotics AI training."}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"logical",children:"Logical"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Analyze the domain randomization parameters that would be most important for training a humanoid robot to operate in diverse indoor environments."}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"implementation-1",children:"Implementation"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Implement a synthetic data generation pipeline that creates a dataset of 1000 images with randomized lighting, textures, and object arrangements suitable for training a humanoid robot's perception system."}),"\n"]})]})}function m(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(p,{...n})}):p(n)}},8453(n,e,t){t.d(e,{R:()=>r,x:()=>o});var a=t(6540);const i={},s=a.createContext(i);function r(n){const e=a.useContext(s);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:r(n.components),a.createElement(s.Provider,{value:e},n.children)}}}]);