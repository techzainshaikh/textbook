"use strict";(globalThis.webpackChunkphysical_ai_book_humanoid=globalThis.webpackChunkphysical_ai_book_humanoid||[]).push([[1300],{7019(n,e,i){i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>s,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"module-3-ai-brain/chapter-6-sim-to-real","title":"Sim-to-Real Transfer for Humanoid Robots","description":"Bridging the reality gap between simulation and real-world humanoid robot deployment using domain randomization and system identification","source":"@site/docs/module-3-ai-brain/chapter-6-sim-to-real.md","sourceDirName":"module-3-ai-brain","slug":"/module-3-ai-brain/chapter-6-sim-to-real","permalink":"/textbook/docs/module-3-ai-brain/chapter-6-sim-to-real","draft":false,"unlisted":false,"editUrl":"https://github.com/Panaversity/physical_ai_book_humanoid/tree/main/docs/module-3-ai-brain/chapter-6-sim-to-real.md","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"title":"Sim-to-Real Transfer for Humanoid Robots","sidebar_position":7,"description":"Bridging the reality gap between simulation and real-world humanoid robot deployment using domain randomization and system identification","keywords":["sim-to-real","domain randomization","system identification","reality gap","humanoid robotics","transfer learning"]},"sidebar":"tutorialSidebar","previous":{"title":"Reinforcement Learning for Robotics with Isaac Sim","permalink":"/textbook/docs/module-3-ai-brain/chapter-5-reinforcement-learning"},"next":{"title":"Introduction to Vision-Language-Action Systems","permalink":"/textbook/docs/module-4-vla/intro"}}');var a=i(4848),o=i(8453);const s={title:"Sim-to-Real Transfer for Humanoid Robots",sidebar_position:7,description:"Bridging the reality gap between simulation and real-world humanoid robot deployment using domain randomization and system identification",keywords:["sim-to-real","domain randomization","system identification","reality gap","humanoid robotics","transfer learning"]},r="Chapter 6: Sim-to-Real Transfer for Humanoid Robots",l={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Core Concepts",id:"core-concepts",level:2},{value:"The Reality Gap",id:"the-reality-gap",level:3},{value:"Domain Randomization Techniques",id:"domain-randomization-techniques",level:3},{value:"Implementation",id:"implementation",level:2},{value:"Domain Randomization Environment",id:"domain-randomization-environment",level:3},{value:"System Identification for Real Robot Calibration",id:"system-identification-for-real-robot-calibration",level:3},{value:"Policy Adaptation and Transfer",id:"policy-adaptation-and-transfer",level:3},{value:"Examples",id:"examples",level:2},{value:"Example 1: Complete Sim-to-Real Pipeline",id:"example-1-complete-sim-to-real-pipeline",level:3},{value:"Example 2: Safety-Critical Deployment",id:"example-2-safety-critical-deployment",level:3},{value:"Summary",id:"summary",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Conceptual",id:"conceptual",level:3},{value:"Logical",id:"logical",level:3},{value:"Implementation",id:"implementation-1",level:3}];function c(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"chapter-6-sim-to-real-transfer-for-humanoid-robots",children:"Chapter 6: Sim-to-Real Transfer for Humanoid Robots"})}),"\n",(0,a.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(e.p,{children:"By the end of this chapter, students will be able to:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Identify and analyze the reality gap between simulation and real-world robotics"}),"\n",(0,a.jsx)(e.li,{children:"Implement domain randomization techniques to improve policy robustness"}),"\n",(0,a.jsx)(e.li,{children:"Perform system identification for real robot parameter estimation"}),"\n",(0,a.jsx)(e.li,{children:"Apply transfer learning methods to adapt simulation-trained policies for real robots"}),"\n",(0,a.jsx)(e.li,{children:"Validate and deploy policies from simulation to real humanoid robots safely"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsx)(e.p,{children:"Students should have:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Understanding of simulation environments (covered in Module 2)"}),"\n",(0,a.jsx)(e.li,{children:"Knowledge of reinforcement learning concepts (covered in Chapter 5)"}),"\n",(0,a.jsx)(e.li,{children:"Experience with robot control and dynamics (covered in Module 1)"}),"\n",(0,a.jsx)(e.li,{children:"Basic understanding of probability and statistics for domain randomization"}),"\n",(0,a.jsx)(e.li,{children:"Familiarity with system identification techniques"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"core-concepts",children:"Core Concepts"}),"\n",(0,a.jsx)(e.p,{children:"Sim-to-real transfer addresses the challenge of deploying policies trained in simulation on real robots. The reality gap arises from differences in dynamics, sensors, actuators, and environmental conditions between simulation and reality."}),"\n",(0,a.jsx)(e.h3,{id:"the-reality-gap",children:"The Reality Gap"}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Sources of Discrepancy:"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Dynamics Modeling"}),": Inaccuracies in mass, inertia, friction, and contact models"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Sensor Noise"}),": Differences in sensor characteristics, noise patterns, and delays"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Actuator Dynamics"}),": Motor response times, gear backlash, and control delays"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Environmental Conditions"}),": Surface properties, lighting, and external disturbances"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Hardware Limitations"}),": Joint limits, power constraints, and mechanical wear"]}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Transfer Approaches:"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Domain Randomization"}),": Randomizing simulation parameters to improve robustness"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"System Identification"}),": Measuring real robot parameters for simulation calibration"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Adaptive Control"}),": Adjusting control policies based on real-world performance"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Domain Adaptation"}),": Learning mappings between simulation and real data"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Few-Shot Learning"}),": Rapid adaptation with minimal real-world data"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"domain-randomization-techniques",children:"Domain Randomization Techniques"}),"\n",(0,a.jsx)(e.p,{children:"Domain randomization involves randomizing simulation parameters across wide ranges to force the policy to learn robust features that work across different conditions."}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Physical Parameters:"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Mass scaling (\xb120%)"}),"\n",(0,a.jsx)(e.li,{children:"Friction coefficients (0.1x to 10x)"}),"\n",(0,a.jsx)(e.li,{children:"Actuator delays and noise"}),"\n",(0,a.jsx)(e.li,{children:"Sensor noise and bias"}),"\n",(0,a.jsx)(e.li,{children:"Contact stiffness and damping"}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Visual Parameters:"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Lighting conditions and directions"}),"\n",(0,a.jsx)(e.li,{children:"Texture variations"}),"\n",(0,a.jsx)(e.li,{children:"Color and contrast changes"}),"\n",(0,a.jsx)(e.li,{children:"Camera noise and distortion"}),"\n",(0,a.jsx)(e.li,{children:"Environmental appearance"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"implementation",children:"Implementation"}),"\n",(0,a.jsx)(e.p,{children:"Let's implement sim-to-real transfer techniques for humanoid robot deployment:"}),"\n",(0,a.jsx)(e.h3,{id:"domain-randomization-environment",children:"Domain Randomization Environment"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n# domain_randomization.py\n\nimport numpy as np\nimport torch\nimport random\nfrom typing import Dict, Any, List, Tuple\nfrom dataclasses import dataclass\n\n@dataclass\nclass DomainRandomizationParams:\n    """Parameters for domain randomization"""\n    mass_range: Tuple[float, float] = (0.8, 1.2)  # \xb120% mass variation\n    friction_range: Tuple[float, float] = (0.1, 2.0)  # 0.1x to 2x friction\n    actuator_noise_range: Tuple[float, float] = (0.001, 0.01)  # Actuator noise\n    sensor_noise_range: Tuple[float, float] = (0.001, 0.01)  # Sensor noise\n    dynamics_randomization: Tuple[float, float] = (0.9, 1.1)  # Dynamics scaling\n    delay_range: Tuple[float, float] = (0.0, 0.05)  # Actuator delays in seconds\n\nclass DomainRandomizedEnvironment:\n    """\n    Environment with domain randomization for sim-to-real transfer\n    """\n\n    def __init__(self, base_env, randomization_params: DomainRandomizationParams):\n        self.base_env = base_env\n        self.params = randomization_params\n        self.current_randomization = self.randomize_parameters()\n        self.episode_count = 0\n\n    def randomize_parameters(self) -> Dict[str, float]:\n        """Randomize simulation parameters"""\n        randomization = {\n            \'mass_multiplier\': random.uniform(*self.params.mass_range),\n            \'friction_multiplier\': random.uniform(*self.params.friction_range),\n            \'actuator_noise_std\': random.uniform(*self.params.actuator_noise_range),\n            \'sensor_noise_std\': random.uniform(*self.params.sensor_noise_range),\n            \'dynamics_multiplier\': random.uniform(*self.params.dynamics_randomization),\n            \'actuator_delay\': random.uniform(*self.params.delay_range)\n        }\n        return randomization\n\n    def reset(self) -> np.ndarray:\n        """Reset environment with new randomization parameters"""\n        if self.episode_count > 0 and self.episode_count % 10 == 0:  # Randomize every 10 episodes\n            self.current_randomization = self.randomize_parameters()\n            self.update_simulation_parameters()\n\n        self.episode_count += 1\n        return self.base_env.reset()\n\n    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, Dict]:\n        """Execute step with randomized parameters"""\n        # Add actuator noise\n        noisy_action = self.add_actuator_noise(action)\n\n        # Apply actuator delay simulation\n        delayed_action = self.apply_actuator_delay(noisy_action)\n\n        # Execute step in base environment\n        obs, reward, done, info = self.base_env.step(delayed_action)\n\n        # Add sensor noise to observation\n        noisy_obs = self.add_sensor_noise(obs)\n\n        # Update info with randomization parameters\n        info[\'randomization_params\'] = self.current_randomization\n\n        return noisy_obs, reward, done, info\n\n    def add_actuator_noise(self, action: np.ndarray) -> np.ndarray:\n        """Add noise to actuator commands"""\n        noise = np.random.normal(0, self.current_randomization[\'actuator_noise_std\'], size=action.shape)\n        return action + noise\n\n    def apply_actuator_delay(self, action: np.ndarray) -> np.ndarray:\n        """Simulate actuator delay"""\n        # In a real implementation, this would maintain a delay buffer\n        return action  # Simplified for this example\n\n    def add_sensor_noise(self, observation: np.ndarray) -> np.ndarray:\n        """Add noise to sensor observations"""\n        noise = np.random.normal(0, self.current_randomization[\'sensor_noise_std\'], size=observation.shape)\n        return observation + noise\n\n    def update_simulation_parameters(self):\n        """Update simulation with current randomization parameters"""\n        # This would interface with the physics simulator to update parameters\n        # For example, updating mass, friction, etc. of robot links\n        pass\n\nclass CurriculumDomainRandomization:\n    """\n    Progressive domain randomization that increases difficulty over time\n    """\n\n    def __init__(self, base_env, initial_params: DomainRandomizationParams,\n                 max_params: DomainRandomizationParams, curriculum_steps: int = 1000):\n        self.base_env = base_env\n        self.initial_params = initial_params\n        self.max_params = max_params\n        self.curriculum_steps = curriculum_steps\n        self.current_step = 0\n\n    def get_current_params(self) -> DomainRandomizationParams:\n        """Get current domain randomization parameters based on curriculum progress"""\n        progress = min(1.0, self.current_step / self.curriculum_steps)\n\n        # Interpolate between initial and max parameters\n        current_params = DomainRandomizationParams()\n\n        # Interpolate mass range\n        initial_mass_range = self.initial_params.mass_range\n        max_mass_range = self.max_params.mass_range\n        current_params.mass_range = (\n            initial_mass_range[0] + progress * (max_mass_range[0] - initial_mass_range[0]),\n            initial_mass_range[1] + progress * (max_mass_range[1] - initial_mass_range[1])\n        )\n\n        # Interpolate friction range\n        initial_friction_range = self.initial_params.friction_range\n        max_friction_range = self.max_params.friction_range\n        current_params.friction_range = (\n            initial_friction_range[0] + progress * (max_friction_range[0] - initial_friction_range[0]),\n            initial_friction_range[1] + progress * (max_friction_range[1] - initial_friction_range[1])\n        )\n\n        # Continue for other parameters...\n        current_params.actuator_noise_range = (\n            self.initial_params.actuator_noise_range[0] +\n            progress * (self.max_params.actuator_noise_range[0] - self.initial_params.actuator_noise_range[0]),\n            self.initial_params.actuator_noise_range[1] +\n            progress * (self.max_params.actuator_noise_range[1] - self.initial_params.actuator_noise_range[1])\n        )\n\n        current_params.sensor_noise_range = (\n            self.initial_params.sensor_noise_range[0] +\n            progress * (self.max_params.sensor_noise_range[0] - self.initial_params.sensor_noise_range[0]),\n            self.initial_params.sensor_noise_range[1] +\n            progress * (self.max_params.sensor_noise_range[1] - self.initial_params.sensor_noise_range[1])\n        )\n\n        return current_params\n\n    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, Dict]:\n        """Execute step with curriculum-based randomization"""\n        # Update curriculum progress\n        self.current_step += 1\n\n        # Get current parameters\n        current_params = self.get_current_params()\n\n        # Create domain randomized environment with current parameters\n        dr_env = DomainRandomizedEnvironment(self.base_env, current_params)\n\n        # Execute step\n        return dr_env.step(action)\n\ndef create_domain_randomized_env(base_env):\n    """Factory function to create domain randomized environment"""\n    initial_params = DomainRandomizationParams(\n        mass_range=(0.9, 1.1),  # Start with \xb110%\n        friction_range=(0.5, 2.0),  # Start with 0.5x to 2x\n        actuator_noise_range=(0.001, 0.005),  # Start with lower noise\n        sensor_noise_range=(0.001, 0.005)   # Start with lower noise\n    )\n\n    max_params = DomainRandomizationParams(\n        mass_range=(0.5, 2.0),  # End with \xb150% to 2x\n        friction_range=(0.1, 5.0),  # End with 0.1x to 5x\n        actuator_noise_range=(0.005, 0.05),  # End with higher noise\n        sensor_noise_range=(0.005, 0.05)   # End with higher noise\n    )\n\n    return CurriculumDomainRandomization(base_env, initial_params, max_params)\n'})}),"\n",(0,a.jsx)(e.h3,{id:"system-identification-for-real-robot-calibration",children:"System Identification for Real Robot Calibration"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n# system_identification.py\n\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy import signal\nimport matplotlib.pyplot as plt\nfrom typing import Dict, Tuple, List\nimport control  # python-control package\n\nclass SystemIdentifier:\n    """\n    System identification for real robot parameter estimation\n    """\n\n    def __init__(self, robot_model):\n        self.robot_model = robot_model\n        self.identified_params = {}\n        self.excitation_signals = []\n        self.measured_responses = []\n\n    def generate_excitation_signal(self, duration: float, frequency_range: Tuple[float, float],\n                                   amplitude: float, sampling_rate: int = 100) -> np.ndarray:\n        """Generate excitation signal for system identification"""\n        t = np.linspace(0, duration, int(duration * sampling_rate))\n\n        # Generate multi-sine signal covering frequency range\n        signal_components = []\n        freqs = np.logspace(np.log10(frequency_range[0]), np.log10(frequency_range[1]), 10)\n\n        for freq in freqs:\n            component = amplitude * np.sin(2 * np.pi * freq * t)\n            signal_components.append(component)\n\n        # Add random phases\n        excitation_signal = np.sum(signal_components, axis=0)\n\n        # Add some random noise for robustness\n        noise = np.random.normal(0, amplitude * 0.1, size=excitation_signal.shape)\n        excitation_signal += noise\n\n        return excitation_signal\n\n    def collect_data(self, joint_indices: List[int], duration: float = 10.0) -> Dict[str, np.ndarray]:\n        """Collect input-output data for system identification"""\n        data = {\n            \'time\': [],\n            \'inputs\': {idx: [] for idx in joint_indices},\n            \'outputs\': {idx: [] for idx in joint_indices},\n            \'velocities\': {idx: [] for idx in joint_indices},\n            \'accelerations\': {idx: [] for idx in joint_indices}\n        }\n\n        # Generate excitation for each joint\n        for joint_idx in joint_indices:\n            # Generate excitation signal\n            excitation = self.generate_excitation_signal(duration, (0.1, 10.0), 0.1)\n\n            # Apply excitation and collect response\n            for i, u in enumerate(excitation):\n                # Apply input to joint\n                self.robot_model.apply_torque(joint_idx, u)\n\n                # Step simulation\n                self.robot_model.step()\n\n                # Collect data\n                data[\'time\'].append(i / len(excitation) * duration)\n                data[\'inputs\'][joint_idx].append(u)\n                data[\'outputs\'][joint_idx].append(self.robot_model.get_joint_position(joint_idx))\n                data[\'velocities\'][joint_idx].append(self.robot_model.get_joint_velocity(joint_idx))\n\n                # Calculate acceleration (approximate)\n                if i > 1:\n                    dt = duration / len(excitation)\n                    prev_vel = self.robot_model.get_joint_velocity(joint_idx, prev=True)\n                    acc = (self.robot_model.get_joint_velocity(joint_idx) - prev_vel) / dt\n                    data[\'accelerations\'][joint_idx].append(acc)\n                else:\n                    data[\'accelerations\'][joint_idx].append(0.0)\n\n        return data\n\n    def identify_mass_matrix(self, data: Dict[str, np.ndarray], joint_indices: List[int]) -> np.ndarray:\n        """Identify mass matrix using collected data"""\n        # For each joint, estimate the mass based on input-output relationship\n        # This is a simplified approach - real system identification would be more complex\n\n        n_joints = len(joint_indices)\n        mass_matrix = np.zeros((n_joints, n_joints))\n\n        for i, joint_idx in enumerate(joint_indices):\n            # Extract relevant data\n            inputs = np.array(data[\'inputs\'][joint_idx])\n            accelerations = np.array(data[\'accelerations\'][joint_idx])\n\n            # Estimate mass: F = M*a => M = F/a (for single DOF)\n            # In reality, this would involve more complex MIMO system identification\n            valid_mask = np.abs(accelerations) > 1e-6  # Avoid division by small numbers\n\n            if np.any(valid_mask):\n                estimated_mass = np.mean(inputs[valid_mask] / accelerations[valid_mask])\n                mass_matrix[i, i] = estimated_mass\n            else:\n                mass_matrix[i, i] = 1.0  # Default value\n\n        return mass_matrix\n\n    def identify_friction_parameters(self, data: Dict[str, np.ndarray], joint_indices: List[int]) -> Dict[int, Dict[str, float]]:\n        """Identify friction parameters (Coulomb and viscous)"""\n        friction_params = {}\n\n        for joint_idx in joint_indices:\n            velocities = np.array(data[\'velocities\'][joint_idx])\n            torques = np.array(data[\'inputs\'][joint_idx])\n\n            # Model: tau_friction = tau_coulomb * sign(v) + tau_viscous * v\n            # Use least squares to fit friction model\n            A = np.column_stack([np.sign(velocities), velocities])\n\n            try:\n                # Solve: min ||A*x - torques||^2\n                params, residuals, rank, s = np.linalg.lstsq(A, torques, rcond=None)\n                coulomb_friction, viscous_friction = params\n\n                friction_params[joint_idx] = {\n                    \'coulomb\': abs(coulomb_friction),\n                    \'viscous\': abs(viscous_friction)\n                }\n            except:\n                # Default values if identification fails\n                friction_params[joint_idx] = {\n                    \'coulomb\': 0.1,\n                    \'viscous\': 0.01\n                }\n\n        return friction_params\n\n    def identify_compliance(self, data: Dict[str, np.ndarray], joint_indices: List[int]) -> Dict[int, float]:\n        """Identify joint compliance/stiffness parameters"""\n        compliance_params = {}\n\n        for joint_idx in joint_indices:\n            positions = np.array(data[\'outputs\'][joint_idx])\n            torques = np.array(data[\'inputs\'][joint_idx])\n\n            # Estimate stiffness: k = delta_tau / delta_theta\n            # Remove mean to focus on variations\n            pos_var = positions - np.mean(positions)\n            tau_var = torques - np.mean(torques)\n\n            # Use least squares to estimate stiffness\n            if np.var(pos_var) > 1e-10:  # Avoid division by small variance\n                stiffness = np.cov(tau_var, pos_var)[0, 1] / np.var(pos_var)\n                compliance_params[joint_idx] = 1.0 / abs(stiffness) if abs(stiffness) > 1e-6 else 1e6\n            else:\n                compliance_params[joint_idx] = 1e-6  # Very stiff (low compliance)\n\n        return compliance_params\n\n    def identify_full_model(self, joint_indices: List[int]) -> Dict[str, Any]:\n        """Perform complete system identification"""\n        print("Collecting data for system identification...")\n        data = self.collect_data(joint_indices)\n\n        print("Identifying mass matrix...")\n        mass_matrix = self.identify_mass_matrix(data, joint_indices)\n\n        print("Identifying friction parameters...")\n        friction_params = self.identify_friction_parameters(data, joint_indices)\n\n        print("Identifying compliance parameters...")\n        compliance_params = self.identify_compliance(data, joint_indices)\n\n        # Compile results\n        results = {\n            \'mass_matrix\': mass_matrix,\n            \'friction_params\': friction_params,\n            \'compliance_params\': compliance_params,\n            \'data_used\': data\n        }\n\n        self.identified_params = results\n        return results\n\n    def update_simulation_model(self, sim_env, identified_params: Dict[str, Any]):\n        """Update simulation environment with identified parameters"""\n        # Update mass properties\n        for i, joint_idx in enumerate(identified_params[\'mass_matrix\']):\n            sim_env.update_joint_mass(joint_idx, identified_params[\'mass_matrix\'][i, i])\n\n        # Update friction parameters\n        for joint_idx, friction_info in identified_params[\'friction_params\'].items():\n            sim_env.update_joint_friction(joint_idx,\n                                        friction_info[\'coulomb\'],\n                                        friction_info[\'viscous\'])\n\n        # Update compliance parameters\n        for joint_idx, compliance in identified_params[\'compliance_params\'].items():\n            sim_env.update_joint_compliance(joint_idx, compliance)\n\ndef validate_identification(identified_model, real_robot, test_trajectories: List[np.ndarray]):\n    """Validate system identification results"""\n    errors = []\n\n    for trajectory in test_trajectories:\n        # Apply same inputs to both models\n        real_response = apply_trajectory(real_robot, trajectory)\n        identified_response = apply_trajectory(identified_model, trajectory)\n\n        # Calculate error\n        error = np.mean(np.abs(real_response - identified_response))\n        errors.append(error)\n\n    return np.mean(errors)\n\ndef apply_trajectory(robot, trajectory: np.ndarray) -> np.ndarray:\n    """Apply trajectory to robot and return response"""\n    # This is a placeholder function\n    # In practice, this would apply the trajectory and collect the response\n    pass\n'})}),"\n",(0,a.jsx)(e.h3,{id:"policy-adaptation-and-transfer",children:"Policy Adaptation and Transfer"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n# policy_adaptation.py\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom typing import Dict, Any, List, Tuple\nimport copy\n\nclass PolicyAdapter:\n    """\n    Adapt simulation-trained policies for real robot deployment\n    """\n\n    def __init__(self, sim_policy: nn.Module, real_robot_params: Dict[str, Any]):\n        self.sim_policy = sim_policy\n        self.real_params = real_robot_params\n        self.adapted_policy = None\n        self.adaptation_network = None\n\n    def create_adaptation_network(self, state_dim: int, action_dim: int, hidden_dim: int = 256):\n        """Create network to adapt simulation policy to real robot"""\n        # Adaptation network learns the residual between sim and real\n        self.adaptation_network = nn.Sequential(\n            nn.Linear(state_dim + action_dim, hidden_dim),  # State and sim action as input\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, action_dim),  # Output action adjustment\n            nn.Tanh()  # Keep adjustments bounded\n        )\n\n    def adapt_policy(self, real_data: List[Tuple[np.ndarray, np.ndarray, np.ndarray]],\n                    learning_rate: float = 1e-4, epochs: int = 100):\n        """Adapt policy using real robot data"""\n        if self.adaptation_network is None:\n            raise ValueError("Adaptation network not created. Call create_adaptation_network first.")\n\n        optimizer = torch.optim.Adam(self.adaptation_network.parameters(), lr=learning_rate)\n\n        for epoch in range(epochs):\n            total_loss = 0\n\n            for state, sim_action, real_action in real_data:\n                state_tensor = torch.FloatTensor(state).unsqueeze(0)\n                sim_action_tensor = torch.FloatTensor(sim_action).unsqueeze(0)\n                real_action_tensor = torch.FloatTensor(real_action).unsqueeze(0)\n\n                # Get sim policy action\n                with torch.no_grad():\n                    sim_action_pred = self.sim_policy(state_tensor)[0]  # Assuming policy returns (action, value)\n\n                # Get adaptation adjustment\n                adaptation_input = torch.cat([state_tensor, sim_action_pred], dim=1)\n                action_adjustment = self.adaptation_network(adaptation_input)\n\n                # Combined action\n                adapted_action = sim_action_pred + action_adjustment\n\n                # Loss: difference between adapted action and real action\n                loss = nn.MSELoss()(adapted_action, real_action_tensor)\n\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n                total_loss += loss.item()\n\n            if epoch % 20 == 0:\n                print(f"Adaptation epoch {epoch}, Loss: {total_loss/len(real_data):.6f}")\n\n        # Create adapted policy\n        self.adapted_policy = self.create_adapted_policy()\n\n    def create_adapted_policy(self):\n        """Create a policy that combines sim policy with adaptation network"""\n        class AdaptedPolicy(nn.Module):\n            def __init__(self, sim_policy, adaptation_network):\n                super().__init__()\n                self.sim_policy = sim_policy\n                self.adaptation_network = adaptation_network\n\n            def forward(self, state):\n                # Get action from sim policy\n                sim_action, value = self.sim_policy(state)\n\n                # Get adaptation adjustment\n                adaptation_input = torch.cat([state, sim_action], dim=1)\n                action_adjustment = self.adaptation_network(adaptation_input)\n\n                # Return adapted action\n                adapted_action = sim_action + action_adjustment\n                return adapted_action, value\n\n        return AdaptedPolicy(self.sim_policy, self.adaptation_network)\n\n    def get_adapted_action(self, state: np.ndarray) -> np.ndarray:\n        """Get action from adapted policy"""\n        if self.adapted_policy is None:\n            # If not adapted, return sim policy action\n            with torch.no_grad():\n                action, _ = self.sim_policy(torch.FloatTensor(state).unsqueeze(0))\n            return action.squeeze(0).numpy()\n\n        with torch.no_grad():\n            action, _ = self.adapted_policy(torch.FloatTensor(state).unsqueeze(0))\n        return action.squeeze(0).numpy()\n\nclass FineTuningAdapter:\n    """\n    Fine-tune simulation policy using real robot data\n    """\n\n    def __init__(self, sim_policy: nn.Module, learning_rate: float = 1e-5):\n        self.sim_policy = copy.deepcopy(sim_policy)\n        self.optimizer = torch.optim.Adam(self.sim_policy.parameters(), lr=learning_rate)\n        self.criterion = nn.MSELoss()\n\n    def fine_tune(self, real_data: List[Tuple[np.ndarray, np.ndarray]],\n                 epochs: int = 50, batch_size: int = 32):\n        """Fine-tune policy using real robot data"""\n        for epoch in range(epochs):\n            # Shuffle data\n            shuffled_data = real_data.copy()\n            np.random.shuffle(shuffled_data)\n\n            total_loss = 0\n            num_batches = len(shuffled_data) // batch_size\n\n            for i in range(num_batches):\n                batch = shuffled_data[i*batch_size:(i+1)*batch_size]\n\n                states = torch.FloatTensor([item[0] for item in batch])\n                actions = torch.FloatTensor([item[1] for item in batch])\n\n                # Get policy predictions\n                pred_actions, _ = self.sim_policy(states)\n\n                # Calculate loss\n                loss = self.criterion(pred_actions, actions)\n\n                # Backpropagate\n                self.optimizer.zero_grad()\n                loss.backward()\n                self.optimizer.step()\n\n                total_loss += loss.item()\n\n            if epoch % 10 == 0:\n                avg_loss = total_loss / num_batches\n                print(f"Fine-tuning epoch {epoch}, Average Loss: {avg_loss:.6f}")\n\n    def get_action(self, state: np.ndarray) -> np.ndarray:\n        """Get action from fine-tuned policy"""\n        with torch.no_grad():\n            action, _ = self.sim_policy(torch.FloatTensor(state).unsqueeze(0))\n        return action.squeeze(0).numpy()\n\nclass DomainAdversarialAdapter:\n    """\n    Use domain adversarial training to adapt policies\n    """\n\n    def __init__(self, policy: nn.Module, state_dim: int):\n        self.policy = policy\n        self.domain_classifier = nn.Sequential(\n            nn.Linear(state_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            nn.Linear(256, 1),\n            nn.Sigmoid()\n        )\n        self.policy_optimizer = torch.optim.Adam(policy.parameters(), lr=1e-4)\n        self.domain_optimizer = torch.optim.Adam(self.domain_classifier.parameters(), lr=1e-4)\n        self.domain_criterion = nn.BCELoss()\n\n    def train_adversarial(self, sim_data: List[Tuple[np.ndarray, np.ndarray]],\n                         real_data: List[Tuple[np.ndarray, np.ndarray]],\n                         epochs: int = 100):\n        """Train using domain adversarial approach"""\n        for epoch in range(epochs):\n            # Train domain classifier\n            self.domain_optimizer.zero_grad()\n\n            # Sim data (label 0)\n            sim_states = torch.FloatTensor([item[0] for item in sim_data])\n            sim_labels = torch.zeros(sim_states.size(0), 1)\n\n            # Real data (label 1)\n            real_states = torch.FloatTensor([item[0] for item in real_data])\n            real_labels = torch.ones(real_states.size(0), 1)\n\n            # Combine and shuffle\n            all_states = torch.cat([sim_states, real_states], dim=0)\n            all_labels = torch.cat([sim_labels, real_labels], dim=0)\n\n            domain_preds = self.domain_classifier(all_states)\n            domain_loss = self.domain_criterion(domain_preds, all_labels)\n\n            domain_loss.backward()\n            self.domain_optimizer.step()\n\n            # Train policy to fool domain classifier (gradient reversal)\n            self.policy_optimizer.zero_grad()\n\n            # Get features from policy (before action output)\n            # This assumes the policy has a feature extractor\n            sim_features = self.get_policy_features(sim_states)\n            real_features = self.get_policy_features(real_states)\n\n            # Domain classifier should not be able to distinguish\n            sim_domain_pred = self.domain_classifier(sim_features)\n            real_domain_pred = self.domain_classifier(real_features)\n\n            # We want domain classifier to output 0.5 for both (unconfident)\n            sim_adv_loss = self.domain_criterion(sim_domain_pred, torch.ones_like(sim_domain_pred) * 0.5)\n            real_adv_loss = self.domain_criterion(real_domain_pred, torch.ones_like(real_domain_pred) * 0.5)\n\n            adv_loss = sim_adv_loss + real_adv_loss\n            adv_loss.backward()\n            self.policy_optimizer.step()\n\n            if epoch % 20 == 0:\n                print(f"Adversarial training epoch {epoch}, Domain Loss: {domain_loss.item():.6f}, Adv Loss: {adv_loss.item():.6f}")\n\n    def get_policy_features(self, states):\n        """Extract features from policy (implementation depends on policy architecture)"""\n        # This is a placeholder - actual implementation depends on policy structure\n        # For a typical policy, this might be the output of the feature extractor\n        # before the action and value heads\n        return states  # Simplified for this example\n\ndef safety_wrapper(policy, safety_checker, state_filter=None):\n    """\n    Wrap policy with safety checks\n    """\n    def safe_policy(state):\n        # Apply state filtering if provided\n        if state_filter:\n            state = state_filter(state)\n\n        # Get action from policy\n        action = policy(state)\n\n        # Check safety\n        if not safety_checker.is_safe(state, action):\n            # Return safe fallback action\n            return safety_checker.get_safe_action(state)\n\n        return action\n\n    return safe_policy\n\nclass SafetyChecker:\n    """\n    Safety checker for real robot deployment\n    """\n\n    def __init__(self, joint_limits: Dict[int, Tuple[float, float]],\n                 max_velocities: Dict[int, Tuple[float, float]]):\n        self.joint_limits = joint_limits\n        self.max_velocities = max_velocities\n\n    def is_safe(self, state: np.ndarray, action: np.ndarray) -> bool:\n        """Check if action is safe given current state"""\n        # Check joint limits (simplified)\n        for joint_idx, (lower, upper) in self.joint_limits.items():\n            if joint_idx < len(state):\n                if state[joint_idx] < lower or state[joint_idx] > upper:\n                    return False\n\n        # Check action magnitude\n        if np.any(np.abs(action) > 10.0):  # Arbitrary threshold\n            return False\n\n        return True\n\n    def get_safe_action(self, state: np.ndarray) -> np.ndarray:\n        """Get safe fallback action"""\n        # Return zero action as safe fallback\n        return np.zeros_like(state[:len(self.joint_limits)])\n'})}),"\n",(0,a.jsx)(e.h2,{id:"examples",children:"Examples"}),"\n",(0,a.jsx)(e.h3,{id:"example-1-complete-sim-to-real-pipeline",children:"Example 1: Complete Sim-to-Real Pipeline"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n# complete_sim_to_real_pipeline.py\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom typing import Dict, Any, List, Tuple\n\nclass CompleteSimToRealPipeline:\n    """\n    Complete pipeline for sim-to-real transfer\n    """\n\n    def __init__(self, sim_env, real_robot):\n        self.sim_env = sim_env\n        self.real_robot = real_robot\n        self.trained_policy = None\n        self.adapted_policy = None\n\n    def train_in_simulation(self, policy_architecture, training_params):\n        """Train policy in simulation"""\n        print("Training policy in simulation...")\n\n        # Create and train policy in simulation environment\n        # This would use the RL training methods from Chapter 5\n        policy = policy_architecture\n\n        # Training loop would go here\n        # For this example, we\'ll assume training is completed\n        self.trained_policy = policy\n\n        print("Simulation training completed")\n\n    def perform_system_identification(self, joint_indices: List[int]):\n        """Perform system identification on real robot"""\n        print("Performing system identification...")\n\n        identifier = SystemIdentifier(self.real_robot)\n        identified_params = identifier.identify_full_model(joint_indices)\n\n        print("Updating simulation with identified parameters...")\n        identifier.update_simulation_model(self.sim_env, identified_params)\n\n        return identified_params\n\n    def collect_real_data(self, num_episodes: int = 10) -> List[Tuple[np.ndarray, np.ndarray, np.ndarray]]:\n        """Collect data from real robot for adaptation"""\n        real_data = []\n\n        for episode in range(num_episodes):\n            state = self.real_robot.reset()\n            done = False\n\n            while not done:\n                # Get action from simulation policy (without noise for data collection)\n                with torch.no_grad():\n                    sim_action_tensor, _ = self.trained_policy(torch.FloatTensor(state).unsqueeze(0))\n                    sim_action = sim_action_tensor.squeeze(0).numpy()\n\n                # Apply action to real robot\n                real_action = self.real_robot.apply_action(sim_action)\n                next_state, reward, done, info = self.real_robot.step(real_action)\n\n                # Store (state, sim_action, real_action) tuple\n                real_data.append((state, sim_action, real_action))\n\n                state = next_state\n\n        return real_data\n\n    def adapt_policy(self, real_data: List[Tuple[np.ndarray, np.ndarray, np.ndarray]]):\n        """Adapt simulation policy using real data"""\n        print("Adapting policy with real data...")\n\n        adapter = PolicyAdapter(self.trained_policy, {})\n        adapter.create_adaptation_network(\n            state_dim=self.sim_env.observation_space.shape[0],\n            action_dim=self.sim_env.action_space.shape[0]\n        )\n\n        adapter.adapt_policy(real_data)\n        self.adapted_policy = adapter\n\n        print("Policy adaptation completed")\n\n    def validate_adapted_policy(self, num_episodes: int = 5) -> Dict[str, float]:\n        """Validate adapted policy on real robot"""\n        print("Validating adapted policy...")\n\n        success_count = 0\n        total_reward = 0\n        episode_lengths = []\n\n        for episode in range(num_episodes):\n            state = self.real_robot.reset()\n            episode_reward = 0\n            step_count = 0\n            done = False\n\n            while not done and step_count < 1000:  # Max 1000 steps per episode\n                # Get action from adapted policy\n                action = self.adapted_policy.get_adapted_action(state)\n\n                # Apply action to real robot\n                next_state, reward, done, info = self.real_robot.step(action)\n\n                state = next_state\n                episode_reward += reward\n                step_count += 1\n\n            total_reward += episode_reward\n            episode_lengths.append(step_count)\n\n            # Check if task was successful (this would depend on specific task)\n            if info.get(\'success\', False):\n                success_count += 1\n\n        results = {\n            \'success_rate\': success_count / num_episodes,\n            \'average_reward\': total_reward / num_episodes,\n            \'average_length\': np.mean(episode_lengths)\n        }\n\n        print(f"Validation results: {results}")\n        return results\n\n    def run_complete_pipeline(self, joint_indices: List[int], num_real_episodes: int = 20):\n        """Run the complete sim-to-real pipeline"""\n        print("Starting complete sim-to-real pipeline...")\n\n        # Step 1: Train in simulation (assumed to be done)\n        # self.train_in_simulation(policy_architecture, training_params)\n\n        # Step 2: System identification\n        identified_params = self.perform_system_identification(joint_indices)\n\n        # Step 3: Collect real data\n        real_data = self.collect_real_data(num_real_episodes)\n\n        # Step 4: Adapt policy\n        self.adapt_policy(real_data)\n\n        # Step 5: Validate adapted policy\n        validation_results = self.validate_adapted_policy()\n\n        print("Complete sim-to-real pipeline finished!")\n        return validation_results\n\ndef main():\n    """Main function for complete sim-to-real pipeline"""\n    # This would integrate with actual simulation and real robot interfaces\n    print("Complete sim-to-real transfer pipeline framework initialized")\n    print("Note: Full implementation requires actual sim and real robot interfaces")\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,a.jsx)(e.h3,{id:"example-2-safety-critical-deployment",children:"Example 2: Safety-Critical Deployment"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n# safety_critical_deployment.py\n\nimport numpy as np\nfrom typing import Dict, Any, List, Tuple\nimport time\nimport threading\nfrom contextlib import contextmanager\n\nclass SafetyManager:\n    """\n    Safety management for real robot deployment\n    """\n\n    def __init__(self, real_robot, max_torque_limits: Dict[int, float],\n                 emergency_stop_callback=None):\n        self.real_robot = real_robot\n        self.max_torque_limits = max_torque_limits\n        self.emergency_stop_callback = emergency_stop_callback\n        self.is_safe_mode = True\n        self.emergency_stop_triggered = False\n\n        # Safety monitoring thread\n        self.monitoring_thread = None\n        self.monitoring_active = False\n\n    def start_safety_monitoring(self):\n        """Start safety monitoring in background thread"""\n        self.monitoring_active = True\n        self.monitoring_thread = threading.Thread(target=self._safety_monitor_loop)\n        self.monitoring_thread.daemon = True\n        self.monitoring_thread.start()\n\n    def stop_safety_monitoring(self):\n        """Stop safety monitoring"""\n        self.monitoring_active = False\n        if self.monitoring_thread:\n            self.monitoring_thread.join()\n\n    def _safety_monitor_loop(self):\n        """Continuous safety monitoring loop"""\n        while self.monitoring_active:\n            try:\n                # Check joint limits\n                if self._check_joint_limits():\n                    self.trigger_emergency_stop("Joint limit violation")\n                    continue\n\n                # Check torque limits\n                if self._check_torque_limits():\n                    self.trigger_emergency_stop("Torque limit violation")\n                    continue\n\n                # Check velocity limits\n                if self._check_velocity_limits():\n                    self.trigger_emergency_stop("Velocity limit violation")\n                    continue\n\n                # Check for hardware faults\n                if self._check_hardware_faults():\n                    self.trigger_emergency_stop("Hardware fault detected")\n                    continue\n\n            except Exception as e:\n                print(f"Safety monitoring error: {e}")\n                self.trigger_emergency_stop("Safety system error")\n\n            time.sleep(0.01)  # 100 Hz monitoring\n\n    def _check_joint_limits(self) -> bool:\n        """Check if any joints are out of safe limits"""\n        # This would interface with real robot to check joint positions\n        return False  # Placeholder\n\n    def _check_torque_limits(self) -> bool:\n        """Check if any joints exceed torque limits"""\n        # This would interface with real robot to check joint torques\n        return False  # Placeholder\n\n    def _check_velocity_limits(self) -> bool:\n        """Check if any joints exceed velocity limits"""\n        # This would interface with real robot to check joint velocities\n        return False  # Placeholder\n\n    def _check_hardware_faults(self) -> bool:\n        """Check for hardware faults"""\n        # This would interface with real robot to check for faults\n        return False  # Placeholder\n\n    def trigger_emergency_stop(self, reason: str):\n        """Trigger emergency stop"""\n        print(f"EMERGENCY STOP: {reason}")\n        self.emergency_stop_triggered = True\n        self.is_safe_mode = True\n\n        # Stop all robot motion\n        self.real_robot.emergency_stop()\n\n        if self.emergency_stop_callback:\n            self.emergency_stop_callback(reason)\n\n    def is_system_safe(self) -> bool:\n        """Check if system is in safe state"""\n        return not self.emergency_stop_triggered and self.is_safe_mode\n\n    @contextmanager\n    def safe_execution(self):\n        """Context manager for safe policy execution"""\n        if not self.is_system_safe():\n            raise RuntimeError("System not in safe state")\n\n        try:\n            self.is_safe_mode = False  # Allow execution\n            yield\n        finally:\n            self.is_safe_mode = True   # Return to safe mode\n            if self.emergency_stop_triggered:\n                raise RuntimeError("Emergency stop was triggered during execution")\n\nclass DeploymentManager:\n    """\n    Manage safe deployment of adapted policies\n    """\n\n    def __init__(self, real_robot, safety_manager: SafetyManager):\n        self.real_robot = real_robot\n        self.safety_manager = safety_manager\n        self.adapted_policy = None\n        self.is_deployed = False\n\n    def deploy_policy(self, adapted_policy, validation_threshold: float = 0.7):\n        """Deploy policy after safety validation"""\n        print("Starting policy deployment...")\n\n        # Validate policy safety\n        if not self._validate_policy_safety(adapted_policy, validation_threshold):\n            raise ValueError("Policy did not pass safety validation")\n\n        # Set the adapted policy\n        self.adapted_policy = adapted_policy\n\n        # Start safety monitoring\n        self.safety_manager.start_safety_monitoring()\n\n        self.is_deployed = True\n        print("Policy deployed successfully")\n\n    def _validate_policy_safety(self, policy, threshold: float) -> bool:\n        """Validate policy safety before deployment"""\n        print("Validating policy safety...")\n\n        # Run safety tests with the policy\n        safety_tests_passed = 0\n        total_tests = 5\n\n        for test_idx in range(total_tests):\n            try:\n                # Run a short safety test\n                test_result = self._run_safety_test(policy)\n                if test_result[\'success\']:\n                    safety_tests_passed += 1\n            except:\n                continue  # Test failed\n\n        success_rate = safety_tests_passed / total_tests\n        print(f"Safety validation: {success_rate:.2f} ({safety_tests_passed}/{total_tests})")\n\n        return success_rate >= threshold\n\n    def _run_safety_test(self, policy) -> Dict[str, Any]:\n        """Run a safety test with the policy"""\n        # Reset robot to safe position\n        self.real_robot.reset_to_safe_position()\n\n        # Run policy for a short duration\n        state = self.real_robot.get_state()\n        for step in range(50):  # 50 steps safety test\n            action = policy.get_adapted_action(state)\n\n            # Check if action is safe\n            if not self._is_action_safe(state, action):\n                return {\'success\': False, \'reason\': \'Unsafe action detected\'}\n\n            # Apply action\n            next_state, reward, done, info = self.real_robot.step(action)\n\n            # Check for safety violations\n            if self.safety_manager.emergency_stop_triggered:\n                return {\'success\': False, \'reason\': \'Emergency stop triggered\'}\n\n            state = next_state\n\n            if done:\n                break\n\n        return {\'success\': True}\n\n    def _is_action_safe(self, state: np.ndarray, action: np.ndarray) -> bool:\n        """Check if action is safe for current state"""\n        # Check torque limits\n        if np.any(np.abs(action) > list(self.safety_manager.max_torque_limits.values())[0:action.shape[0]]):\n            return False\n\n        # Check for potential joint limit violations\n        # This would involve forward simulation\n        return True\n\n    def execute_policy(self, max_duration: float = 60.0):\n        """Execute deployed policy for specified duration"""\n        if not self.is_deployed:\n            raise ValueError("No policy deployed")\n\n        if not self.safety_manager.is_system_safe():\n            raise ValueError("System not in safe state")\n\n        start_time = time.time()\n        step_count = 0\n\n        try:\n            with self.safety_manager.safe_execution():\n                state = self.real_robot.reset()\n\n                while (time.time() - start_time) < max_duration:\n                    if self.safety_manager.emergency_stop_triggered:\n                        break\n\n                    # Get action from adapted policy\n                    action = self.adapted_policy.get_adapted_action(state)\n\n                    # Apply action to robot\n                    next_state, reward, done, info = self.real_robot.step(action)\n\n                    state = next_state\n                    step_count += 1\n\n                    if done:\n                        state = self.real_robot.reset()\n\n                    # Small delay to allow safety systems to respond\n                    time.sleep(0.001)\n\n        except Exception as e:\n            print(f"Policy execution error: {e}")\n            self.safety_manager.trigger_emergency_stop(f"Execution error: {e}")\n\n        execution_time = time.time() - start_time\n        print(f"Policy executed for {execution_time:.2f}s ({step_count} steps)")\n\n    def undeploy_policy(self):\n        """Safely undeploy the policy"""\n        self.safety_manager.stop_safety_monitoring()\n        self.is_deployed = False\n        self.adapted_policy = None\n        print("Policy undeployed safely")\n\ndef main():\n    """Main function for safety-critical deployment"""\n    print("Safety-critical deployment system initialized")\n    print("Note: Full implementation requires real robot interface and safety systems")\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,a.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(e.p,{children:"Sim-to-real transfer is a critical step in deploying simulation-trained policies on real humanoid robots. Key components include:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Domain Randomization"}),": Randomizing simulation parameters to improve policy robustness across different conditions"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"System Identification"}),": Measuring real robot parameters to calibrate simulation models"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Policy Adaptation"}),": Techniques to adapt simulation-trained policies for real robot characteristics"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Safety Management"}),": Ensuring safe deployment and operation of learned policies on real robots"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Validation"}),": Comprehensive testing to ensure policies perform safely and effectively on real hardware"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"exercises",children:"Exercises"}),"\n",(0,a.jsx)(e.h3,{id:"conceptual",children:"Conceptual"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:'Explain the concept of the "reality gap" in robotics and describe three different approaches to address it.'}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"logical",children:"Logical"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Analyze the trade-offs between using domain randomization versus system identification for sim-to-real transfer. When would you choose one approach over the other?"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"implementation-1",children:"Implementation"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Implement a complete sim-to-real pipeline for a simple humanoid reaching task, including domain randomization, system identification, policy adaptation, and safe deployment on a simulated real robot."}),"\n"]})]})}function p(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(c,{...n})}):c(n)}},8453(n,e,i){i.d(e,{R:()=>s,x:()=>r});var t=i(6540);const a={},o=t.createContext(a);function s(n){const e=t.useContext(o);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:s(n.components),t.createElement(o.Provider,{value:e},n.children)}}}]);