"use strict";(globalThis.webpackChunkphysical_ai_book_humanoid=globalThis.webpackChunkphysical_ai_book_humanoid||[]).push([[91],{4141(n,e,a){a.r(e),a.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-3-ai-brain/chapter-4-nav2-planning","title":"Navigation and Path Planning with Isaac ROS","description":"Implementing navigation and path planning systems using Isaac ROS and Nav2","source":"@site/docs/module-3-ai-brain/chapter-4-nav2-planning.md","sourceDirName":"module-3-ai-brain","slug":"/module-3-ai-brain/chapter-4-nav2-planning","permalink":"/textbook/docs/module-3-ai-brain/chapter-4-nav2-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/Panaversity/physical_ai_book_humanoid/tree/main/docs/module-3-ai-brain/chapter-4-nav2-planning.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"title":"Navigation and Path Planning with Isaac ROS","sidebar_position":5,"description":"Implementing navigation and path planning systems using Isaac ROS and Nav2","keywords":["navigation","path planning","nav2","robotics","Isaac ROS","localization","mapping"]},"sidebar":"tutorialSidebar","previous":{"title":"Perception Pipelines for Robotics AI","permalink":"/textbook/docs/module-3-ai-brain/chapter-3-perception-pipelines"},"next":{"title":"Reinforcement Learning for Robotics with Isaac Sim","permalink":"/textbook/docs/module-3-ai-brain/chapter-5-reinforcement-learning"}}');var i=a(4848),s=a(8453);const o={title:"Navigation and Path Planning with Isaac ROS",sidebar_position:5,description:"Implementing navigation and path planning systems using Isaac ROS and Nav2",keywords:["navigation","path planning","nav2","robotics","Isaac ROS","localization","mapping"]},r="Chapter 4: Navigation and Path Planning with Isaac ROS",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Core Concepts",id:"core-concepts",level:2},{value:"Navigation System Architecture",id:"navigation-system-architecture",level:3},{value:"Isaac ROS Navigation Framework",id:"isaac-ros-navigation-framework",level:3},{value:"Implementation",id:"implementation",level:2},{value:"Isaac ROS Navigation Setup",id:"isaac-ros-navigation-setup",level:3},{value:"Isaac ROS Navigation with Perception Integration",id:"isaac-ros-navigation-with-perception-integration",level:3},{value:"Examples",id:"examples",level:2},{value:"Example 1: Isaac ROS Path Planning with Dynamic Obstacle Avoidance",id:"example-1-isaac-ros-path-planning-with-dynamic-obstacle-avoidance",level:3},{value:"Example 2: Navigation Performance Validation",id:"example-2-navigation-performance-validation",level:3},{value:"Summary",id:"summary",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Conceptual",id:"conceptual",level:3},{value:"Logical",id:"logical",level:3},{value:"Implementation",id:"implementation-1",level:3}];function p(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"chapter-4-navigation-and-path-planning-with-isaac-ros",children:"Chapter 4: Navigation and Path Planning with Isaac ROS"})}),"\n",(0,i.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(e.p,{children:"By the end of this chapter, students will be able to:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Implement navigation systems using Isaac ROS and Nav2 for humanoid robots"}),"\n",(0,i.jsx)(e.li,{children:"Configure localization, mapping, and path planning algorithms"}),"\n",(0,i.jsx)(e.li,{children:"Design costmap layers and obstacle avoidance strategies for humanoid navigation"}),"\n",(0,i.jsx)(e.li,{children:"Integrate perception data with navigation for robust path planning"}),"\n",(0,i.jsx)(e.li,{children:"Validate navigation performance in complex environments"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsx)(e.p,{children:"Students should have:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Understanding of mobile robot navigation concepts (localization, mapping, path planning)"}),"\n",(0,i.jsx)(e.li,{children:"Knowledge of perception systems (covered in Chapter 3)"}),"\n",(0,i.jsx)(e.li,{children:"Experience with ROS 2 navigation stack (Nav2)"}),"\n",(0,i.jsx)(e.li,{children:"Basic understanding of control theory for motion execution"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"core-concepts",children:"Core Concepts"}),"\n",(0,i.jsx)(e.p,{children:"Navigation systems enable humanoid robots to autonomously move through environments safely and efficiently. Isaac ROS provides GPU-accelerated navigation capabilities that integrate with perception systems for robust autonomous navigation."}),"\n",(0,i.jsx)(e.h3,{id:"navigation-system-architecture",children:"Navigation System Architecture"}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Localization Layer:"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"AMCL (Adaptive Monte Carlo Localization) for pose estimation"}),"\n",(0,i.jsx)(e.li,{children:"Visual-inertial odometry for position tracking"}),"\n",(0,i.jsx)(e.li,{children:"Sensor fusion for robust pose estimation"}),"\n",(0,i.jsx)(e.li,{children:"Global and local map management"}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Mapping Layer:"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Occupancy grid mapping from sensor data"}),"\n",(0,i.jsx)(e.li,{children:"Costmap construction for obstacle representation"}),"\n",(0,i.jsx)(e.li,{children:"Dynamic obstacle tracking and prediction"}),"\n",(0,i.jsx)(e.li,{children:"Map updates and maintenance"}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Path Planning Layer:"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Global planner for long-term path computation"}),"\n",(0,i.jsx)(e.li,{children:"Local planner for obstacle avoidance and trajectory execution"}),"\n",(0,i.jsx)(e.li,{children:"Recovery behaviors for navigation failures"}),"\n",(0,i.jsx)(e.li,{children:"Dynamic path replanning for changing environments"}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Motion Execution Layer:"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Velocity controllers for smooth motion execution"}),"\n",(0,i.jsx)(e.li,{children:"Footstep planning for bipedal humanoid navigation"}),"\n",(0,i.jsx)(e.li,{children:"Balance control during motion execution"}),"\n",(0,i.jsx)(e.li,{children:"Integration with robot dynamics"}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"isaac-ros-navigation-framework",children:"Isaac ROS Navigation Framework"}),"\n",(0,i.jsx)(e.p,{children:"Isaac ROS provides specialized navigation capabilities:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Isaac ROS Nav2"}),": GPU-accelerated navigation algorithms"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Isaac ROS Occupancy Grids"}),": High-resolution mapping"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Isaac ROS Path Planners"}),": Optimized planning algorithms"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Isaac ROS Controllers"}),": Specialized motion controllers for humanoid robots"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"implementation",children:"Implementation"}),"\n",(0,i.jsx)(e.p,{children:"Let's implement a comprehensive navigation system using Isaac ROS and Nav2:"}),"\n",(0,i.jsx)(e.h3,{id:"isaac-ros-navigation-setup",children:"Isaac ROS Navigation Setup"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n# navigation_system.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom nav_msgs.msg import OccupancyGrid, Path, Odometry\nfrom geometry_msgs.msg import PoseStamped, Twist\nfrom sensor_msgs.msg import LaserScan, PointCloud2\nfrom std_msgs.msg import String\nfrom tf2_ros import TransformListener, Buffer\nimport numpy as np\nimport math\nfrom scipy.spatial import distance\nfrom geometry_msgs.msg import Point\nfrom visualization_msgs.msg import Marker, MarkerArray\n\nclass IsaacNavigationSystem(Node):\n    """\n    Navigation system using Isaac ROS and Nav2 for humanoid robot\n    """\n\n    def __init__(self):\n        super().__init__(\'isaac_navigation_system\')\n\n        # Publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/humanoid/cmd_vel\', 10)\n        self.goal_pub = self.create_publisher(PoseStamped, \'/humanoid/goal_pose\', 10)\n        self.path_pub = self.create_publisher(Path, \'/humanoid/planned_path\', 10)\n        self.status_pub = self.create_publisher(String, \'/humanoid/navigation/status\', 10)\n        self.debug_markers_pub = self.create_publisher(MarkerArray, \'/humanoid/navigation/debug_markers\', 10)\n\n        # Subscribers\n        self.odom_sub = self.create_subscription(\n            Odometry,\n            \'/humanoid/odom\',\n            self.odom_callback,\n            10\n        )\n\n        self.scan_sub = self.create_subscription(\n            LaserScan,\n            \'/humanoid/lidar/scan\',\n            self.scan_callback,\n            10\n        )\n\n        self.map_sub = self.create_subscription(\n            OccupancyGrid,\n            \'/humanoid/map\',\n            self.map_callback,\n            10\n        )\n\n        # TF buffer and listener\n        self.tf_buffer = Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n\n        # Navigation state\n        self.current_pose = None\n        self.current_goal = None\n        self.global_path = []\n        self.local_plan = []\n        self.obstacles = []\n        self.map_data = None\n\n        # Navigation parameters\n        self.linear_speed = 0.3  # m/s\n        self.angular_speed = 0.5  # rad/s\n        self.goal_tolerance = 0.5  # meters\n        self.obstacle_threshold = 0.7  # meters for obstacle detection\n        self.replan_distance = 2.0  # distance at which to replan\n\n        # Timers\n        self.navigation_timer = self.create_timer(0.1, self.navigation_loop)  # 10 Hz\n        self.path_planner_timer = self.create_timer(1.0, self.plan_path)  # Plan path periodically\n\n        # Navigation modes\n        self.navigation_active = False\n        self.avoiding_obstacles = False\n\n        self.get_logger().info(\'Isaac Navigation System initialized\')\n\n    def odom_callback(self, msg):\n        """Update robot pose from odometry"""\n        self.current_pose = msg.pose.pose\n\n    def scan_callback(self, msg):\n        """Process laser scan for obstacle detection"""\n        # Convert scan to obstacle points in robot frame\n        angles = np.linspace(msg.angle_min, msg.angle_max, len(msg.ranges))\n        ranges = np.array(msg.ranges)\n\n        # Filter valid ranges\n        valid_indices = np.isfinite(ranges) & (ranges < msg.range_max) & (ranges > msg.range_min)\n        valid_angles = angles[valid_indices]\n        valid_ranges = ranges[valid_indices]\n\n        # Convert to Cartesian coordinates\n        obstacle_x = valid_ranges * np.cos(valid_angles)\n        obstacle_y = valid_ranges * np.sin(valid_angles)\n\n        # Store obstacles\n        self.obstacles = list(zip(obstacle_x, obstacle_y))\n\n    def map_callback(self, msg):\n        """Update internal map representation"""\n        self.map_data = {\n            \'info\': msg.info,\n            \'data\': np.array(msg.data).reshape(msg.info.height, msg.info.width),\n            \'resolution\': msg.info.resolution,\n            \'origin\': (msg.info.origin.position.x, msg.info.origin.position.y)\n        }\n\n    def set_goal(self, x, y, theta=0.0):\n        """Set navigation goal"""\n        goal_msg = PoseStamped()\n        goal_msg.header.stamp = self.get_clock().now().to_msg()\n        goal_msg.header.frame_id = \'map\'\n        goal_msg.pose.position.x = x\n        goal_msg.pose.position.y = y\n        goal_msg.pose.position.z = 0.0\n\n        # Convert angle to quaternion\n        goal_msg.pose.orientation.z = math.sin(theta / 2.0)\n        goal_msg.pose.orientation.w = math.cos(theta / 2.0)\n\n        self.current_goal = goal_msg.pose\n        self.navigation_active = True\n        self.get_logger().info(f\'Set navigation goal to ({x:.2f}, {y:.2f})\')\n\n    def navigation_loop(self):\n        """Main navigation control loop"""\n        if not self.current_pose or not self.current_goal or not self.navigation_active:\n            return\n\n        try:\n            # Calculate distance to goal\n            dx = self.current_goal.position.x - self.current_pose.position.x\n            dy = self.current_goal.position.y - self.current_pose.position.y\n            dist_to_goal = math.sqrt(dx*dx + dy*dy)\n\n            # Check if goal reached\n            if dist_to_goal < self.goal_tolerance:\n                self.navigation_active = False\n                self.get_logger().info(\'Goal reached!\')\n\n                # Stop robot\n                cmd_vel = Twist()\n                self.cmd_vel_pub.publish(cmd_vel)\n                return\n\n            # Check for obstacles in path\n            obstacle_in_path = self.check_obstacles_in_path()\n\n            if obstacle_in_path:\n                self.get_logger().warn(\'Obstacle in path, initiating avoidance...\')\n                self.avoiding_obstacles = True\n                self.execute_obstacle_avoidance()\n            else:\n                self.avoiding_obstacles = False\n                self.follow_path()\n\n            # Update status\n            status_msg = String()\n            status_msg.data = f"Distance to goal: {dist_to_goal:.2f}m, Obstacles: {len(self.obstacles)}, Active: {self.navigation_active}"\n            self.status_pub.publish(status_msg)\n\n        except Exception as e:\n            self.get_logger().error(f\'Error in navigation loop: {e}\')\n\n    def check_obstacles_in_path(self):\n        """Check if there are obstacles blocking the current path"""\n        if not self.global_path or not self.obstacles:\n            return False\n\n        # Check obstacles near the robot\n        robot_x = self.current_pose.position.x\n        robot_y = self.current_pose.position.y\n\n        for obs_x, obs_y in self.obstacles:\n            # Convert obstacle from robot frame to global frame\n            # (In a real implementation, this would use TF transforms)\n            global_obs_x = robot_x + obs_x\n            global_obs_y = robot_y + obs_y\n\n            # Check if obstacle is near the robot and in the general direction of movement\n            obs_dist = math.sqrt((global_obs_x - robot_x)**2 + (global_obs_y - robot_y)**2)\n\n            if obs_dist < self.obstacle_threshold:\n                # Calculate angle between robot-to-goal and robot-to-obstacle\n                goal_angle = math.atan2(dy, dx) if (dx != 0 or dy != 0) else 0\n                obs_angle = math.atan2(global_obs_y - robot_y, global_obs_x - robot_x)\n\n                # If obstacle is within 45 degrees of goal direction\n                angle_diff = abs(goal_angle - obs_angle)\n                if angle_diff > math.pi:\n                    angle_diff = 2*math.pi - angle_diff\n\n                if angle_diff < math.pi/4:  # Within 45 degrees\n                    return True\n\n        return False\n\n    def execute_obstacle_avoidance(self):\n        """Execute obstacle avoidance behavior"""\n        cmd_vel = Twist()\n\n        # Simple obstacle avoidance: turn away from nearest obstacle\n        if self.obstacles:\n            # Find nearest obstacle\n            nearest_obs = min(self.obstacles, key=lambda obs: math.sqrt(obs[0]**2 + obs[1]**2))\n            obs_x, obs_y = nearest_obs\n\n            # Calculate avoidance direction (turn away from obstacle)\n            avoidance_angle = math.atan2(obs_y, obs_x) + math.pi  # Opposite direction\n            cmd_vel.angular.z = self.angular_speed * math.sin(avoidance_angle)\n\n        # Move forward slowly during avoidance\n        cmd_vel.linear.x = self.linear_speed * 0.5\n\n        self.cmd_vel_pub.publish(cmd_vel)\n\n    def follow_path(self):\n        """Follow the global path to the goal"""\n        cmd_vel = Twist()\n\n        if self.global_path:\n            # Simple path following: move toward next waypoint\n            next_waypoint = self.global_path[0]  # In a real implementation, find closest point ahead\n\n            # Calculate direction to next waypoint\n            dx = next_waypoint.pose.position.x - self.current_pose.position.x\n            dy = next_waypoint.pose.position.y - self.current_pose.position.y\n\n            # Calculate distance to waypoint\n            dist_to_wp = math.sqrt(dx*dx + dy*dy)\n\n            # Calculate angle to waypoint\n            angle_to_wp = math.atan2(dy, dx)\n\n            # Current robot orientation\n            robot_yaw = 2 * math.asin(self.current_pose.orientation.z)  # Simplified for z-axis rotation\n\n            # Angular error\n            angle_error = angle_to_wp - robot_yaw\n            while angle_error > math.pi:\n                angle_error -= 2*math.pi\n            while angle_error < -math.pi:\n                angle_error += 2*math.pi\n\n            # Proportional controller for angular velocity\n            cmd_vel.angular.z = 0.5 * angle_error  # Limit angular velocity\n\n            # Linear velocity proportional to progress toward goal\n            cmd_vel.linear.x = min(self.linear_speed, max(0.1, dist_to_wp * 0.5))\n\n        self.cmd_vel_pub.publish(cmd_vel)\n\n    def plan_path(self):\n        """Plan path from current position to goal using a simple algorithm"""\n        if not self.current_pose or not self.current_goal or not self.navigation_active:\n            return\n\n        # In a real implementation, this would call Nav2 path planner\n        # For this example, we\'ll use a simple straight-line path with obstacle avoidance\n\n        # Calculate straight-line path\n        start_x = self.current_pose.position.x\n        start_y = self.current_pose.position.y\n        goal_x = self.current_goal.position.x\n        goal_y = self.current_goal.position.y\n\n        # Create waypoints along straight line\n        num_waypoints = max(10, int(math.sqrt((goal_x-start_x)**2 + (goal_y-start_y)**2) / 0.5))\n\n        path = []\n        for i in range(num_waypoints + 1):\n            t = i / num_waypoints\n            wp_x = start_x + t * (goal_x - start_x)\n            wp_y = start_y + t * (goal_y - start_y)\n\n            # Create pose for waypoint\n            wp_pose = PoseStamped()\n            wp_pose.header.stamp = self.get_clock().now().to_msg()\n            wp_pose.header.frame_id = \'map\'\n            wp_pose.pose.position.x = wp_x\n            wp_pose.pose.position.y = wp_y\n            wp_pose.pose.position.z = 0.0\n\n            path.append(wp_pose)\n\n        self.global_path = path\n\n        # Publish path for visualization\n        path_msg = Path()\n        path_msg.header.stamp = self.get_clock().now().to_msg()\n        path_msg.header.frame_id = \'map\'\n        path_msg.poses = [wp for wp in path]\n\n        self.path_pub.publish(path_msg)\n\n    def visualize_navigation_elements(self):\n        """Visualize navigation elements in RViz"""\n        marker_array = MarkerArray()\n\n        # Visualize current path\n        if self.global_path:\n            path_marker = Marker()\n            path_marker.header.stamp = self.get_clock().now().to_msg()\n            path_marker.header.frame_id = \'map\'\n            path_marker.ns = \'navigation_path\'\n            path_marker.id = 0\n            path_marker.type = Marker.LINE_STRIP\n            path_marker.action = Marker.ADD\n\n            path_marker.scale.x = 0.05  # Line width\n            path_marker.color.r = 0.0\n            path_marker.color.g = 1.0\n            path_marker.color.b = 0.0\n            path_marker.color.a = 0.8\n\n            for wp in self.global_path:\n                point = Point()\n                point.x = wp.pose.position.x\n                point.y = wp.pose.position.y\n                point.z = 0.1  # Slightly above ground\n                path_marker.points.append(point)\n\n            marker_array.markers.append(path_marker)\n\n        # Visualize obstacles\n        for i, (obs_x, obs_y) in enumerate(self.obstacles):\n            if math.sqrt(obs_x**2 + obs_y**2) < 5.0:  # Only visualize nearby obstacles\n                obs_marker = Marker()\n                obs_marker.header.stamp = self.get_clock().now().to_msg()\n                obs_marker.header.frame_id = \'base_link\'  # Robot frame\n                obs_marker.ns = \'obstacles\'\n                obs_marker.id = i + 100  # Offset to avoid conflicts\n                obs_marker.type = Marker.SPHERE\n                obs_marker.action = Marker.ADD\n\n                obs_marker.pose.position.x = obs_x\n                obs_marker.pose.position.y = obs_y\n                obs_marker.pose.position.z = 0.5  # Half a meter high\n                obs_marker.pose.orientation.w = 1.0\n\n                obs_marker.scale.x = 0.2\n                obs_marker.scale.y = 0.2\n                obs_marker.scale.z = 0.2\n\n                obs_marker.color.r = 1.0  # Red for obstacles\n                obs_marker.color.g = 0.0\n                obs_marker.color.b = 0.0\n                obs_marker.color.a = 0.8\n\n                marker_array.markers.append(obs_marker)\n\n        self.debug_markers_pub.publish(marker_array)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    nav_node = IsaacNavigationSystem()\n\n    # Example: Set a goal after initialization\n    nav_node.set_goal(5.0, 5.0, 0.0)  # Navigate to (5m, 5m)\n\n    try:\n        rclpy.spin(nav_node)\n    except KeyboardInterrupt:\n        nav_node.get_logger().info(\'Shutting down Isaac Navigation System...\')\n    finally:\n        nav_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(e.h3,{id:"isaac-ros-navigation-with-perception-integration",children:"Isaac ROS Navigation with Perception Integration"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n# perception_navigation_integration.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, LaserScan\nfrom nav_msgs.msg import OccupancyGrid, Path\nfrom geometry_msgs.msg import PoseStamped, Twist\nfrom vision_msgs.msg import Detection2DArray\nfrom std_msgs.msg import String\nimport numpy as np\nfrom scipy.spatial.distance import cdist\nimport tf2_ros\n\nclass PerceptionNavigationIntegration(Node):\n    \"\"\"\n    Integrate perception and navigation systems for robust autonomous navigation\n    \"\"\"\n\n    def __init__(self):\n        super().__init__('perception_navigation_integration')\n\n        # Publishers\n        self.nav_cmd_pub = self.create_publisher(Twist, '/humanoid/cmd_vel', 10)\n        self.nav_status_pub = self.create_publisher(String, '/humanoid/nav_perception/status', 10)\n\n        # Subscribers\n        self.detection_sub = self.create_subscription(\n            Detection2DArray,\n            '/humanoid/perception/detections',\n            self.detection_callback,\n            10\n        )\n\n        self.scan_sub = self.create_subscription(\n            LaserScan,\n            '/humanoid/lidar/scan',\n            self.scan_callback,\n            10\n        )\n\n        self.odom_sub = self.create_subscription(\n            Odometry,\n            '/humanoid/odom',\n            self.odom_callback,\n            10\n        )\n\n        # TF buffer\n        self.tf_buffer = tf2_ros.Buffer()\n        self.tf_listener = tf2_ros.TransformListener(self.tf_buffer, self)\n\n        # Navigation and perception state\n        self.current_pose = None\n        self.detected_objects = []\n        self.laser_obstacles = []\n        self.navigation_goals = []\n        self.active_goal = None\n\n        # Integration parameters\n        self.object_tracking_horizon = 5.0  # seconds to track objects\n        self.dynamic_costmap_inflation = 1.0  # inflation factor for dynamic obstacles\n        self.safety_margin = 0.8  # safety margin around detected objects\n\n        # Timers\n        self.integration_timer = self.create_timer(0.1, self.integrate_perception_navigation)\n        self.safety_timer = self.create_timer(0.05, self.safety_check)\n\n        self.get_logger().info('Perception-Navigation Integration initialized')\n\n    def detection_callback(self, msg):\n        \"\"\"Process perception detections and update navigation awareness\"\"\"\n        # Update detected objects list\n        self.detected_objects = []\n\n        for detection in msg.detections:\n            for result in detection.results:\n                # Get object position from detection\n                # In a real implementation, this would require depth information or stereo triangulation\n                # For this example, we'll simulate object positions based on bounding box\n                center_x = detection.bbox.center.x\n                center_y = detection.bbox.center.y\n\n                # Convert image coordinates to world coordinates (simplified)\n                # In reality, this would use depth data and camera calibration\n                world_x = center_x * 0.01  # Rough conversion\n                world_y = center_y * 0.01  # Rough conversion\n\n                obj_info = {\n                    'id': result.id,\n                    'score': result.score,\n                    'position': (world_x, world_y),\n                    'timestamp': self.get_clock().now(),\n                    'bbox': detection.bbox\n                }\n\n                self.detected_objects.append(obj_info)\n\n    def scan_callback(self, msg):\n        \"\"\"Process laser scan data\"\"\"\n        # Process laser scan for static obstacle detection\n        angles = np.linspace(msg.angle_min, msg.angle_max, len(msg.ranges))\n        ranges = np.array(msg.ranges)\n\n        # Filter valid ranges\n        valid_mask = np.isfinite(ranges) & (ranges > msg.range_min) & (ranges < msg.range_max)\n        self.laser_obstacles = list(zip(ranges[valid_mask], angles[valid_mask]))\n\n    def odom_callback(self, msg):\n        \"\"\"Update robot pose from odometry\"\"\"\n        self.current_pose = msg.pose.pose\n\n    def integrate_perception_navigation(self):\n        \"\"\"Integrate perception data with navigation decisions\"\"\"\n        if not self.current_pose:\n            return\n\n        try:\n            # Update navigation based on perceived environment\n            self.update_navigation_with_perception()\n\n            # Adjust path planning based on dynamic obstacles\n            self.adjust_path_for_dynamic_obstacles()\n\n            # Update costmap with perception data\n            self.update_dynamic_costmap()\n\n        except Exception as e:\n            self.get_logger().error(f'Error in perception-navigation integration: {e}')\n\n    def update_navigation_with_perception(self):\n        \"\"\"Update navigation behavior based on perception data\"\"\"\n        # Check for humans or moving objects that require special consideration\n        humans_nearby = [obj for obj in self.detected_objects\n                        if obj['id'] == 1 and obj['score'] > 0.7]  # Assuming ID 1 is 'person'\n\n        if humans_nearby:\n            # Reduce speed and increase safety margins when humans are nearby\n            self.get_logger().info(f'Humans detected nearby: {len(humans_nearby)} persons')\n\n            # Increase safety margin for path planning\n            self.safety_margin = 1.2  # Increase from 0.8 to 1.2m when humans are nearby\n\n        # Check for other important objects (tables, chairs) for navigation planning\n        furniture = [obj for obj in self.detected_objects\n                    if obj['id'] in [2, 3] and obj['score'] > 0.6]  # Assuming IDs 2,3 are furniture\n\n        if furniture:\n            self.get_logger().info(f'Furniture detected: {len(furniture)} items')\n\n    def adjust_path_for_dynamic_obstacles(self):\n        \"\"\"Adjust planned path to account for dynamic obstacles\"\"\"\n        # In a real implementation, this would modify the global/local planner\n        # to account for detected dynamic obstacles\n        # For this example, we'll log the adjustment\n\n        dynamic_obstacles = [obj for obj in self.detected_objects\n                            if obj['timestamp'].nanoseconds > (self.get_clock().now().nanoseconds - 5e9)]  # Last 5 seconds\n\n        if dynamic_obstacles:\n            self.get_logger().info(f'Adjusting path for {len(dynamic_obstacles)} dynamic obstacles')\n\n    def update_dynamic_costmap(self):\n        \"\"\"Update costmap with perception-derived obstacle information\"\"\"\n        # This would update the navigation costmap with information from perception system\n        # In a real Isaac ROS implementation, this would interface with the Nav2 costmap\n\n        # For this example, we'll just log the activity\n        perception_obstacles = len(self.detected_objects)\n        laser_obstacles = len(self.laser_obstacles)\n\n        self.get_logger().info(f'Costmap update: {perception_obstacles} perception obstacles, {laser_obstacles} laser obstacles')\n\n    def safety_check(self):\n        \"\"\"Perform safety checks combining perception and navigation data\"\"\"\n        if not self.current_pose:\n            return\n\n        # Check for immediate collision risks\n        immediate_risks = []\n\n        # Check laser-based obstacles\n        for range_val, angle in self.laser_obstacles:\n            if range_val < 0.5:  # Less than 50cm\n                # Calculate position of obstacle in world frame\n                robot_yaw = 2 * math.asin(self.current_pose.orientation.z)\n                world_angle = robot_yaw + angle\n                obs_x = self.current_pose.position.x + range_val * math.cos(world_angle)\n                obs_y = self.current_pose.position.y + range_val * math.sin(world_angle)\n\n                immediate_risks.append(('laser', range_val, (obs_x, obs_y)))\n\n        # Check perception-based obstacles\n        for obj in self.detected_objects:\n            if obj['score'] > 0.8:  # High confidence detection\n                dist_to_obj = math.sqrt(\n                    (obj['position'][0] - self.current_pose.position.x)**2 +\n                    (obj['position'][1] - self.current_pose.position.y)**2\n                )\n\n                if dist_to_obj < 1.0:  # Within 1 meter\n                    immediate_risks.append(('perception', dist_to_obj, obj['position']))\n\n        # Handle immediate risks\n        if immediate_risks:\n            closest_risk = min(immediate_risks, key=lambda x: x[1])\n            risk_type, dist, pos = closest_risk\n\n            self.get_logger().warn(f'IMMEDIATE RISK: {risk_type} obstacle at {dist:.2f}m - stopping navigation')\n\n            # Emergency stop\n            cmd_vel = Twist()\n            self.nav_cmd_pub.publish(cmd_vel)\n\n    def publish_integration_status(self):\n        \"\"\"Publish status of perception-navigation integration\"\"\"\n        status_msg = String()\n        status_msg.data = f\"Perception-Navigation Integration: {len(self.detected_objects)} objects, \" \\\n                         f\"{len(self.laser_obstacles)} laser obstacles, \" \\\n                         f\"Navigation active: {bool(self.active_goal)}\"\n        self.nav_status_pub.publish(status_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    integration_node = PerceptionNavigationIntegration()\n\n    try:\n        rclpy.spin(integration_node)\n    except KeyboardInterrupt:\n        integration_node.get_logger().info('Shutting down Perception-Navigation Integration...')\n    finally:\n        integration_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(e.h2,{id:"examples",children:"Examples"}),"\n",(0,i.jsx)(e.h3,{id:"example-1-isaac-ros-path-planning-with-dynamic-obstacle-avoidance",children:"Example 1: Isaac ROS Path Planning with Dynamic Obstacle Avoidance"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n# dynamic_path_planning.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import PoseStamped, Twist\nfrom sensor_msgs.msg import LaserScan\nfrom visualization_msgs.msg import MarkerArray\nfrom std_msgs.msg import String\nimport numpy as np\nfrom scipy.spatial.distance import cdist\nimport math\n\nclass DynamicPathPlanner(Node):\n    """\n    Dynamic path planning with real-time obstacle avoidance using Isaac ROS\n    """\n\n    def __init__(self):\n        super().__init__(\'dynamic_path_planner\')\n\n        # Publishers\n        self.path_cmd_pub = self.create_publisher(PoseStamped, \'/humanoid/path_goal\', 10)\n        self.vel_cmd_pub = self.create_publisher(Twist, \'/humanoid/cmd_vel\', 10)\n        self.debug_pub = self.create_publisher(MarkerArray, \'/humanoid/path_planning/debug\', 10)\n\n        # Subscribers\n        self.scan_sub = self.create_subscription(\n            LaserScan,\n            \'/humanoid/lidar/scan\',\n            self.scan_callback,\n            10\n        )\n\n        # Navigation state\n        self.current_pose = None\n        self.current_goal = None\n        self.waypoints = []\n        self.dynamic_obstacles = []\n        self.obstacle_velocities = {}  # Track obstacle velocities for prediction\n\n        # Path planning parameters\n        self.lookahead_distance = 2.0  # meters ahead to plan\n        self.obstacle_buffer = 0.6     # buffer distance around obstacles\n        self.max_linear_speed = 0.4    # max linear speed\n        self.max_angular_speed = 0.6   # max angular speed\n\n        # Timers\n        self.path_update_timer = self.create_timer(0.2, self.update_dynamic_path)\n        self.motion_control_timer = self.create_timer(0.1, self.motion_control)\n\n        self.get_logger().info(\'Dynamic Path Planner initialized\')\n\n    def scan_callback(self, msg):\n        """Process laser scan to detect and track dynamic obstacles"""\n        # Convert scan to obstacle positions\n        angles = np.linspace(msg.angle_min, msg.angle_max, len(msg.ranges))\n        ranges = np.array(msg.ranges)\n\n        # Filter valid ranges\n        valid_mask = np.isfinite(ranges) & (ranges > msg.range_min) & (ranges < msg.range_max)\n        valid_ranges = ranges[valid_mask]\n        valid_angles = angles[valid_mask]\n\n        # Convert to Cartesian coordinates relative to robot\n        obstacle_x = valid_ranges * np.cos(valid_angles)\n        obstacle_y = valid_ranges * np.sin(valid_angles)\n        current_obstacles = list(zip(obstacle_x, obstacle_y))\n\n        # Update dynamic obstacle tracking\n        self.update_obstacle_tracking(current_obstacles)\n\n    def update_obstacle_tracking(self, current_obstacles):\n        """Track obstacles over time to estimate velocities"""\n        # For simplicity, we\'ll just store the current obstacles\n        # In a real implementation, we would track obstacles across multiple scans\n        # to estimate their velocities and predict future positions\n        self.dynamic_obstacles = current_obstacles\n\n    def update_dynamic_path(self):\n        """Update path considering dynamic obstacles"""\n        if not self.current_goal:\n            return\n\n        # Plan path considering current obstacle positions\n        # In a real implementation, this would use a dynamic path planning algorithm\n        # like Dynamic Window Approach (DWA) or Time Elastic Bands (TEB)\n\n        # For this example, we\'ll implement a simple reactive approach\n        self.reactive_path_adjustment()\n\n    def reactive_path_adjustment(self):\n        """Reactive path adjustment based on current obstacles"""\n        if not self.current_pose or not self.current_goal:\n            return\n\n        # Calculate direct path to goal\n        dx = self.current_goal.position.x - self.current_pose.position.x\n        dy = self.current_goal.position.y - self.current_pose.position.y\n        direct_distance = math.sqrt(dx*dx + dy*dy)\n\n        # Check if path is blocked by obstacles\n        if self.check_path_blocked(dx, dy, direct_distance):\n            self.get_logger().info(\'Direct path blocked, calculating alternative route\')\n            # In a real implementation, this would calculate a new path around obstacles\n            # For now, we\'ll just adjust heading to avoid immediate obstacles\n            self.calculate_avoidance_direction()\n\n    def check_path_blocked(self, dx, dy, distance):\n        """Check if the direct path to goal is blocked by obstacles"""\n        # Simple check: if obstacles are within a cone in front of the robot\n        robot_yaw = 2 * math.asin(self.current_pose.orientation.z)\n\n        for obs_x, obs_y in self.dynamic_obstacles:\n            # Convert obstacle from robot frame to direction relative to goal\n            obs_angle = math.atan2(obs_y, obs_x)\n            obs_dist = math.sqrt(obs_x*obs_x + obs_y*obs_y)\n\n            # Check if obstacle is in front of robot and roughly in direction of goal\n            angle_diff = abs(obs_angle - robot_yaw)\n            if angle_diff > math.pi:\n                angle_diff = 2*math.pi - angle_diff\n\n            if obs_dist < 1.0 and angle_diff < math.pi/3:  # Within 1m and 60-degree cone\n                return True\n\n        return False\n\n    def calculate_avoidance_direction(self):\n        """Calculate direction to avoid obstacles"""\n        if not self.dynamic_obstacles:\n            return\n\n        # Find the clearest direction to move\n        best_direction = 0.0\n        max_clear_distance = 0.0\n\n        # Sample different directions\n        for angle_offset in np.linspace(-math.pi/2, math.pi/2, 9):\n            # Check clearance in this direction\n            direction_yaw = 2 * math.asin(self.current_pose.orientation.z) + angle_offset\n            test_x = math.cos(direction_yaw)\n            test_y = math.sin(direction_yaw)\n\n            # Find closest obstacle in this direction\n            min_dist = float(\'inf\')\n            for obs_x, obs_y in self.dynamic_obstacles:\n                # Project obstacle onto test direction\n                proj = obs_x * test_x + obs_y * test_y\n                if proj > 0:  # Only consider obstacles ahead\n                    perp_dist = abs(obs_x * test_y - obs_y * test_x)  # Perpendicular distance\n                    if perp_dist < 0.5:  # Within corridor width\n                        dist = math.sqrt(obs_x*obs_x + obs_y*obs_y)\n                        min_dist = min(min_dist, dist)\n\n            if min_dist > max_clear_distance:\n                max_clear_distance = min_dist\n                best_direction = angle_offset\n\n        # Adjust robot heading toward clearest direction\n        cmd_vel = Twist()\n        cmd_vel.angular.z = max(-self.max_angular_speed, min(self.max_angular_speed, best_direction))\n        cmd_vel.linear.x = self.max_linear_speed * 0.5  # Move forward at reduced speed\n\n        self.vel_cmd_pub.publish(cmd_vel)\n\n    def motion_control(self):\n        """Low-level motion control based on path planning"""\n        if not self.current_pose or not self.current_goal:\n            return\n\n        # Calculate control commands to follow planned path\n        cmd_vel = Twist()\n\n        # Calculate direction to goal\n        dx = self.current_goal.position.x - self.current_pose.position.x\n        dy = self.current_goal.position.y - self.current_pose.position.y\n        distance_to_goal = math.sqrt(dx*dx + dy*dy)\n\n        if distance_to_goal > 0.2:  # Not at goal\n            # Calculate desired heading\n            desired_yaw = math.atan2(dy, dx)\n            current_yaw = 2 * math.asin(self.current_pose.orientation.z)\n\n            # Calculate angular error\n            angle_error = desired_yaw - current_yaw\n            while angle_error > math.pi:\n                angle_error -= 2*math.pi\n            while angle_error < -math.pi:\n                angle_error += 2*math.pi\n\n            # Proportional control for angular velocity\n            cmd_vel.angular.z = max(-self.max_angular_speed, min(self.max_angular_speed, angle_error * 1.0))\n\n            # Move forward if facing approximately the right direction\n            if abs(angle_error) < math.pi/4:\n                cmd_vel.linear.x = min(self.max_linear_speed, distance_to_goal * 0.5)\n\n        self.vel_cmd_pub.publish(cmd_vel)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    planner = DynamicPathPlanner()\n\n    try:\n        rclpy.spin(planner)\n    except KeyboardInterrupt:\n        planner.get_logger().info(\'Shutting down Dynamic Path Planner...\')\n    finally:\n        planner.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(e.h3,{id:"example-2-navigation-performance-validation",children:"Example 2: Navigation Performance Validation"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n# navigation_performance_validator.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom nav_msgs.msg import Path, Odometry\nfrom geometry_msgs.msg import PoseStamped, Twist\nfrom std_msgs.msg import Float32\nimport numpy as np\nimport math\nfrom collections import deque\n\nclass NavigationPerformanceValidator(Node):\n    """\n    Validate navigation performance metrics for Isaac ROS navigation system\n    """\n\n    def __init__(self):\n        super().__init__(\'navigation_performance_validator\')\n\n        # Publishers\n        self.path_efficiency_pub = self.create_publisher(Float32, \'/humanoid/navigation/path_efficiency\', 10)\n        self.success_rate_pub = self.create_publisher(Float32, \'/humanoid/navigation/success_rate\', 10)\n        self.execution_time_pub = self.create_publisher(Float32, \'/humanoid/navigation/execution_time\', 10)\n\n        # Subscribers\n        self.odom_sub = self.create_subscription(\n            Odometry,\n            \'/humanoid/odom\',\n            self.odom_callback,\n            10\n        )\n\n        self.path_sub = self.create_subscription(\n            Path,\n            \'/humanoid/planned_path\',\n            self.path_callback,\n            10\n        )\n\n        self.goal_sub = self.create_subscription(\n            PoseStamped,\n            \'/humanoid/goal_pose\',\n            self.goal_callback,\n            10\n        )\n\n        # Performance tracking\n        self.trajectory_history = deque(maxlen=1000)  # Last 1000 poses\n        self.path_history = deque(maxlen=100)        # Last 100 planned paths\n        self.goal_history = deque(maxlen=50)         # Last 50 goals\n        self.start_time = None\n        self.current_goal = None\n        self.navigation_start_pose = None\n\n        # Performance metrics\n        self.successful_navigations = 0\n        self.failed_navigations = 0\n        self.total_executions = 0\n\n        # Timers\n        self.performance_timer = self.create_timer(1.0, self.calculate_performance_metrics)\n        self.metrics_publish_timer = self.create_timer(5.0, self.publish_performance_metrics)\n\n        self.get_logger().info(\'Navigation Performance Validator initialized\')\n\n    def odom_callback(self, msg):\n        """Track robot trajectory for performance analysis"""\n        self.trajectory_history.append({\n            \'timestamp\': self.get_clock().now(),\n            \'pose\': msg.pose.pose,\n            \'velocity\': msg.twist.twist\n        })\n\n    def path_callback(self, msg):\n        """Track planned paths for efficiency analysis"""\n        self.path_history.append(msg)\n\n    def goal_callback(self, msg):\n        """Track navigation goals"""\n        self.current_goal = msg.pose\n        self.navigation_start_pose = self.get_current_pose()\n        self.start_time = self.get_clock().now()\n\n    def get_current_pose(self):\n        """Get current pose from trajectory history"""\n        if self.trajectory_history:\n            return self.trajectory_history[-1][\'pose\']\n        return None\n\n    def calculate_path_efficiency(self):\n        """Calculate path efficiency metric"""\n        if not self.path_history or not self.trajectory_history:\n            return 0.0\n\n        # Get the most recent planned path\n        planned_path = self.path_history[-1]\n        if not planned_path.poses:\n            return 0.0\n\n        # Calculate planned path length\n        planned_length = 0.0\n        for i in range(1, len(planned_path.poses)):\n            p1 = planned_path.poses[i-1].pose.position\n            p2 = planned_path.poses[i].pose.position\n            dist = math.sqrt((p2.x - p1.x)**2 + (p2.y - p1.y)**2)\n            planned_length += dist\n\n        # Calculate actual path length (from trajectory)\n        if len(self.trajectory_history) < 2:\n            return 0.0\n\n        actual_length = 0.0\n        for i in range(1, len(self.trajectory_history)):\n            p1 = self.trajectory_history[i-1][\'pose\'].position\n            p2 = self.trajectory_history[i][\'pose\'].position\n            dist = math.sqrt((p2.x - p1.x)**2 + (p2.y - p1.y)**2)\n            actual_length += dist\n\n        # Calculate efficiency (optimal path would be straight line)\n        if self.current_goal and self.navigation_start_pose:\n            straight_line_dist = math.sqrt(\n                (self.current_goal.position.x - self.navigation_start_pose.position.x)**2 +\n                (self.current_goal.position.y - self.navigation_start_pose.position.y)**2\n            )\n\n            if straight_line_dist > 0:\n                optimality_ratio = straight_line_dist / planned_length if planned_length > 0 else 0\n                efficiency = min(1.0, optimality_ratio)  # Cap at 1.0\n                return efficiency\n\n        return 0.0\n\n    def calculate_success_rate(self):\n        """Calculate navigation success rate"""\n        total = self.successful_navigations + self.failed_navigations\n        if total == 0:\n            return 0.0\n        return float(self.successful_navigations) / total\n\n    def calculate_execution_time(self):\n        """Calculate average navigation execution time"""\n        if not self.start_time or not self.current_goal:\n            return 0.0\n\n        # Check if goal reached (simplified)\n        if self.get_current_pose():\n            current_pos = self.get_current_pose().position\n            goal_pos = self.current_goal.position\n            dist_to_goal = math.sqrt((goal_pos.x - current_pos.x)**2 + (goal_pos.y - current_pos.y)**2)\n\n            if dist_to_goal < 0.5:  # Goal reached\n                execution_time = (self.get_clock().now() - self.start_time).nanoseconds / 1e9\n                return execution_time\n\n        return 0.0\n\n    def calculate_performance_metrics(self):\n        """Calculate all performance metrics"""\n        path_efficiency = self.calculate_path_efficiency()\n        success_rate = self.calculate_success_rate()\n        execution_time = self.calculate_execution_time()\n\n        # Log metrics\n        self.get_logger().info(f\'Performance Metrics - Path Efficiency: {path_efficiency:.3f}, \'\n                              f\'Success Rate: {success_rate:.3f}, \'\n                              f\'Execution Time: {execution_time:.3f}s\')\n\n    def publish_performance_metrics(self):\n        """Publish performance metrics to ROS topics"""\n        # Publish path efficiency\n        efficiency_msg = Float32()\n        efficiency_msg.data = self.calculate_path_efficiency()\n        self.path_efficiency_pub.publish(efficiency_msg)\n\n        # Publish success rate\n        success_msg = Float32()\n        success_msg.data = self.calculate_success_rate()\n        self.success_rate_pub.publish(success_msg)\n\n        # Publish execution time\n        time_msg = Float32()\n        time_msg.data = self.calculate_execution_time()\n        self.execution_time_pub.publish(time_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    validator = NavigationPerformanceValidator()\n\n    try:\n        rclpy.spin(validator)\n    except KeyboardInterrupt:\n        validator.get_logger().info(\'Shutting down Navigation Performance Validator...\')\n    finally:\n        validator.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(e.p,{children:"Navigation and path planning systems are essential for autonomous humanoid robot operation. Isaac ROS provides GPU-accelerated navigation capabilities that integrate perception and planning for robust autonomous navigation. Key components include:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Localization"}),": Accurate pose estimation using multiple sensors and fusion algorithms"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Mapping"}),": Construction and maintenance of environment representations"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Path Planning"}),": Computation of optimal paths considering static and dynamic obstacles"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Motion Execution"}),": Smooth execution of planned trajectories with obstacle avoidance"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Performance Validation"}),": Metrics and validation tools to ensure navigation quality"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"exercises",children:"Exercises"}),"\n",(0,i.jsx)(e.h3,{id:"conceptual",children:"Conceptual"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Explain the differences between global path planning and local path planning in robotics navigation systems."}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"logical",children:"Logical"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Analyze the trade-offs between navigation optimality and computational performance in real-time humanoid robot applications. When would you prioritize one over the other?"}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"implementation-1",children:"Implementation"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Implement a complete navigation system that integrates perception data with path planning for dynamic obstacle avoidance in a humanoid robot simulation."}),"\n"]})]})}function d(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(p,{...n})}):p(n)}},8453(n,e,a){a.d(e,{R:()=>o,x:()=>r});var t=a(6540);const i={},s=t.createContext(i);function o(n){const e=t.useContext(s);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:o(n.components),t.createElement(s.Provider,{value:e},n.children)}}}]);