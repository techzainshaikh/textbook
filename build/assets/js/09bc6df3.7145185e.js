"use strict";(globalThis.webpackChunkphysical_ai_book_humanoid=globalThis.webpackChunkphysical_ai_book_humanoid||[]).push([[9204],{1444(n,e,i){i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>t,default:()=>m,frontMatter:()=>r,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-2-digital-twin/chapter-2-sensor-simulation","title":"Sensor Simulation in Gazebo","description":"Understanding sensor simulation for digital twins using Gazebo","source":"@site/docs/module-2-digital-twin/chapter-2-sensor-simulation.md","sourceDirName":"module-2-digital-twin","slug":"/module-2-digital-twin/chapter-2-sensor-simulation","permalink":"/textbook/docs/module-2-digital-twin/chapter-2-sensor-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/Panaversity/physical_ai_book_humanoid/tree/main/docs/module-2-digital-twin/chapter-2-sensor-simulation.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Sensor Simulation in Gazebo","sidebar_position":3,"description":"Understanding sensor simulation for digital twins using Gazebo","keywords":["sensor simulation","gazebo","lidar","camera","imu","robotics simulation"]},"sidebar":"tutorialSidebar","previous":{"title":"Physics Simulation in Gazebo","permalink":"/textbook/docs/module-2-digital-twin/chapter-1-physics-simulation"},"next":{"title":"Environment Modeling for Digital Twins","permalink":"/textbook/docs/module-2-digital-twin/chapter-3-environment-modeling"}}');var a=i(4848),o=i(8453);const r={title:"Sensor Simulation in Gazebo",sidebar_position:3,description:"Understanding sensor simulation for digital twins using Gazebo",keywords:["sensor simulation","gazebo","lidar","camera","imu","robotics simulation"]},t="Chapter 2: Sensor Simulation in Gazebo",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Core Concepts",id:"core-concepts",level:2},{value:"Types of Sensors in Robotics Simulation",id:"types-of-sensors-in-robotics-simulation",level:3},{value:"Noise Modeling",id:"noise-modeling",level:3},{value:"Implementation",id:"implementation",level:2},{value:"LiDAR Sensor Configuration",id:"lidar-sensor-configuration",level:3},{value:"Camera Sensor Configuration",id:"camera-sensor-configuration",level:3},{value:"IMU Sensor Configuration",id:"imu-sensor-configuration",level:3},{value:"Examples",id:"examples",level:2},{value:"Example 1: Sensor Validation Node",id:"example-1-sensor-validation-node",level:3},{value:"Example 2: Multi-Sensor Fusion Node",id:"example-2-multi-sensor-fusion-node",level:3},{value:"Summary",id:"summary",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Conceptual",id:"conceptual",level:3},{value:"Logical",id:"logical",level:3},{value:"Implementation",id:"implementation-1",level:3}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"chapter-2-sensor-simulation-in-gazebo",children:"Chapter 2: Sensor Simulation in Gazebo"})}),"\n",(0,a.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(e.p,{children:"By the end of this chapter, students will be able to:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Configure various sensor types in Gazebo simulation (LiDAR, cameras, IMU, GPS, etc.)"}),"\n",(0,a.jsx)(e.li,{children:"Implement realistic sensor noise models to match real-world performance"}),"\n",(0,a.jsx)(e.li,{children:"Validate sensor data quality and accuracy in simulation vs. real hardware"}),"\n",(0,a.jsx)(e.li,{children:"Integrate simulated sensors with ROS 2 topics and message types"}),"\n",(0,a.jsx)(e.li,{children:"Design custom sensor configurations for specific robotic applications"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsx)(e.p,{children:"Students should have:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Understanding of basic sensor types and their applications in robotics"}),"\n",(0,a.jsx)(e.li,{children:"Knowledge of ROS 2 message types for sensor data (sensor_msgs package)"}),"\n",(0,a.jsx)(e.li,{children:"Completed Chapter 1 (Physics Simulation in Gazebo)"}),"\n",(0,a.jsx)(e.li,{children:"Basic understanding of probability and statistics for noise modeling"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"core-concepts",children:"Core Concepts"}),"\n",(0,a.jsx)(e.p,{children:"Sensor simulation is critical for developing robust perception and navigation systems. Realistic sensor simulation allows developers to test algorithms under various conditions before deploying to physical hardware."}),"\n",(0,a.jsx)(e.h3,{id:"types-of-sensors-in-robotics-simulation",children:"Types of Sensors in Robotics Simulation"}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Range Sensors (LiDAR, Sonar, IR):"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Provide distance measurements to obstacles"}),"\n",(0,a.jsx)(e.li,{children:"Critical for navigation and mapping"}),"\n",(0,a.jsx)(e.li,{children:"Subject to noise, occlusion, and range limitations"}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Cameras (RGB, Depth, Stereo):"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Provide visual information for object recognition and scene understanding"}),"\n",(0,a.jsx)(e.li,{children:"Require significant computational resources"}),"\n",(0,a.jsx)(e.li,{children:"Affected by lighting conditions and motion blur"}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Inertial Measurement Units (IMU):"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Measure acceleration and angular velocity"}),"\n",(0,a.jsx)(e.li,{children:"Essential for localization and stabilization"}),"\n",(0,a.jsx)(e.li,{children:"Drift over time, affected by bias and noise"}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"GPS and Positioning:"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Provide global position estimates"}),"\n",(0,a.jsx)(e.li,{children:"Accuracy varies with environmental conditions"}),"\n",(0,a.jsx)(e.li,{children:"Affected by multipath and signal obstruction"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"noise-modeling",children:"Noise Modeling"}),"\n",(0,a.jsx)(e.p,{children:"Real sensors are subject to various types of noise:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Gaussian noise"}),": Random variations around the true measurement"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Bias"}),": Systematic offset in measurements"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Drift"}),": Slow variation in sensor characteristics over time"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Quantization"}),": Discretization effects in digital sensors"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"implementation",children:"Implementation"}),"\n",(0,a.jsx)(e.p,{children:"Let's implement sensor simulation for our humanoid robot. We'll configure realistic sensors with appropriate noise models."}),"\n",(0,a.jsx)(e.h3,{id:"lidar-sensor-configuration",children:"LiDAR Sensor Configuration"}),"\n",(0,a.jsx)(e.p,{children:"Here's an example of configuring a 3D LiDAR sensor in Gazebo:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'\x3c!-- LiDAR sensor configuration --\x3e\n<gazebo reference="lidar_link">\n  <sensor type="ray" name="humanoid_lidar">\n    <pose>0 0 0 0 0 0</pose>\n    <visualize>true</visualize>\n    <update_rate>10</update_rate>\n    <ray>\n      <scan>\n        <horizontal>\n          <samples>640</samples>\n          <resolution>1</resolution>\n          <min_angle>-1.570796</min_angle> \x3c!-- -90 degrees --\x3e\n          <max_angle>1.570796</max_angle>   \x3c!-- 90 degrees --\x3e\n        </horizontal>\n        <vertical>\n          <samples>32</samples>\n          <resolution>1</resolution>\n          <min_angle>-0.261799</min_angle> \x3c!-- -15 degrees --\x3e\n          <max_angle>0.261799</max_angle>   \x3c!-- 15 degrees --\x3e\n        </vertical>\n      </scan>\n      <range>\n        <min>0.1</min>\n        <max>30.0</max>\n        <resolution>0.01</resolution>\n      </range>\n    </ray>\n    <plugin name="lidar_controller" filename="libgazebo_ros_ray_sensor.so">\n      <ros>\n        <namespace>/humanoid</namespace>\n        <remapping>~/out:=scan</remapping>\n      </ros>\n      <output_type>sensor_msgs/LaserScan</output_type>\n      <frame_name>lidar_link</frame_name>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,a.jsx)(e.h3,{id:"camera-sensor-configuration",children:"Camera Sensor Configuration"}),"\n",(0,a.jsx)(e.p,{children:"Configuring a realistic RGB camera with noise parameters:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'\x3c!-- Camera sensor configuration --\x3e\n<gazebo reference="camera_link">\n  <sensor type="camera" name="humanoid_camera">\n    <update_rate>30</update_rate>\n    <camera name="head">\n      <horizontal_fov>1.3962634</horizontal_fov> \x3c!-- 80 degrees --\x3e\n      <image>\n        <width>640</width>\n        <height>480</height>\n        <format>R8G8B8</format>\n      </image>\n      <clip>\n        <near>0.1</near>\n        <far>100</far>\n      </clip>\n      <noise>\n        <type>gaussian</type>\n        <mean>0.0</mean>\n        <stddev>0.007</stddev>\n      </noise>\n    </camera>\n    <plugin name="camera_controller" filename="libgazebo_ros_camera.so">\n      <ros>\n        <namespace>/humanoid</namespace>\n        <remapping>image_raw:=camera/image_raw</remapping>\n        <remapping>camera_info:=camera/camera_info</remapping>\n      </ros>\n      <camera_name>camera</camera_name>\n      <image_topic_name>image_raw</image_topic_name>\n      <camera_info_topic_name>camera_info</camera_info_topic_name>\n      <frame_name>camera_link</frame_name>\n      <hack_baseline>0.07</hack_baseline>\n      <distortion_k1>0.0</distortion_k1>\n      <distortion_k2>0.0</distortion_k2>\n      <distortion_k3>0.0</distortion_k3>\n      <distortion_t1>0.0</distortion_t1>\n      <distortion_t2>0.0</distortion_t2>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,a.jsx)(e.h3,{id:"imu-sensor-configuration",children:"IMU Sensor Configuration"}),"\n",(0,a.jsx)(e.p,{children:"Configuring an IMU with realistic noise characteristics:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'\x3c!-- IMU sensor configuration --\x3e\n<gazebo reference="imu_link">\n  <sensor type="imu" name="humanoid_imu">\n    <always_on>true</always_on>\n    <update_rate>100</update_rate>\n    <imu>\n      <angular_velocity>\n        <x>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.0017</stddev> \x3c!-- ~0.1 deg/s --\x3e\n            <bias_mean>0.0001</bias_mean>\n            <bias_stddev>0.00001</bias_stddev>\n          </noise>\n        </x>\n        <y>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.0017</stddev>\n            <bias_mean>0.0001</bias_mean>\n            <bias_stddev>0.00001</bias_stddev>\n          </noise>\n        </y>\n        <z>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.0017</stddev>\n            <bias_mean>0.0001</bias_mean>\n            <bias_stddev>0.00001</bias_stddev>\n          </noise>\n        </z>\n      </angular_velocity>\n      <linear_acceleration>\n        <x>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>1.7e-2</stddev> \x3c!-- ~0.0017 m/s\xb2 --\x3e\n            <bias_mean>0.01</bias_mean>\n            <bias_stddev>0.001</bias_stddev>\n          </noise>\n        </x>\n        <y>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>1.7e-2</stddev>\n            <bias_mean>0.01</bias_mean>\n            <bias_stddev>0.001</bias_stddev>\n          </noise>\n        </y>\n        <z>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>1.7e-2</stddev>\n            <bias_mean>0.01</bias_mean>\n            <bias_stddev>0.001</bias_stddev>\n          </noise>\n        </z>\n      </linear_acceleration>\n    </imu>\n    <plugin name="imu_controller" filename="libgazebo_ros_imu.so">\n      <ros>\n        <namespace>/humanoid</namespace>\n        <remapping>~/out:=imu/data</remapping>\n      </ros>\n      <frame_name>imu_link</frame_name>\n      <body_name>torso</body_name>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,a.jsx)(e.h2,{id:"examples",children:"Examples"}),"\n",(0,a.jsx)(e.h3,{id:"example-1-sensor-validation-node",children:"Example 1: Sensor Validation Node"}),"\n",(0,a.jsx)(e.p,{children:"Let's create a node to validate sensor data quality:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n# sensor_validation.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, Image, Imu\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport cv2\n\nclass SensorValidator(Node):\n    \"\"\"Validate sensor data quality and accuracy\"\"\"\n\n    def __init__(self):\n        super().__init__('sensor_validator')\n\n        # Subscribers for different sensor types\n        self.scan_sub = self.create_subscription(\n            LaserScan,\n            '/humanoid/scan',\n            self.scan_callback,\n            10\n        )\n\n        self.image_sub = self.create_subscription(\n            Image,\n            '/humanoid/camera/image_raw',\n            self.image_callback,\n            10\n        )\n\n        self.imu_sub = self.create_subscription(\n            Imu,\n            '/humanoid/imu/data',\n            self.imu_callback,\n            10\n        )\n\n        # CV Bridge for image processing\n        self.cv_bridge = CvBridge()\n\n        # Statistics tracking\n        self.scan_stats = {'count': 0, 'range_errors': 0}\n        self.image_stats = {'count': 0, 'quality_issues': 0}\n        self.imu_stats = {'count': 0, 'drift_warnings': 0}\n\n        # Timers for periodic reporting\n        self.report_timer = self.create_timer(5.0, self.report_statistics)\n\n    def scan_callback(self, msg):\n        \"\"\"Process LiDAR scan data\"\"\"\n        self.scan_stats['count'] += 1\n\n        # Check for invalid ranges\n        invalid_ranges = 0\n        for r in msg.ranges:\n            if np.isnan(r) or np.isinf(r):\n                invalid_ranges += 1\n\n        if invalid_ranges > len(msg.ranges) * 0.1:  # More than 10% invalid\n            self.get_logger().warn(f'High invalid range readings: {invalid_ranges}/{len(msg.ranges)}')\n            self.scan_stats['range_errors'] += 1\n\n        # Log scan quality metrics\n        valid_ranges = [r for r in msg.ranges if not (np.isnan(r) or np.isinf(r))]\n        if valid_ranges:\n            avg_range = sum(valid_ranges) / len(valid_ranges)\n            self.get_logger().info(f'Avg scan range: {avg_range:.2f}m')\n\n    def image_callback(self, msg):\n        \"\"\"Process camera image data\"\"\"\n        self.image_stats['count'] += 1\n\n        try:\n            # Convert ROS image to OpenCV\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, 'bgr8')\n\n            # Calculate image quality metrics\n            gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)\n\n            # Measure sharpness using Laplacian variance\n            laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n\n            if laplacian_var < 100:  # Low sharpness threshold\n                self.get_logger().warn(f'Blurry image detected: sharpness={laplacian_var:.2f}')\n                self.image_stats['quality_issues'] += 1\n\n            # Measure brightness\n            mean_brightness = np.mean(gray)\n            if mean_brightness < 30 or mean_brightness > 220:  # Too dark or too bright\n                self.get_logger().info(f'Image brightness: {mean_brightness:.2f}')\n\n        except Exception as e:\n            self.get_logger().error(f'Image processing error: {str(e)}')\n\n    def imu_callback(self, msg):\n        \"\"\"Process IMU data\"\"\"\n        self.imu_stats['count'] += 1\n\n        # Check for unrealistic acceleration values (likely sensor error)\n        lin_acc_mag = np.sqrt(\n            msg.linear_acceleration.x**2 +\n            msg.linear_acceleration.y**2 +\n            msg.linear_acceleration.z**2\n        )\n\n        # Earth's gravity is ~9.81 m/s\xb2, so total acceleration shouldn't exceed ~15 m/s\xb2\n        # unless the robot is accelerating rapidly\n        if lin_acc_mag > 15.0:\n            self.get_logger().warn(f'High acceleration detected: {lin_acc_mag:.2f} m/s\xb2')\n\n        # Check angular velocity magnitude\n        ang_vel_mag = np.sqrt(\n            msg.angular_velocity.x**2 +\n            msg.angular_velocity.y**2 +\n            msg.angular_velocity.z**2\n        )\n\n        if ang_vel_mag > 10.0:  # Unusually high rotation rate\n            self.get_logger().info(f'High angular velocity: {ang_vel_mag:.2f} rad/s')\n\n    def report_statistics(self):\n        \"\"\"Report sensor validation statistics\"\"\"\n        self.get_logger().info('=== Sensor Validation Report ===')\n        self.get_logger().info(f'LiDAR: {self.scan_stats[\"count\"]} scans, {self.scan_stats[\"range_errors\"]} errors')\n        self.get_logger().info(f'Camera: {self.image_stats[\"count\"]} images, {self.image_stats[\"quality_issues\"]} quality issues')\n        self.get_logger().info(f'IMU: {self.imu_stats[\"count\"]} samples, {self.imu_stats[\"drift_warnings\"]} warnings')\n        self.get_logger().info('===============================')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    validator = SensorValidator()\n\n    try:\n        rclpy.spin(validator)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        validator.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(e.h3,{id:"example-2-multi-sensor-fusion-node",children:"Example 2: Multi-Sensor Fusion Node"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n# sensor_fusion.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, Imu\nfrom geometry_msgs.msg import PoseWithCovarianceStamped\nfrom tf2_ros import TransformBroadcaster\nimport numpy as np\nfrom collections import deque\n\nclass SensorFusion(Node):\n    """Fuse multiple sensor inputs for improved state estimation"""\n\n    def __init__(self):\n        super().__init__(\'sensor_fusion\')\n\n        # Subscribers\n        self.scan_sub = self.create_subscription(\n            LaserScan,\n            \'/humanoid/scan\',\n            self.scan_callback,\n            10\n        )\n\n        self.imu_sub = self.create_subscription(\n            Imu,\n            \'/humanoid/imu/data\',\n            self.imu_callback,\n            10\n        )\n\n        # Publisher for fused pose estimate\n        self.pose_pub = self.create_publisher(\n            PoseWithCovarianceStamped,\n            \'/humanoid/pose_fused\',\n            10\n        )\n\n        # Transform broadcaster\n        self.tf_broadcaster = TransformBroadcaster(self)\n\n        # State estimation variables\n        self.position = np.array([0.0, 0.0, 0.0])  # x, y, z\n        self.velocity = np.array([0.0, 0.0, 0.0])  # vx, vy, vz\n        self.orientation = np.array([0.0, 0.0, 0.0, 1.0])  # quaternion\n        self.acceleration_bias = np.zeros(3)  # Bias in acceleration measurements\n\n        # Buffers for sensor fusion\n        self.imu_buffer = deque(maxlen=10)\n        self.last_scan_time = self.get_clock().now()\n\n        # Timing\n        self.prev_time = None\n\n    def scan_callback(self, msg):\n        """Process LiDAR scan for position correction"""\n        # In a real implementation, we\'d use scan matching or landmark detection\n        # For this example, we\'ll just log the scan and use it to correct position estimates\n\n        self.get_logger().info(f\'Received scan with {len(msg.ranges)} beams\')\n\n        # Simple obstacle detection\n        min_range = min([r for r in msg.ranges if not (np.isnan(r) or np.isinf(r))], default=float(\'inf\'))\n\n        if min_range < 1.0:  # Obstacle within 1 meter\n            self.get_logger().info(f\'Obstacle detected at {min_range:.2f}m, adjusting position estimate\')\n\n    def imu_callback(self, msg):\n        """Process IMU data for orientation and acceleration"""\n        current_time = self.get_clock().now()\n\n        if self.prev_time is not None:\n            dt = (current_time - self.prev_time).nanoseconds / 1e9\n\n            if dt > 0:\n                # Extract acceleration (remove gravity)\n                raw_acc = np.array([\n                    msg.linear_acceleration.x,\n                    msg.linear_acceleration.y,\n                    msg.linear_acceleration.z\n                ])\n\n                # Apply bias correction\n                corrected_acc = raw_acc - self.acceleration_bias\n\n                # Update velocity using acceleration\n                self.velocity += corrected_acc * dt\n\n                # Update position using velocity\n                self.position += self.velocity * dt + 0.5 * corrected_acc * dt**2\n\n                # Extract orientation from quaternion\n                self.orientation = np.array([\n                    msg.orientation.x,\n                    msg.orientation.y,\n                    msg.orientation.z,\n                    msg.orientation.w\n                ])\n\n                # Publish fused pose estimate\n                self.publish_pose_estimate(current_time)\n\n        self.prev_time = current_time\n\n    def publish_pose_estimate(self, timestamp):\n        """Publish the fused pose estimate"""\n        pose_msg = PoseWithCovarianceStamped()\n        pose_msg.header.stamp = timestamp.to_msg()\n        pose_msg.header.frame_id = \'odom\'\n\n        # Fill position\n        pose_msg.pose.pose.position.x = self.position[0]\n        pose_msg.pose.pose.position.y = self.position[1]\n        pose_msg.pose.pose.position.z = self.position[2]\n\n        # Fill orientation\n        pose_msg.pose.pose.orientation.x = self.orientation[0]\n        pose_msg.pose.pose.orientation.y = self.orientation[1]\n        pose_msg.pose.pose.orientation.z = self.orientation[2]\n        pose_msg.pose.pose.orientation.w = self.orientation[3]\n\n        # Set covariance (diagonal values only for simplicity)\n        cov_diag = [0.1, 0.1, 0.1, 0.01, 0.01, 0.1]  # px, py, pz, orx, ory, orz\n        for i, val in enumerate(cov_diag):\n            pose_msg.pose.covariance[i*7] = val  # Diagonal elements in 6x6 covariance matrix\n\n        self.pose_pub.publish(pose_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    fusion_node = SensorFusion()\n\n    try:\n        rclpy.spin(fusion_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        fusion_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,a.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(e.p,{children:"Sensor simulation is a critical component of digital twin technology in robotics. Properly configured sensors with realistic noise models allow for robust algorithm development and testing. Key considerations include:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Realistic Noise Models"}),": Matching simulation noise characteristics to real hardware"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Update Rates"}),": Configuring appropriate sensor update rates for the application"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Data Quality"}),": Validating sensor data for accuracy and consistency"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Multi-Sensor Fusion"}),": Combining data from multiple sensors for improved state estimation"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Calibration"}),": Ensuring simulated sensors are properly calibrated relative to the robot frame"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"exercises",children:"Exercises"}),"\n",(0,a.jsx)(e.h3,{id:"conceptual",children:"Conceptual"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Explain the importance of realistic noise modeling in sensor simulation and how it affects algorithm development."}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"logical",children:"Logical"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Analyze the trade-offs between sensor accuracy and computational performance in simulation. How would you prioritize different sensor types for a humanoid robot navigation system?"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"implementation-1",children:"Implementation"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Create a complete sensor simulation setup for a humanoid robot with LiDAR, camera, and IMU sensors, and implement a basic sensor validation node that monitors data quality."}),"\n"]})]})}function m(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(d,{...n})}):d(n)}},8453(n,e,i){i.d(e,{R:()=>r,x:()=>t});var s=i(6540);const a={},o=s.createContext(a);function r(n){const e=s.useContext(o);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function t(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:r(n.components),s.createElement(o.Provider,{value:e},n.children)}}}]);