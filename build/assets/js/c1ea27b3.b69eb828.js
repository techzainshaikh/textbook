"use strict";(globalThis.webpackChunkphysical_ai_book_humanoid=globalThis.webpackChunkphysical_ai_book_humanoid||[]).push([[7597],{5305(e,n,t){t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>u,frontMatter:()=>s,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"module-4-vla/chapter-2-speech-recognition","title":"Speech Recognition for Robotics with OpenAI Whisper","description":"Implementing speech recognition systems for humanoid robots using OpenAI Whisper and other speech-to-text technologies","source":"@site/docs/module-4-vla/chapter-2-speech-recognition.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/chapter-2-speech-recognition","permalink":"/textbook/docs/module-4-vla/chapter-2-speech-recognition","draft":false,"unlisted":false,"editUrl":"https://github.com/Panaversity/physical_ai_book_humanoid/tree/main/docs/module-4-vla/chapter-2-speech-recognition.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Speech Recognition for Robotics with OpenAI Whisper","sidebar_position":3,"description":"Implementing speech recognition systems for humanoid robots using OpenAI Whisper and other speech-to-text technologies","keywords":["speech recognition","OpenAI Whisper","speech-to-text","robotics","natural language processing","humanoid AI"]},"sidebar":"tutorialSidebar","previous":{"title":"Vision-Language-Action Systems Overview","permalink":"/textbook/docs/module-4-vla/chapter-1-vla-overview"},"next":{"title":"LLM-Based Task Planning for Robotics","permalink":"/textbook/docs/module-4-vla/chapter-3-llm-planning"}}');var o=t(4848),r=t(8453);const s={title:"Speech Recognition for Robotics with OpenAI Whisper",sidebar_position:3,description:"Implementing speech recognition systems for humanoid robots using OpenAI Whisper and other speech-to-text technologies",keywords:["speech recognition","OpenAI Whisper","speech-to-text","robotics","natural language processing","humanoid AI"]},a="Chapter 2: Speech Recognition for Robotics with OpenAI Whisper",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Core Concepts",id:"core-concepts",level:2},{value:"Speech Recognition Pipeline",id:"speech-recognition-pipeline",level:3},{value:"OpenAI Whisper Architecture",id:"openai-whisper-architecture",level:3},{value:"Implementation",id:"implementation",level:2},{value:"Whisper-Based Speech Recognition System",id:"whisper-based-speech-recognition-system",level:3},{value:"Advanced Speech Recognition Features",id:"advanced-speech-recognition-features",level:3},{value:"Examples",id:"examples",level:2},{value:"Example 1: Robot Command Recognition System",id:"example-1-robot-command-recognition-system",level:3},{value:"Example 2: Speech Recognition with Error Handling and Validation",id:"example-2-speech-recognition-with-error-handling-and-validation",level:3},{value:"Summary",id:"summary",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Conceptual",id:"conceptual",level:3},{value:"Logical",id:"logical",level:3},{value:"Implementation",id:"implementation-1",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-2-speech-recognition-for-robotics-with-openai-whisper",children:"Chapter 2: Speech Recognition for Robotics with OpenAI Whisper"})}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(n.p,{children:"By the end of this chapter, students will be able to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Implement speech recognition systems for humanoid robot command input"}),"\n",(0,o.jsx)(n.li,{children:"Integrate OpenAI Whisper for accurate speech-to-text conversion"}),"\n",(0,o.jsx)(n.li,{children:"Process and validate speech commands for robotic applications"}),"\n",(0,o.jsx)(n.li,{children:"Handle speech recognition errors and uncertainties in robot systems"}),"\n",(0,o.jsx)(n.li,{children:"Design robust speech interfaces for noisy environments"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,o.jsx)(n.p,{children:"Students should have:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Understanding of basic signal processing concepts"}),"\n",(0,o.jsx)(n.li,{children:"Knowledge of Python programming and audio processing libraries"}),"\n",(0,o.jsx)(n.li,{children:"Familiarity with natural language processing fundamentals"}),"\n",(0,o.jsx)(n.li,{children:"Basic understanding of robotics command interfaces (covered in Module 1)"}),"\n",(0,o.jsx)(n.li,{children:"Experience with machine learning frameworks (PyTorch/TensorFlow)"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"core-concepts",children:"Core Concepts"}),"\n",(0,o.jsx)(n.p,{children:"Speech recognition systems enable humanoid robots to understand natural language commands through spoken input. Modern approaches leverage deep learning models to convert audio signals into text, which can then be processed by language understanding systems."}),"\n",(0,o.jsx)(n.h3,{id:"speech-recognition-pipeline",children:"Speech Recognition Pipeline"}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Audio Input:"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Microphone Array"}),": Multiple microphones for noise reduction and directionality"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Audio Preprocessing"}),": Filtering, noise reduction, and signal enhancement"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Feature Extraction"}),": Mel-spectrograms, MFCCs, or raw waveform processing"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Model Processing:"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Acoustic Model"}),": Maps audio features to phonemes or subword units"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Language Model"}),": Provides linguistic context for word prediction"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Decoder"}),": Combines acoustic and language models to generate text"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Post-Processing:"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Text Refinement"}),": Grammar correction and context-based validation"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Command Extraction"}),": Identifying actionable commands from speech"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Confidence Scoring"}),": Assessing recognition quality and reliability"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"openai-whisper-architecture",children:"OpenAI Whisper Architecture"}),"\n",(0,o.jsx)(n.p,{children:"Whisper represents a significant advancement in speech recognition, using a large-scale transformer-based approach:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Encoder"}),": Processes audio spectrograms using transformer layers"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Decoder"}),": Generates text tokens conditioned on audio context"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Multilingual Capability"}),": Trained on multiple languages simultaneously"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Robustness"}),": Performs well across diverse accents and audio conditions"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"implementation",children:"Implementation"}),"\n",(0,o.jsx)(n.p,{children:"Let's implement speech recognition for humanoid robotics using OpenAI Whisper:"}),"\n",(0,o.jsx)(n.h3,{id:"whisper-based-speech-recognition-system",children:"Whisper-Based Speech Recognition System"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# whisper_speech_recognition.py\n\nimport torch\nimport whisper\nimport numpy as np\nimport librosa\nimport pyaudio\nimport wave\nimport threading\nimport queue\nimport time\nfrom typing import Dict, Any, List, Optional, Tuple\nfrom dataclasses import dataclass\nimport asyncio\n\n@dataclass\nclass SpeechRecognitionResult:\n    """Result from speech recognition"""\n    text: str\n    confidence: float\n    language: str\n    timestamp: float\n    audio_duration: float\n    raw_transcription: Dict[str, Any]  # Raw Whisper output\n\nclass AudioCapture:\n    """\n    Audio capture system for speech recognition\n    """\n\n    def __init__(self, sample_rate: int = 16000, chunk_size: int = 1024, channels: int = 1):\n        self.sample_rate = sample_rate\n        self.chunk_size = chunk_size\n        self.channels = channels\n        self.audio = pyaudio.PyAudio()\n        self.stream = None\n        self.recording = False\n        self.audio_queue = queue.Queue()\n\n    def start_recording(self):\n        """Start audio recording"""\n        self.stream = self.audio.open(\n            format=pyaudio.paFloat32,\n            channels=self.channels,\n            rate=self.sample_rate,\n            input=True,\n            frames_per_buffer=self.chunk_size\n        )\n        self.recording = True\n\n        # Start recording thread\n        self.record_thread = threading.Thread(target=self._record_audio)\n        self.record_thread.start()\n\n    def stop_recording(self):\n        """Stop audio recording"""\n        self.recording = False\n        if self.record_thread:\n            self.record_thread.join()\n        if self.stream:\n            self.stream.stop_stream()\n            self.stream.close()\n        self.audio.terminate()\n\n    def _record_audio(self):\n        """Internal recording loop"""\n        while self.recording:\n            try:\n                data = self.stream.read(self.chunk_size, exception_on_overflow=False)\n                audio_chunk = np.frombuffer(data, dtype=np.float32)\n                self.audio_queue.put(audio_chunk)\n            except Exception as e:\n                print(f"Audio recording error: {e}")\n                break\n\n    def get_audio_chunk(self, timeout: float = 1.0) -> Optional[np.ndarray]:\n        """Get an audio chunk from the queue"""\n        try:\n            return self.audio_queue.get(timeout=timeout)\n        except queue.Empty:\n            return None\n\n    def record_audio_segment(self, duration: float = 3.0) -> np.ndarray:\n        """Record a specific duration of audio"""\n        frames = []\n        samples_to_record = int(self.sample_rate * duration)\n        chunks_to_record = int(samples_to_record / self.chunk_size)\n\n        for _ in range(chunks_to_record):\n            chunk = self.get_audio_chunk()\n            if chunk is not None:\n                frames.append(chunk)\n\n        if frames:\n            return np.concatenate(frames)\n        else:\n            return np.array([])\n\nclass WhisperSpeechRecognizer:\n    """\n    Speech recognition using OpenAI Whisper\n    """\n\n    def __init__(self, model_size: str = "base", device: str = None):\n        self.model_size = model_size\n        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")\n\n        # Load Whisper model\n        print(f"Loading Whisper {model_size} model on {self.device}...")\n        self.model = whisper.load_model(model_size).to(self.device)\n\n        # Audio capture system\n        self.audio_capture = AudioCapture()\n\n    def preprocess_audio(self, audio: np.ndarray, target_sr: int = 16000) -> np.ndarray:\n        """Preprocess audio for Whisper"""\n        # Ensure audio is in the right format\n        if audio.dtype != np.float32:\n            audio = audio.astype(np.float32)\n\n        # Resample if needed\n        if target_sr != 16000:  # Whisper expects 16kHz\n            audio = librosa.resample(audio, orig_sr=target_sr, target_sr=16000)\n\n        # Normalize audio\n        audio = audio / np.max(np.abs(audio)) if np.max(np.abs(audio)) > 0 else audio\n\n        return audio\n\n    def transcribe_audio(self, audio: np.ndarray, language: str = "en") -> SpeechRecognitionResult:\n        """Transcribe audio using Whisper"""\n        # Preprocess audio\n        processed_audio = self.preprocess_audio(audio)\n\n        # Transcribe using Whisper\n        result = self.model.transcribe(\n            processed_audio,\n            language=language,\n            temperature=0.0,  # Deterministic output\n            best_of=1,\n            fp16=(self.device == "cuda")\n        )\n\n        # Calculate confidence (Whisper doesn\'t provide confidence, so we estimate)\n        confidence = self._estimate_confidence(result)\n\n        return SpeechRecognitionResult(\n            text=result["text"],\n            confidence=confidence,\n            language=language,\n            timestamp=time.time(),\n            audio_duration=len(processed_audio) / 16000,  # 16kHz sample rate\n            raw_transcription=result\n        )\n\n    def _estimate_confidence(self, transcription: Dict[str, Any]) -> float:\n        """Estimate confidence from Whisper output"""\n        # Whisper doesn\'t provide confidence scores directly\n        # We can estimate based on the length and complexity of the transcription\n        text = transcription.get("text", "")\n\n        if not text.strip():\n            return 0.0\n\n        # Simple heuristic: longer, more complex text is more likely to be confident\n        # In practice, you might use other metrics or implement custom confidence estimation\n        return min(0.9, len(text.strip().split()) * 0.1 + 0.5)\n\n    def continuous_recognition(self, callback_func, silence_threshold: float = 0.01,\n                             silence_duration: float = 2.0) -> None:\n        """Continuous speech recognition with silence detection"""\n        self.audio_capture.start_recording()\n\n        audio_buffer = np.array([])\n        silence_start = None\n        sample_rate = self.audio_capture.sample_rate\n\n        try:\n            while True:\n                chunk = self.audio_capture.get_audio_chunk(timeout=0.1)\n                if chunk is not None:\n                    audio_buffer = np.concatenate([audio_buffer, chunk])\n\n                    # Check for silence\n                    rms = np.sqrt(np.mean(chunk ** 2))\n\n                    if rms < silence_threshold:\n                        if silence_start is None:\n                            silence_start = time.time()\n                        elif time.time() - silence_start > silence_duration and len(audio_buffer) > sample_rate:\n                            # Process the speech segment\n                            if len(audio_buffer) > sample_rate:  # At least 1 second of audio\n                                result = self.transcribe_audio(audio_buffer)\n                                if result.confidence > 0.3:  # Minimum confidence threshold\n                                    callback_func(result)\n\n                            # Reset for next segment\n                            audio_buffer = np.array([])\n                            silence_start = None\n                    else:\n                        silence_start = None\n\n        except KeyboardInterrupt:\n            print("Stopping continuous recognition...")\n        finally:\n            self.audio_capture.stop_recording()\n\n    def recognize_from_file(self, audio_file_path: str, language: str = "en") -> SpeechRecognitionResult:\n        """Recognize speech from an audio file"""\n        # Load audio file\n        audio, sr = librosa.load(audio_file_path, sr=16000)  # Whisper expects 16kHz\n\n        # Transcribe\n        return self.transcribe_audio(audio, language)\n\n    def recognize_from_microphone(self, duration: float = 5.0, language: str = "en") -> SpeechRecognitionResult:\n        """Recognize speech from microphone input"""\n        self.audio_capture.start_recording()\n        time.sleep(0.1)  # Brief delay to ensure recording starts\n\n        # Record audio\n        audio = self.audio_capture.record_audio_segment(duration)\n\n        self.audio_capture.stop_recording()\n\n        if len(audio) == 0:\n            return SpeechRecognitionResult(\n                text="",\n                confidence=0.0,\n                language=language,\n                timestamp=time.time(),\n                audio_duration=0.0,\n                raw_transcription={}\n            )\n\n        # Transcribe\n        return self.transcribe_audio(audio, language)\n\nclass SpeechCommandValidator:\n    """\n    Validate and process recognized speech commands\n    """\n\n    def __init__(self):\n        # Define valid robot commands and their patterns\n        self.valid_commands = {\n            "navigation": [\n                "go to", "move to", "navigate to", "walk to", "go", "move", "navigate"\n            ],\n            "manipulation": [\n                "pick up", "grasp", "take", "lift", "place", "put", "drop", "release"\n            ],\n            "interaction": [\n                "hello", "hi", "greet", "introduce", "help", "stop", "wait", "continue"\n            ]\n        }\n\n        # Object categories for manipulation\n        self.object_categories = [\n            "cube", "ball", "box", "bottle", "cup", "object", "item"\n        ]\n\n    def validate_command(self, text: str) -> Dict[str, Any]:\n        """Validate if the recognized text is a valid robot command"""\n        text_lower = text.lower().strip()\n\n        validation_result = {\n            "is_valid": False,\n            "command_type": None,\n            "extracted_entities": [],\n            "intent": None,\n            "confidence": 0.0\n        }\n\n        # Check for valid command patterns\n        for cmd_type, patterns in self.valid_commands.items():\n            for pattern in patterns:\n                if pattern in text_lower:\n                    validation_result["is_valid"] = True\n                    validation_result["command_type"] = cmd_type\n                    validation_result["intent"] = pattern\n\n                    # Extract entities (objects, locations)\n                    entities = self._extract_entities(text_lower, pattern)\n                    validation_result["extracted_entities"] = entities\n\n                    # Set confidence based on command clarity\n                    validation_result["confidence"] = self._calculate_command_confidence(\n                        text_lower, entities\n                    )\n\n                    break\n\n        return validation_result\n\n    def _extract_entities(self, text: str, command_pattern: str) -> List[Dict[str, str]]:\n        """Extract entities (objects, locations) from command text"""\n        entities = []\n\n        # Remove the command pattern to get the object/location part\n        remaining_text = text.replace(command_pattern, "").strip()\n\n        # Extract object names\n        for obj_cat in self.object_categories:\n            if obj_cat in remaining_text:\n                entities.append({\n                    "type": "object",\n                    "value": obj_cat,\n                    "confidence": 0.8\n                })\n\n        # Extract location names\n        location_keywords = ["table", "shelf", "desk", "floor", "box", "container"]\n        for loc in location_keywords:\n            if loc in remaining_text:\n                entities.append({\n                    "type": "location",\n                    "value": loc,\n                    "confidence": 0.8\n                })\n\n        return entities\n\n    def _calculate_command_confidence(self, text: str, entities: List[Dict[str, str]]) -> float:\n        """Calculate confidence in the command validity"""\n        confidence = 0.5  # Base confidence\n\n        # Increase confidence if entities are present\n        if entities:\n            confidence += 0.3\n\n        # Increase confidence if command is specific\n        if len(text.split()) > 2:\n            confidence += 0.2\n\n        return min(1.0, confidence)\n\nclass RobotSpeechInterface:\n    """\n    Complete speech interface for humanoid robots\n    """\n\n    def __init__(self, whisper_model_size: str = "base"):\n        self.speech_recognizer = WhisperSpeechRecognizer(whisper_model_size)\n        self.command_validator = SpeechCommandValidator()\n        self.command_history = []\n        self.is_listening = False\n\n    def process_speech_command(self, audio_input: np.ndarray = None,\n                             audio_file: str = None,\n                             microphone_duration: float = 5.0) -> Dict[str, Any]:\n        """Process speech command from various input sources"""\n\n        # Get speech recognition result\n        if audio_file:\n            result = self.speech_recognizer.recognize_from_file(audio_file)\n        elif audio_input is not None:\n            result = self.speech_recognizer.transcribe_audio(audio_input)\n        else:\n            # Use microphone\n            result = self.speech_recognizer.recognize_from_microphone(microphone_duration)\n\n        # Validate the recognized command\n        validation = self.command_validator.validate_command(result.text)\n\n        # Create complete response\n        response = {\n            "recognition_result": result,\n            "validation": validation,\n            "can_execute": validation["is_valid"] and result.confidence > 0.5,\n            "timestamp": time.time()\n        }\n\n        # Add to command history\n        self.command_history.append(response)\n\n        return response\n\n    def start_continuous_listening(self, command_callback):\n        """Start continuous listening for speech commands"""\n        self.is_listening = True\n\n        def callback_wrapper(result):\n            if self.is_listening:\n                response = self.process_speech_command(audio_input=result)\n                if response["can_execute"]:\n                    command_callback(response)\n\n        # Start continuous recognition\n        self.speech_recognizer.continuous_recognition(callback_wrapper)\n\n    def stop_listening(self):\n        """Stop continuous listening"""\n        self.is_listening = False\n\n    def get_recent_commands(self, limit: int = 10) -> List[Dict[str, Any]]:\n        """Get recent speech commands"""\n        return self.command_history[-limit:]\n\ndef create_robot_speech_interface(model_size: str = "base") -> RobotSpeechInterface:\n    """Factory function to create a robot speech interface"""\n    return RobotSpeechInterface(model_size)\n\n# Example usage and testing\ndef test_speech_recognition():\n    """Test function for speech recognition"""\n    print("Testing Whisper-based speech recognition...")\n\n    # Create speech interface\n    speech_interface = create_robot_speech_interface("base")\n\n    # Test with sample text (in practice, you\'d record from microphone or load audio file)\n    # For demonstration, we\'ll simulate audio processing\n\n    print("Speech recognition system initialized successfully!")\n    print("You can now use this system to process speech commands for your humanoid robot.")\n\nif __name__ == "__main__":\n    test_speech_recognition()\n'})}),"\n",(0,o.jsx)(n.h3,{id:"advanced-speech-recognition-features",children:"Advanced Speech Recognition Features"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# advanced_speech_features.py\n\nimport torch\nimport whisper\nimport numpy as np\nfrom typing import Dict, Any, List, Optional\nimport asyncio\nimport threading\nfrom dataclasses import dataclass\nimport time\n\n@dataclass\nclass EnhancedRecognitionResult:\n    """Enhanced result with additional metadata"""\n    text: str\n    confidence: float\n    language: str\n    timestamp: float\n    audio_duration: float\n    word_timestamps: List[Dict[str, float]]  # Timestamps for each word\n    detected_language: str\n    temperature: float\n    no_speech_prob: float\n\nclass MultilingualSpeechRecognizer:\n    """\n    Multilingual speech recognition with language detection\n    """\n\n    def __init__(self, model_size: str = "medium"):\n        self.model = whisper.load_model(model_size)\n        self.supported_languages = {\n            "en": "english",\n            "es": "spanish",\n            "fr": "french",\n            "de": "german",\n            "it": "italian",\n            "pt": "portuguese",\n            "zh": "chinese",\n            "ja": "japanese",\n            "ko": "korean"\n        }\n\n    def detect_language(self, audio: np.ndarray) -> str:\n        """Detect the language of the audio"""\n        # Pad/trim audio to fit in memory\n        audio = whisper.pad_or_trim(audio)\n\n        # Make log-Mel spectrogram and move to the same device as the model\n        mel = whisper.log_mel_spectrogram(audio).to(self.model.device)\n\n        # Detect language\n        _, probs = self.model.detect_language(mel)\n        detected_lang = max(probs, key=probs.get)\n\n        return detected_lang\n\n    def transcribe_multilingual(self, audio: np.ndarray,\n                              detect_language: bool = True) -> EnhancedRecognitionResult:\n        """Transcribe audio with automatic language detection"""\n        if detect_language:\n            detected_lang = self.detect_language(audio)\n        else:\n            detected_lang = "en"  # Default to English\n\n        # Transcribe with detected language\n        result = self.model.transcribe(\n            audio,\n            language=detected_lang,\n            temperature=0.0,\n            word_timestamps=True\n        )\n\n        # Extract word-level timestamps if available\n        word_timestamps = []\n        if "segments" in result:\n            for segment in result["segments"]:\n                if "words" in segment:\n                    for word in segment["words"]:\n                        word_timestamps.append({\n                            "text": word.get("word", ""),\n                            "start": word.get("start", 0.0),\n                            "end": word.get("end", 0.0)\n                        })\n\n        # Estimate confidence based on no_speech_prob\n        no_speech_prob = result.get("no_speech_prob", 0.0)\n        confidence = 1.0 - no_speech_prob\n\n        return EnhancedRecognitionResult(\n            text=result["text"],\n            confidence=confidence,\n            language=result.get("language", "unknown"),\n            timestamp=time.time(),\n            audio_duration=len(audio) / 16000,\n            word_timestamps=word_timestamps,\n            detected_language=detected_lang,\n            temperature=0.0,\n            no_speech_prob=no_speech_prob\n        )\n\nclass NoiseRobustRecognizer:\n    """\n    Speech recognition optimized for noisy environments\n    """\n\n    def __init__(self, base_model_size: str = "base"):\n        self.model = whisper.load_model(base_model_size)\n        self.noise_threshold = 0.01  # RMS threshold for speech detection\n\n    def preprocess_for_noise(self, audio: np.ndarray) -> np.ndarray:\n        """Preprocess audio to handle noise"""\n        import librosa\n\n        # Apply noise reduction\n        # This is a simplified approach - in practice, you might use more sophisticated techniques\n        reduced_audio = librosa.effects.percussive(audio)\n\n        # Normalize\n        reduced_audio = reduced_audio / max(np.max(reduced_audio), abs(np.min(reduced_audio))) if np.max(reduced_audio) != 0 else reduced_audio\n\n        return reduced_audio\n\n    def vad_segmentation(self, audio: np.ndarray, sample_rate: int = 16000) -> List[np.ndarray]:\n        """Voice activity detection to segment speech from noise"""\n        import librosa\n\n        # Simple VAD based on energy\n        frame_length = 2048\n        hop_length = 512\n\n        # Calculate frame energy\n        frames = librosa.util.frame(audio, frame_length=frame_length, hop_length=hop_length)\n        frame_energy = np.sum(frames**2, axis=0)\n\n        # Normalize energy\n        normalized_energy = frame_energy / np.max(frame_energy) if np.max(frame_energy) > 0 else frame_energy\n\n        # Identify speech frames (above threshold)\n        speech_frames = normalized_energy > self.noise_threshold\n\n        # Group consecutive speech frames into segments\n        segments = []\n        current_segment = []\n\n        for i, is_speech in enumerate(speech_frames):\n            if is_speech:\n                current_segment.append(i)\n            else:\n                if len(current_segment) > 0:\n                    # Convert frame indices back to audio samples\n                    start_frame = current_segment[0] * hop_length\n                    end_frame = current_segment[-1] * hop_length + frame_length\n                    segment_audio = audio[start_frame:end_frame]\n                    segments.append(segment_audio)\n                    current_segment = []\n\n        # Add final segment if exists\n        if len(current_segment) > 0:\n            start_frame = current_segment[0] * hop_length\n            end_frame = current_segment[-1] * hop_length + frame_length\n            segment_audio = audio[start_frame:end_frame]\n            segments.append(segment_audio)\n\n        return segments\n\n    def recognize_in_noise(self, audio: np.ndarray) -> List[EnhancedRecognitionResult]:\n        """Recognize speech in noisy conditions"""\n        # Preprocess audio\n        clean_audio = self.preprocess_for_noise(audio)\n\n        # Perform VAD segmentation\n        segments = self.vad_segmentation(clean_audio)\n\n        results = []\n        for segment in segments:\n            # Transcribe each segment\n            result = self.model.transcribe(segment, temperature=0.0)\n\n            # Create result object\n            enhanced_result = EnhancedRecognitionResult(\n                text=result["text"],\n                confidence=0.8,  # Placeholder confidence\n                language=result.get("language", "en"),\n                timestamp=time.time(),\n                audio_duration=len(segment) / 16000,\n                word_timestamps=[],\n                detected_language=result.get("language", "en"),\n                temperature=0.0,\n                no_speech_prob=result.get("no_speech_prob", 0.0)\n            )\n\n            results.append(enhanced_result)\n\n        return results\n\nclass KeywordSpotting:\n    """\n    Keyword spotting for robot wake word detection\n    """\n\n    def __init__(self, wake_words: List[str] = None):\n        self.wake_words = wake_words or ["robot", "hey robot", "hello robot", "assistant"]\n        self.activation_threshold = 0.7\n\n    def detect_wake_word(self, text: str) -> bool:\n        """Check if text contains a wake word"""\n        text_lower = text.lower().strip()\n\n        for wake_word in self.wake_words:\n            if wake_word in text_lower:\n                return True\n\n        return False\n\n    def extract_command_after_wake(self, text: str) -> str:\n        """Extract the command after the wake word"""\n        text_lower = text.lower().strip()\n\n        for wake_word in self.wake_words:\n            if wake_word in text_lower:\n                # Remove the wake word and return the rest\n                command = text_lower.replace(wake_word, "", 1).strip()\n                return command\n\n        # If no wake word found, return original text\n        return text\n\nclass SpeechRecognitionPipeline:\n    """\n    Complete pipeline for speech recognition in robotics\n    """\n\n    def __init__(self, model_size: str = "base"):\n        self.multilingual_recognizer = MultilingualSpeechRecognizer(model_size)\n        self.noise_robust_recognizer = NoiseRobustRecognizer(model_size)\n        self.keyword_spotter = KeywordSpotting()\n        self.is_active = False\n\n    def process_audio_stream(self, audio_chunk: np.ndarray, is_noisy: bool = False) -> Dict[str, Any]:\n        """Process a chunk of audio in the pipeline"""\n        start_time = time.time()\n\n        # Determine if we need noise-robust processing\n        if is_noisy:\n            results = self.noise_robust_recognizer.recognize_in_noise(audio_chunk)\n        else:\n            result = self.multilingual_recognizer.transcribe_multilingual(audio_chunk)\n            results = [result]\n\n        # Process each result\n        processed_results = []\n        for result in results:\n            # Check for wake word\n            has_wake_word = self.keyword_spotter.detect_wake_word(result.text)\n            command = self.keyword_spotter.extract_command_after_wake(result.text)\n\n            processed_result = {\n                "original_text": result.text,\n                "command": command,\n                "has_wake_word": has_wake_word,\n                "confidence": result.confidence,\n                "language": result.detected_language,\n                "is_robot_command": has_wake_word and len(command.strip()) > 0\n            }\n\n            processed_results.append(processed_result)\n\n        processing_time = time.time() - start_time\n\n        return {\n            "results": processed_results,\n            "processing_time": processing_time,\n            "timestamp": time.time()\n        }\n\ndef create_advanced_speech_pipeline(model_size: str = "base") -> SpeechRecognitionPipeline:\n    """Factory function to create an advanced speech recognition pipeline"""\n    return SpeechRecognitionPipeline(model_size)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"examples",children:"Examples"}),"\n",(0,o.jsx)(n.h3,{id:"example-1-robot-command-recognition-system",children:"Example 1: Robot Command Recognition System"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# robot_command_recognition.py\n\nimport numpy as np\nimport asyncio\nfrom typing import Dict, Any, Callable\nimport time\n\nclass RobotCommandRecognitionSystem:\n    """\n    Complete system for recognizing and executing robot commands from speech\n    """\n\n    def __init__(self):\n        self.speech_pipeline = create_advanced_speech_pipeline("base")\n        self.robot_interface = None  # This would connect to actual robot\n        self.command_queue = []\n        self.is_running = False\n\n    def set_robot_interface(self, robot_interface):\n        """Set the robot interface for command execution"""\n        self.robot_interface = robot_interface\n\n    async def process_speech_command(self, audio_data: np.ndarray) -> Dict[str, Any]:\n        """Process speech command and prepare for execution"""\n        # Process through the speech pipeline\n        pipeline_result = self.speech_pipeline.process_audio_stream(audio_data)\n\n        # Extract the highest confidence robot command\n        robot_commands = [\n            result for result in pipeline_result["results"]\n            if result["is_robot_command"] and result["confidence"] > 0.6\n        ]\n\n        if robot_commands:\n            # Sort by confidence and get the best one\n            best_command = max(robot_commands, key=lambda x: x["confidence"])\n\n            # Parse the command for robot execution\n            parsed_command = self._parse_robot_command(best_command["command"])\n\n            return {\n                "success": True,\n                "command": parsed_command,\n                "confidence": best_command["confidence"],\n                "raw_text": best_command["original_text"],\n                "processing_time": pipeline_result["processing_time"]\n            }\n        else:\n            return {\n                "success": False,\n                "command": None,\n                "confidence": 0.0,\n                "raw_text": "",\n                "processing_time": pipeline_result["processing_time"],\n                "error": "No valid robot command detected"\n            }\n\n    def _parse_robot_command(self, command_text: str) -> Dict[str, Any]:\n        """Parse natural language command into robot actions"""\n        command_text = command_text.lower().strip()\n\n        # Define command patterns\n        if any(word in command_text for word in ["move", "go", "navigate", "walk"]):\n            # Navigation command\n            target_location = self._extract_location(command_text)\n            return {\n                "action": "navigation",\n                "target": target_location,\n                "parameters": {"speed": 0.5}\n            }\n        elif any(word in command_text for word in ["pick", "grasp", "take", "lift"]):\n            # Manipulation command\n            target_object = self._extract_object(command_text)\n            return {\n                "action": "manipulation",\n                "target": target_object,\n                "parameters": {"gripper_position": 0.8}\n            }\n        elif any(word in command_text for word in ["place", "put", "drop", "release"]):\n            # Placement command\n            target_location = self._extract_location(command_text)\n            return {\n                "action": "placement",\n                "target": target_location,\n                "parameters": {"gripper_position": 0.0}\n            }\n        else:\n            # Unknown command\n            return {\n                "action": "unknown",\n                "target": command_text,\n                "parameters": {}\n            }\n\n    def _extract_location(self, command: str) -> str:\n        """Extract location from command"""\n        # Simple keyword-based extraction\n        locations = ["table", "shelf", "desk", "floor", "box", "cabinet"]\n\n        for location in locations:\n            if location in command:\n                return location\n\n        return "unknown_location"\n\n    def _extract_object(self, command: str) -> str:\n        """Extract object from command"""\n        # Simple keyword-based extraction\n        objects = ["cube", "ball", "bottle", "cup", "object", "item", "box"]\n\n        for obj in objects:\n            if obj in command:\n                return obj\n\n        return "unknown_object"\n\n    async def execute_robot_command(self, command_result: Dict[str, Any]) -> Dict[str, Any]:\n        """Execute the parsed robot command"""\n        if not command_result["success"]:\n            return {\n                "success": False,\n                "error": command_result.get("error", "No command to execute")\n            }\n\n        command = command_result["command"]\n\n        # In a real system, this would interface with the robot\n        # For simulation, we\'ll just return success\n        execution_result = {\n            "action": command["action"],\n            "target": command["target"],\n            "parameters": command["parameters"],\n            "status": "executed",  # In real system, this would come from robot feedback\n            "timestamp": time.time()\n        }\n\n        return {\n            "success": True,\n            "execution_result": execution_result,\n            "original_command": command_result["raw_text"]\n        }\n\n    async def continuous_command_processing(self, audio_source_func: Callable,\n                                          command_callback: Callable = None):\n        """Continuously process commands from an audio source"""\n        self.is_running = True\n\n        while self.is_running:\n            try:\n                # Get audio data from source\n                audio_data = await audio_source_func()\n\n                if audio_data is not None and len(audio_data) > 0:\n                    # Process the speech command\n                    command_result = await self.process_speech_command(audio_data)\n\n                    if command_result["success"]:\n                        # Execute the command\n                        execution_result = await self.execute_robot_command(command_result)\n\n                        # Call the callback if provided\n                        if command_callback:\n                            await command_callback(execution_result)\n\n                        # Add to command queue\n                        self.command_queue.append(execution_result)\n\n                        print(f"Executed command: {command_result[\'raw_text\']}")\n                    else:\n                        print(f"Command not recognized: {command_result.get(\'error\', \'Unknown error\')}")\n\n                # Small delay to prevent overwhelming the system\n                await asyncio.sleep(0.1)\n\n            except Exception as e:\n                print(f"Error in command processing: {e}")\n                await asyncio.sleep(0.5)  # Brief pause before continuing\n\n    def stop_processing(self):\n        """Stop continuous command processing"""\n        self.is_running = False\n\n    def get_command_history(self, limit: int = 10) -> List[Dict[str, Any]]:\n        """Get recent command history"""\n        return self.command_queue[-limit:]\n\ndef simulate_audio_source():\n    """Simulate an audio source for testing"""\n    # In a real implementation, this would capture from microphone\n    # For simulation, return empty array\n    return np.array([])\n\nasync def command_execution_callback(execution_result: Dict[str, Any]):\n    """Callback function for executed commands"""\n    print(f"Command executed: {execution_result[\'execution_result\'][\'action\']} "\n          f"to {execution_result[\'execution_result\'][\'target\']}")\n\nasync def main():\n    """Main function to demonstrate robot command recognition"""\n    print("Initializing Robot Command Recognition System...")\n\n    # Create the system\n    robot_system = RobotCommandRecognitionSystem()\n\n    print("System initialized. Ready to process speech commands.")\n    print("In a real implementation, this would connect to a robot interface.")\n\n    # Example of processing a single command\n    # In practice, you would get this from a microphone or audio file\n    sample_audio = np.random.random(16000 * 3)  # 3 seconds of random audio for demo\n\n    result = await robot_system.process_speech_command(sample_audio)\n    print(f"Processing result: {result}")\n\nif __name__ == "__main__":\n    asyncio.run(main())\n'})}),"\n",(0,o.jsx)(n.h3,{id:"example-2-speech-recognition-with-error-handling-and-validation",children:"Example 2: Speech Recognition with Error Handling and Validation"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# speech_error_handling.py\n\nimport numpy as np\nimport asyncio\nfrom typing import Dict, Any, List, Optional\nimport logging\nfrom dataclasses import dataclass\n\n@dataclass\nclass RecognitionError:\n    """Information about recognition errors"""\n    error_type: str\n    error_message: str\n    confidence: float\n    timestamp: float\n    audio_context: Optional[np.ndarray] = None\n\nclass SpeechRecognitionValidator:\n    """\n    Validate and handle errors in speech recognition\n    """\n\n    def __init__(self):\n        self.error_log = []\n        self.confidence_threshold = 0.6\n        self.min_audio_length = 0.5  # seconds\n        self.max_audio_length = 30.0  # seconds\n\n    def validate_audio_input(self, audio: np.ndarray, sample_rate: int = 16000) -> Dict[str, Any]:\n        """Validate audio input before processing"""\n        validation_result = {\n            "is_valid": True,\n            "issues": [],\n            "suggested_action": "proceed"\n        }\n\n        # Check audio length\n        audio_duration = len(audio) / sample_rate\n        if audio_duration < self.min_audio_length:\n            validation_result["is_valid"] = False\n            validation_result["issues"].append({\n                "type": "too_short",\n                "message": f"Audio too short: {audio_duration:.2f}s (min: {self.min_audio_length}s)",\n                "severity": "high"\n            })\n            validation_result["suggested_action"] = "request_longer_audio"\n        elif audio_duration > self.max_audio_length:\n            validation_result["is_valid"] = False\n            validation_result["issues"].append({\n                "type": "too_long",\n                "message": f"Audio too long: {audio_duration:.2f}s (max: {self.max_audio_length}s)",\n                "severity": "medium"\n            })\n            validation_result["suggested_action"] = "truncate_or_split"\n\n        # Check for silence\n        rms_energy = np.sqrt(np.mean(audio ** 2))\n        if rms_energy < 0.001:  # Very low energy indicates silence\n            validation_result["is_valid"] = False\n            validation_result["issues"].append({\n                "type": "silence",\n                "message": f"Audio appears to be silent (RMS: {rms_energy:.6f})",\n                "severity": "high"\n            })\n            validation_result["suggested_action"] = "request_new_input"\n\n        # Check for clipping\n        max_amplitude = np.max(np.abs(audio))\n        if max_amplitude > 0.9:  # Close to maximum range indicates potential clipping\n            validation_result["issues"].append({\n                "type": "clipping",\n                "message": f"Audio may be clipped (max amplitude: {max_amplitude:.3f})",\n                "severity": "low"\n            })\n\n        return validation_result\n\n    def validate_recognition_result(self, result: Dict[str, Any],\n                                  original_audio: np.ndarray) -> Dict[str, Any]:\n        """Validate the recognition result"""\n        validation = {\n            "is_valid": True,\n            "confidence_score": result.get("confidence", 0.0),\n            "issues": [],\n            "suggested_action": "accept"\n        }\n\n        # Check confidence\n        if result.get("confidence", 0.0) < self.confidence_threshold:\n            validation["is_valid"] = False\n            validation["issues"].append({\n                "type": "low_confidence",\n                "message": f"Recognition confidence too low: {result.get(\'confidence\', 0.0):.3f}",\n                "severity": "high"\n            })\n            validation["suggested_action"] = "request_confirmation"\n\n        # Check for empty result\n        text = result.get("text", "").strip()\n        if not text:\n            validation["is_valid"] = False\n            validation["issues"].append({\n                "type": "empty_result",\n                "message": "No text recognized",\n                "severity": "high"\n            })\n            validation["suggested_action"] = "request_new_input"\n\n        # Check for potential errors in the recognized text\n        if self._detect_recognition_errors(text):\n            validation["issues"].append({\n                "type": "potential_error",\n                "message": "Recognition may contain errors",\n                "severity": "medium"\n            })\n            validation["suggested_action"] = "request_confirmation"\n\n        return validation\n\n    def _detect_recognition_errors(self, text: str) -> bool:\n        """Detect potential recognition errors in text"""\n        # Check for repeated words (common recognition error)\n        words = text.split()\n        if len(words) >= 4:\n            for i in range(len(words) - 3):\n                if words[i] == words[i+1] == words[i+2]:\n                    return True\n\n        # Check for non-sensical sequences\n        non_sensical_patterns = [\n            "um um um", "uh uh uh", "ah ah ah",  # Repeated filler words\n            "the the the", "and and and"  # Repeated common words\n        ]\n\n        text_lower = text.lower()\n        for pattern in non_sensical_patterns:\n            if pattern in text_lower:\n                return True\n\n        return False\n\n    def log_error(self, error: RecognitionError):\n        """Log recognition errors for analysis"""\n        self.error_log.append(error)\n\n        # Keep only recent errors (last 1000)\n        if len(self.error_log) > 1000:\n            self.error_log = self.error_log[-1000:]\n\n    def get_error_statistics(self) -> Dict[str, Any]:\n        """Get statistics about recognition errors"""\n        if not self.error_log:\n            return {"total_errors": 0}\n\n        error_types = {}\n        for error in self.error_log:\n            error_types[error.error_type] = error_types.get(error.error_type, 0) + 1\n\n        return {\n            "total_errors": len(self.error_log),\n            "error_types": error_types,\n            "recent_errors": len([e for e in self.error_log if time.time() - e.timestamp < 3600])  # Last hour\n        }\n\nclass RobustSpeechRecognitionSystem:\n    """\n    Robust speech recognition with comprehensive error handling\n    """\n\n    def __init__(self):\n        self.speech_pipeline = create_advanced_speech_pipeline("base")\n        self.validator = SpeechRecognitionValidator()\n        self.error_handlers = self._initialize_error_handlers()\n        self.recovery_strategies = self._initialize_recovery_strategies()\n\n    def _initialize_error_handlers(self) -> Dict[str, Callable]:\n        """Initialize error handling strategies"""\n        return {\n            "low_confidence": self._handle_low_confidence,\n            "empty_result": self._handle_empty_result,\n            "silence": self._handle_silence,\n            "too_short": self._handle_too_short,\n            "too_long": self._handle_too_long\n        }\n\n    def _initialize_recovery_strategies(self) -> Dict[str, Callable]:\n        """Initialize recovery strategies"""\n        return {\n            "request_confirmation": self._request_confirmation,\n            "request_new_input": self._request_new_input,\n            "truncate_or_split": self._truncate_audio,\n            "request_longer_audio": self._request_longer_audio\n        }\n\n    async def robust_recognition(self, audio: np.ndarray) -> Dict[str, Any]:\n        """Perform robust speech recognition with error handling"""\n        # Step 1: Validate audio input\n        audio_validation = self.validator.validate_audio_input(audio)\n\n        if not audio_validation["is_valid"]:\n            # Handle audio validation errors\n            error_type = audio_validation["issues"][0]["type"]\n            if error_type in self.error_handlers:\n                return await self.error_handlers[error_type](audio, audio_validation)\n\n        # Step 2: Perform recognition\n        try:\n            pipeline_result = self.speech_pipeline.process_audio_stream(audio)\n\n            # Get the best result\n            if pipeline_result["results"]:\n                best_result = max(pipeline_result["results"],\n                                key=lambda x: x.get("confidence", 0.0))\n\n                # Step 3: Validate recognition result\n                result_validation = self.validator.validate_recognition_result(\n                    best_result, audio\n                )\n\n                if result_validation["is_valid"]:\n                    return {\n                        "success": True,\n                        "text": best_result["original_text"],\n                        "confidence": best_result["confidence"],\n                        "command": best_result["command"] if "command" in best_result else "",\n                        "validation": result_validation\n                    }\n                else:\n                    # Handle recognition validation errors\n                    action = result_validation["suggested_action"]\n                    if action in self.recovery_strategies:\n                        return await self.recovery_strategies[action](\n                            best_result, result_validation\n                        )\n                    else:\n                        return {\n                            "success": False,\n                            "error": f"Unrecognized validation issue: {action}",\n                            "confidence": best_result.get("confidence", 0.0)\n                        }\n            else:\n                return {\n                    "success": False,\n                    "error": "No results from speech pipeline",\n                    "confidence": 0.0\n                }\n\n        except Exception as e:\n            # Log the error\n            error_info = RecognitionError(\n                error_type="recognition_exception",\n                error_message=str(e),\n                confidence=0.0,\n                timestamp=time.time()\n            )\n            self.validator.log_error(error_info)\n\n            return {\n                "success": False,\n                "error": f"Recognition exception: {str(e)}",\n                "confidence": 0.0\n            }\n\n    async def _handle_low_confidence(self, audio: np.ndarray, validation: Dict[str, Any]) -> Dict[str, Any]:\n        """Handle low confidence recognition"""\n        # Try with different model settings\n        try:\n            # For demonstration, we\'ll just return a low confidence result\n            # In practice, you might try different temperature settings or models\n            result = self.speech_pipeline.noise_robust_recognizer.recognize_in_noise(audio)\n\n            if result:\n                best_result = max(result, key=lambda x: x.confidence)\n                return {\n                    "success": True,\n                    "text": best_result.text,\n                    "confidence": best_result.confidence,\n                    "validation": validation,\n                    "warning": "Low confidence result - consider requesting confirmation"\n                }\n        except:\n            pass\n\n        return {\n            "success": False,\n            "error": "Low confidence recognition failed",\n            "confidence": 0.0\n        }\n\n    async def _handle_empty_result(self, audio: np.ndarray, validation: Dict[str, Any]) -> Dict[str, Any]:\n        """Handle empty recognition result"""\n        return {\n            "success": False,\n            "error": "No speech detected in audio",\n            "confidence": 0.0,\n            "suggested_action": "request_new_input"\n        }\n\n    async def _request_confirmation(self, result: Dict[str, Any], validation: Dict[str, Any]) -> Dict[str, Any]:\n        """Request user confirmation for uncertain recognition"""\n        return {\n            "success": True,\n            "text": result.get("original_text", ""),\n            "confidence": result.get("confidence", 0.0),\n            "requires_confirmation": True,\n            "suggested_confirmation_text": f"Did you say: \'{result.get(\'original_text\', \'\')}\'?",\n            "validation": validation\n        }\n\n    async def _request_new_input(self, result: Dict[str, Any] = None,\n                                validation: Dict[str, Any] = None) -> Dict[str, Any]:\n        """Request new audio input"""\n        return {\n            "success": False,\n            "error": "Invalid input detected",\n            "requires_new_input": True,\n            "suggested_action": "Please speak again more clearly"\n        }\n\n    def get_system_status(self) -> Dict[str, Any]:\n        """Get the status of the recognition system"""\n        return {\n            "error_statistics": self.validator.get_error_statistics(),\n            "confidence_threshold": self.validator.confidence_threshold,\n            "validation_rules": {\n                "min_audio_length": self.validator.min_audio_length,\n                "max_audio_length": self.validator.max_audio_length\n            }\n        }\n\ndef main():\n    """Main function for speech recognition error handling"""\n    print("Initializing Robust Speech Recognition System...")\n\n    # Create the robust system\n    robust_system = RobustSpeechRecognitionSystem()\n\n    print("System initialized with error handling capabilities.")\n    print("Status:", robust_system.get_system_status())\n\n    print("\\nIn a real implementation, this system would handle various recognition errors")\n    print("and provide appropriate recovery strategies for robotic applications.")\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"Speech recognition is a critical component of Vision-Language-Action systems, enabling natural human-robot interaction through spoken commands. Key aspects include:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"OpenAI Whisper"}),": State-of-the-art speech recognition model that provides high accuracy across multiple languages"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Audio Processing"}),": Proper handling of audio input, preprocessing, and noise reduction for robotics applications"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Error Handling"}),": Robust validation and error recovery mechanisms for reliable operation"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Integration"}),": Seamless connection between speech recognition and robot command execution systems"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Real-time Performance"}),": Efficient processing for responsive robot interaction"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,o.jsx)(n.h3,{id:"conceptual",children:"Conceptual"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Compare and contrast different speech recognition approaches (traditional HMM-based vs. modern transformer-based like Whisper). What are the advantages and disadvantages of each for robotics applications?"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"logical",children:"Logical"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Design a speech recognition pipeline that can handle multiple speakers in a household robotics scenario. How would you incorporate speaker identification and personalized recognition for different family members?"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"implementation-1",children:"Implementation"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Implement a complete speech recognition system with Wake Word detection, Whisper-based transcription, and command validation for a humanoid robot, including proper error handling and confidence scoring."}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453(e,n,t){t.d(n,{R:()=>s,x:()=>a});var i=t(6540);const o={},r=i.createContext(o);function s(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);