"use strict";(globalThis.webpackChunkphysical_ai_book_humanoid=globalThis.webpackChunkphysical_ai_book_humanoid||[]).push([[8502],{2236(n,e,t){t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>p,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-3-ai-brain/chapter-5-reinforcement-learning","title":"Reinforcement Learning for Robotics with Isaac Sim","description":"Implementing reinforcement learning algorithms for humanoid robot control using Isaac Sim and NVIDIA Omniverse","source":"@site/docs/module-3-ai-brain/chapter-5-reinforcement-learning.md","sourceDirName":"module-3-ai-brain","slug":"/module-3-ai-brain/chapter-5-reinforcement-learning","permalink":"/textbook/docs/module-3-ai-brain/chapter-5-reinforcement-learning","draft":false,"unlisted":false,"editUrl":"https://github.com/Panaversity/physical_ai_book_humanoid/tree/main/docs/module-3-ai-brain/chapter-5-reinforcement-learning.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"title":"Reinforcement Learning for Robotics with Isaac Sim","sidebar_position":6,"description":"Implementing reinforcement learning algorithms for humanoid robot control using Isaac Sim and NVIDIA Omniverse","keywords":["reinforcement learning","robotics","Isaac Sim","NVIDIA Omniverse","humanoid control","AI training"]},"sidebar":"tutorialSidebar","previous":{"title":"Navigation and Path Planning with Isaac ROS","permalink":"/textbook/docs/module-3-ai-brain/chapter-4-nav2-planning"},"next":{"title":"Sim-to-Real Transfer for Humanoid Robots","permalink":"/textbook/docs/module-3-ai-brain/chapter-6-sim-to-real"}}');var o=t(4848),a=t(8453);const r={title:"Reinforcement Learning for Robotics with Isaac Sim",sidebar_position:6,description:"Implementing reinforcement learning algorithms for humanoid robot control using Isaac Sim and NVIDIA Omniverse",keywords:["reinforcement learning","robotics","Isaac Sim","NVIDIA Omniverse","humanoid control","AI training"]},s="Chapter 5: Reinforcement Learning for Robotics with Isaac Sim",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Core Concepts",id:"core-concepts",level:2},{value:"RL Framework for Robotics",id:"rl-framework-for-robotics",level:3},{value:"Isaac Sim RL Integration",id:"isaac-sim-rl-integration",level:3},{value:"Implementation",id:"implementation",level:2},{value:"Isaac Sim RL Environment Setup",id:"isaac-sim-rl-environment-setup",level:3},{value:"Isaac Sim RL Training Pipeline",id:"isaac-sim-rl-training-pipeline",level:3},{value:"Examples",id:"examples",level:2},{value:"Example 1: Humanoid Locomotion Training with PPO",id:"example-1-humanoid-locomotion-training-with-ppo",level:3},{value:"Example 2: RL Policy Validation and Transfer",id:"example-2-rl-policy-validation-and-transfer",level:3},{value:"Summary",id:"summary",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Conceptual",id:"conceptual",level:3},{value:"Logical",id:"logical",level:3},{value:"Implementation",id:"implementation-1",level:3}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"chapter-5-reinforcement-learning-for-robotics-with-isaac-sim",children:"Chapter 5: Reinforcement Learning for Robotics with Isaac Sim"})}),"\n",(0,o.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(e.p,{children:"By the end of this chapter, students will be able to:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Understand the fundamentals of reinforcement learning for robotics applications"}),"\n",(0,o.jsx)(e.li,{children:"Implement RL algorithms for humanoid robot control using Isaac Sim"}),"\n",(0,o.jsx)(e.li,{children:"Design reward functions for complex robotic tasks"}),"\n",(0,o.jsx)(e.li,{children:"Train and validate RL policies in simulation before deployment"}),"\n",(0,o.jsx)(e.li,{children:"Apply transfer learning techniques from simulation to real robots"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,o.jsx)(e.p,{children:"Students should have:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Understanding of basic machine learning concepts and algorithms"}),"\n",(0,o.jsx)(e.li,{children:"Knowledge of humanoid robot kinematics and dynamics (covered in Module 1)"}),"\n",(0,o.jsx)(e.li,{children:"Experience with Isaac Sim simulation environment (covered in Chapter 1)"}),"\n",(0,o.jsx)(e.li,{children:"Basic Python programming skills for AI/ML frameworks"}),"\n",(0,o.jsx)(e.li,{children:"Familiarity with control theory concepts"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"core-concepts",children:"Core Concepts"}),"\n",(0,o.jsx)(e.p,{children:"Reinforcement learning (RL) provides a framework for training robots to perform complex tasks through trial and error, using reward signals to guide behavior. In robotics, RL enables the learning of control policies that can handle complex, dynamic environments and tasks that are difficult to program explicitly."}),"\n",(0,o.jsx)(e.h3,{id:"rl-framework-for-robotics",children:"RL Framework for Robotics"}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"Agent-Environment Interaction:"})}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"State Space (S)"}),": Robot's configuration, sensor readings, and environmental context"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Action Space (A)"}),": Joint torques, velocities, or position commands"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Reward Function (R)"}),": Scalar feedback for task success, efficiency, and safety"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Policy (\u03c0)"}),": Mapping from states to actions that maximizes expected reward"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Environment Dynamics"}),": Robot kinematics, dynamics, and environmental physics"]}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"RL Algorithms for Robotics:"})}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Deep Q-Networks (DQN)"}),": For discrete action spaces and contact-rich tasks"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Actor-Critic Methods"}),": For continuous control of joint torques and velocities"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Proximal Policy Optimization (PPO)"}),": Stable policy gradient method for robotics"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Soft Actor-Critic (SAC)"}),": Maximum entropy RL for exploration and robustness"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Twin Delayed DDPG (TD3)"}),": Off-policy method for continuous control"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"isaac-sim-rl-integration",children:"Isaac Sim RL Integration"}),"\n",(0,o.jsx)(e.p,{children:"Isaac Sim provides specialized RL capabilities:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Isaac Gym"}),": GPU-accelerated physics simulation for parallel training"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"RL Environments"}),": Pre-built environments for manipulation and locomotion"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Observation Spaces"}),": Integrated sensor data, kinematics, and dynamics"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Action Spaces"}),": Joint control interfaces with various command types"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Reward Functions"}),": Configurable reward computation graphs"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"implementation",children:"Implementation"}),"\n",(0,o.jsx)(e.p,{children:"Let's implement reinforcement learning for humanoid robot control using Isaac Sim:"}),"\n",(0,o.jsx)(e.h3,{id:"isaac-sim-rl-environment-setup",children:"Isaac Sim RL Environment Setup"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n# rl_environment.py\n\nimport numpy as np\nimport torch\nimport gym\nfrom gym import spaces\nfrom pxr import Usd, UsdGeom, Gf\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom omni.isaac.core.utils.prims import get_prim_at_path\nfrom omni.isaac.core.articulations import Articulation\nfrom omni.isaac.core.utils.torch.maths import torch_acos, torch_cross, torch_normalize\nimport torch.nn as nn\nimport torch.optim as optim\nfrom typing import Dict, Tuple, Any\n\nclass HumanoidRLEnvironment:\n    """\n    Reinforcement learning environment for humanoid robot control using Isaac Sim\n    """\n\n    def __init__(self, world: World, robot_path: str = "/World/Franka/robot"):\n        self.world = world\n        self.robot = self.world.scene.get_articulation(robot_path)\n\n        # RL environment parameters\n        self.max_episode_length = 1000\n        self.current_step = 0\n        self.episode_reward = 0.0\n\n        # Define action and observation spaces\n        self.num_actions = len(self.robot.dof_names)\n        self.num_observations = 14 + 14 + 3 + 3 + 3  # joint_pos + joint_vel + root_pos + root_rot + target_pos\n\n        # Action space: joint position targets (normalized to [-1, 1])\n        self.action_space = spaces.Box(\n            low=-1.0,\n            high=1.0,\n            shape=(self.num_actions,),\n            dtype=np.float32\n        )\n\n        # Observation space: joint positions, velocities, root pose, target position\n        self.observation_space = spaces.Box(\n            low=-np.inf,\n            high=np.inf,\n            shape=(self.num_observations,),\n            dtype=np.float32\n        )\n\n        # Robot state tracking\n        self.initial_joint_positions = None\n        self.target_position = np.array([0.5, 0.0, 0.0])  # Target position in world frame\n\n        self.get_logger().info(\'Humanoid RL Environment initialized\')\n\n    def reset(self) -> np.ndarray:\n        """Reset the environment to initial state"""\n        # Reset robot to initial configuration\n        if self.initial_joint_positions is None:\n            self.initial_joint_positions = self.robot.get_joint_positions()\n\n        self.robot.set_joint_positions(self.initial_joint_positions)\n        self.current_step = 0\n        self.episode_reward = 0.0\n\n        # Randomize target position for training diversity\n        self.target_position = np.random.uniform(low=-0.5, high=0.5, size=3)\n        self.target_position[2] = 0.0  # Keep target on ground plane\n\n        return self.get_observation()\n\n    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, Dict]:\n        """Execute one step of the environment"""\n        # Apply action to robot\n        self.apply_action(action)\n\n        # Step the physics simulation\n        self.world.step(render=True)\n\n        # Get current observation\n        observation = self.get_observation()\n\n        # Calculate reward\n        reward = self.calculate_reward()\n        self.episode_reward += reward\n\n        # Check if episode is done\n        done = self.current_step >= self.max_episode_length\n        self.current_step += 1\n\n        # Additional info for debugging\n        info = {\n            \'episode_reward\': self.episode_reward,\n            \'distance_to_target\': np.linalg.norm(observation[28:31] - self.target_position)\n        }\n\n        return observation, reward, done, info\n\n    def apply_action(self, action: np.ndarray):\n        """Apply action to the robot"""\n        # Convert normalized action to joint position targets\n        joint_targets = action * 0.5  # Scale to reasonable range\n\n        # Apply position control to robot\n        self.robot.set_joint_position_targets(joint_targets)\n\n    def get_observation(self) -> np.ndarray:\n        """Get current observation from environment"""\n        # Get robot state\n        joint_positions = self.robot.get_joint_positions()\n        joint_velocities = self.robot.get_joint_velocities()\n        root_position = self.robot.get_world_poses()[0][0].cpu().numpy()\n        root_rotation = self.robot.get_world_poses()[1][0].cpu().numpy()\n\n        # Create observation vector\n        obs = np.concatenate([\n            joint_positions,      # 14 joint positions\n            joint_velocities,     # 14 joint velocities\n            root_position,        # 3 root position\n            root_rotation,        # 4 root rotation (quaternion)\n            self.target_position  # 3 target position\n        ])\n\n        return obs\n\n    def calculate_reward(self) -> float:\n        """Calculate reward based on current state"""\n        # Get current position\n        current_pos = self.robot.get_world_poses()[0][0].cpu().numpy()\n\n        # Distance to target reward\n        distance_to_target = np.linalg.norm(current_pos - self.target_position)\n        distance_reward = -distance_to_target  # Negative distance encourages approach\n\n        # Velocity penalty to encourage smooth motion\n        joint_velocities = self.robot.get_joint_velocities()\n        velocity_penalty = -0.01 * np.sum(np.square(joint_velocities))\n\n        # Energy efficiency reward (penalize high torques)\n        joint_efforts = self.robot.get_measured_joint_efforts()\n        energy_penalty = -0.001 * np.sum(np.abs(joint_efforts))\n\n        # Safety reward (penalize joint limits)\n        joint_positions = self.robot.get_joint_positions()\n        joint_limits = self.robot.get_dof_properties()[\'upper\'][:len(joint_positions)]\n        safety_penalty = 0.0\n        for i, (pos, limit) in enumerate(zip(joint_positions, joint_limits)):\n            if abs(pos) > 0.9 * abs(limit):  # Within 10% of joint limit\n                safety_penalty -= 0.1\n\n        total_reward = distance_reward + velocity_penalty + energy_penalty + safety_penalty\n        return total_reward\n\n    def get_logger(self):\n        """Get logger instance"""\n        import logging\n        return logging.getLogger(__name__)\n\nclass PPOAgent(nn.Module):\n    """\n    Proximal Policy Optimization agent for humanoid control\n    """\n\n    def __init__(self, observation_dim: int, action_dim: int, hidden_dim: int = 256):\n        super(PPOAgent, self).__init__()\n\n        # Actor network (policy)\n        self.actor = nn.Sequential(\n            nn.Linear(observation_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, action_dim),\n            nn.Tanh()  # Output actions in [-1, 1] range\n        )\n\n        # Critic network (value function)\n        self.critic = nn.Sequential(\n            nn.Linear(observation_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)  # Output value estimate\n        )\n\n    def forward(self, x):\n        """Forward pass through both actor and critic"""\n        action_mean = self.actor(x)\n        value = self.critic(x)\n        return action_mean, value\n\n    def get_action(self, x):\n        """Sample action from policy"""\n        action_mean, value = self.forward(x)\n        # In PPO, we typically add noise during training but not during inference\n        return action_mean, value\n\nclass PPOTrainer:\n    """\n    PPO training loop for humanoid robot control\n    """\n\n    def __init__(self, agent: PPOAgent, lr: float = 3e-4, gamma: float = 0.99,\n                 eps_clip: float = 0.2, k_epochs: int = 4):\n        self.agent = agent\n        self.optimizer = optim.Adam(agent.parameters(), lr=lr)\n        self.gamma = gamma\n        self.eps_clip = eps_clip\n        self.k_epochs = k_epochs\n\n    def train_step(self, states, actions, rewards, logprobs, state_values, masks):\n        """Perform one training step of PPO"""\n        # Calculate discounted rewards\n        discounted_rewards = []\n        running_reward = 0\n\n        for reward, mask in zip(reversed(rewards), reversed(masks)):\n            running_reward = reward + self.gamma * running_reward * mask\n            discounted_rewards.insert(0, running_reward)\n\n        # Normalize discounted rewards\n        discounted_rewards = torch.tensor(discounted_rewards, dtype=torch.float32)\n        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-7)\n\n        # Convert to tensors\n        old_states = torch.squeeze(torch.stack(states, dim=0)).detach()\n        old_actions = torch.squeeze(torch.stack(actions, dim=0)).detach()\n        old_logprobs = torch.squeeze(torch.stack(logprobs, dim=0)).detach()\n        old_state_values = torch.squeeze(torch.stack(state_values, dim=0)).detach()\n\n        # Calculate advantages\n        advantages = discounted_rewards - old_state_values.detach()\n\n        # Optimize policy for K epochs\n        for _ in range(self.k_epochs):\n            # Evaluate old actions and values using current policy\n            logprobs, state_values = self.agent.get_action(old_states)\n\n            # Find the ratio (pi_theta / pi_theta_old)\n            ratios = torch.exp(logprobs - old_logprobs.detach())\n\n            # Calculate surrogate losses\n            surr1 = ratios * advantages\n            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n\n            # Calculate actor and critic losses\n            actor_loss = -torch.min(surr1, surr2).mean()\n            critic_loss = nn.MSELoss()(state_values, discounted_rewards.unsqueeze(1))\n\n            # Total loss\n            loss = actor_loss + 0.5 * critic_loss\n\n            # Perform backpropagation\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n\ndef main():\n    """Main training loop"""\n    # Initialize Isaac Sim world\n    world = World(stage_units_in_meters=1.0)\n\n    # Add robot to the stage\n    assets_root_path = get_assets_root_path()\n    if assets_root_path is None:\n        print("Could not find Isaac Sim assets. Please enable Isaac Sim Nucleus server.")\n        return\n\n    # Load robot (example with a generic humanoid)\n    add_reference_to_stage(\n        usd_path=f"{assets_root_path}/Isaac/Robots/Franka/franka_instanceable.usd",\n        prim_path="/World/Robot"\n    )\n\n    # Create RL environment\n    env = HumanoidRLEnvironment(world, "/World/Robot")\n\n    # Create PPO agent\n    observation_dim = env.observation_space.shape[0]\n    action_dim = env.action_space.shape[0]\n    agent = PPOAgent(observation_dim, action_dim)\n    trainer = PPOTrainer(agent)\n\n    # Training parameters\n    num_episodes = 1000\n    max_timesteps = 1000\n\n    # Training loop\n    for episode in range(num_episodes):\n        state = env.reset()\n        episode_reward = 0\n\n        for t in range(max_timesteps):\n            # Convert state to tensor\n            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n\n            # Get action from agent\n            with torch.no_grad():\n                action_mean, _ = agent.get_action(state_tensor)\n                action = action_mean.cpu().numpy()[0]\n\n            # Execute action in environment\n            next_state, reward, done, info = env.step(action)\n\n            # Update state\n            state = next_state\n            episode_reward += reward\n\n            if done:\n                break\n\n        # Print episode statistics\n        if episode % 10 == 0:\n            print(f"Episode {episode}, Reward: {episode_reward:.2f}")\n\n    print("Training completed!")\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,o.jsx)(e.h3,{id:"isaac-sim-rl-training-pipeline",children:"Isaac Sim RL Training Pipeline"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n# rl_training_pipeline.py\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\nimport os\nfrom datetime import datetime\nimport pickle\nfrom typing import List, Tuple, Dict, Any\n\nclass RLTrainingPipeline:\n    """\n    Complete RL training pipeline for humanoid robot control\n    """\n\n    def __init__(self, agent: nn.Module, env, log_dir: str = "./logs"):\n        self.agent = agent\n        self.env = env\n        self.log_dir = log_dir\n        self.writer = SummaryWriter(log_dir=os.path.join(log_dir, datetime.now().strftime("%Y%m%d-%H%M%S")))\n\n        # Training parameters\n        self.batch_size = 1024\n        self.update_timestep = 2000  # Update policy every n timesteps\n        self.max_episodes = 10000\n        self.max_timesteps = 1000\n        self.print_freq = 10\n        self.save_freq = 100\n\n        # Storage for training data\n        self.states = []\n        self.actions = []\n        self.logprobs = []\n        self.rewards = []\n        self.state_values = []\n        self.is_terminals = []\n\n        # Training statistics\n        self.running_reward = 0\n        self.avg_length = 0\n        self.time_step = 0\n\n    def train(self):\n        """Main training loop"""\n        print(f"Starting RL training with {self.max_episodes} episodes...")\n\n        for episode in range(1, self.max_episodes + 1):\n            # Reset environment\n            state = self.env.reset()\n            episode_reward = 0\n\n            for t in range(self.max_timesteps):\n                # Select action\n                action, logprob, state_value = self.select_action(state)\n\n                # Store data\n                self.states.append(torch.FloatTensor(state))\n                self.actions.append(torch.FloatTensor(action))\n                self.logprobs.append(logprob)\n                self.state_values.append(state_value)\n\n                # Execute action\n                state, reward, done, info = self.env.step(action)\n                self.rewards.append(reward)\n                self.is_terminals.append(done)\n\n                # Update statistics\n                episode_reward += reward\n                self.time_step += 1\n\n                if done:\n                    break\n\n            self.running_reward += episode_reward\n            self.avg_length += t\n\n            # Update policy if enough data collected\n            if self.time_step % self.update_timestep == 0:\n                self.update()\n                self.time_step = 0\n\n            # Print average reward every n episodes\n            if episode % self.print_freq == 0:\n                avg_reward = self.running_reward / self.print_freq\n                avg_length = int(self.avg_length / self.print_freq)\n\n                print(f\'Episode: {episode}, Average Reward: {avg_reward:.2f}, Average Length: {avg_length}\')\n\n                self.writer.add_scalar(\'Reward/Average\', avg_reward, episode)\n                self.writer.add_scalar(\'Episode/Length\', avg_length, episode)\n\n                self.running_reward = 0\n                self.avg_length = 0\n\n            # Save model every n episodes\n            if episode % self.save_freq == 0:\n                self.save_model(f"rl_model_episode_{episode}.pth")\n\n        # Close tensorboard writer\n        self.writer.close()\n        print("Training completed!")\n\n    def select_action(self, state: np.ndarray) -> Tuple[np.ndarray, torch.Tensor, torch.Tensor]:\n        """Select action using current policy"""\n        with torch.no_grad():\n            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n            action_mean, state_value = self.agent.get_action(state_tensor)\n\n            # For PPO, we add noise during training\n            action = torch.normal(action_mean, 0.1)  # Add exploration noise\n            action = torch.clamp(action, -1, 1)  # Ensure action is within bounds\n\n            # Calculate log probability\n            action_logprob = torch.log(1 - torch.tanh(action)**2 + 1e-6)  # Log prob for tanh output\n\n        return action.cpu().numpy()[0], action_logprob, state_value\n\n    def update(self):\n        """Update policy using collected data"""\n        # Convert lists to tensors\n        old_states = torch.stack(self.states).detach()\n        old_actions = torch.stack(self.actions).detach()\n        old_logprobs = torch.stack(self.logprobs).detach()\n        old_state_values = torch.stack(self.state_values).detach()\n\n        # Calculate discounted rewards\n        rewards = []\n        discounted_reward = 0\n\n        for reward, is_terminal in zip(reversed(self.rewards), reversed(self.is_terminals)):\n            if is_terminal:\n                discounted_reward = 0\n            discounted_reward = reward + (0.99 * discounted_reward)\n            rewards.insert(0, discounted_reward)\n\n        # Normalize rewards\n        rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n\n        # Calculate advantages\n        advantages = rewards - old_state_values.detach()\n\n        # Optimize policy\n        for _ in range(4):  # PPO epochs\n            # Evaluate old actions and values using current policy\n            logprobs, state_values = self.agent.get_action(old_states)\n\n            # Find the ratio (pi_theta / pi_theta_old)\n            ratios = torch.exp(logprobs - old_logprobs.detach())\n\n            # Calculate surrogate losses\n            surr1 = ratios * advantages\n            surr2 = torch.clamp(ratios, 0.8, 1.2) * advantages\n\n            # Calculate actor and critic losses\n            actor_loss = -torch.min(surr1, surr2).mean()\n            critic_loss = nn.MSELoss()(state_values, rewards)\n\n            # Total loss\n            loss = actor_loss + 0.5 * critic_loss - 0.01 * torch.mean(logprobs)\n\n            # Perform backpropagation\n            self.agent.optimizer.zero_grad()\n            loss.backward()\n            self.agent.optimizer.step()\n\n        # Clear stored data\n        del self.states[:]\n        del self.actions[:]\n        del self.logprobs[:]\n        del self.rewards[:]\n        del self.state_values[:]\n        del self.is_terminals[:]\n\n    def save_model(self, path: str):\n        """Save trained model"""\n        torch.save(self.agent.state_dict(), path)\n        print(f"Model saved to {path}")\n\n    def load_model(self, path: str):\n        """Load trained model"""\n        self.agent.load_state_dict(torch.load(path))\n        print(f"Model loaded from {path}")\n\nclass CurriculumLearning:\n    """\n    Curriculum learning for progressive skill acquisition\n    """\n\n    def __init__(self, env, difficulty_levels: List[Dict[str, Any]]):\n        self.env = env\n        self.difficulty_levels = difficulty_levels\n        self.current_level = 0\n        self.success_threshold = 0.8  # 80% success rate to advance\n        self.episode_count = 0\n        self.success_count = 0\n\n    def update_curriculum(self, episode_reward: float, target_reward: float) -> bool:\n        """Update curriculum based on performance"""\n        self.episode_count += 1\n        if episode_reward >= target_reward * self.success_threshold:\n            self.success_count += 1\n\n        # Check if we should advance to next level\n        if (self.episode_count >= 100 and\n            self.success_count / self.episode_count >= self.success_threshold):\n\n            if self.current_level < len(self.difficulty_levels) - 1:\n                self.current_level += 1\n                self.episode_count = 0\n                self.success_count = 0\n                print(f"Advancing to difficulty level {self.current_level + 1}")\n\n                # Update environment parameters for new level\n                self.update_environment()\n                return True\n\n        return False\n\n    def update_environment(self):\n        """Update environment parameters for current difficulty level"""\n        level_params = self.difficulty_levels[self.current_level]\n\n        # Update environment based on current level\n        # This would typically involve changing task complexity, obstacles, etc.\n        print(f"Updated environment for difficulty level {self.current_level + 1}")\n\ndef create_humanoid_rl_environment():\n    """Factory function to create a complete humanoid RL environment"""\n    # This would integrate with Isaac Sim to create a complete environment\n    # For this example, we\'ll return a placeholder that would connect to Isaac Sim\n    pass\n\ndef main():\n    """Main function to run the RL training pipeline"""\n    # Initialize Isaac Sim environment (simplified for example)\n    # In practice, this would connect to the actual Isaac Sim simulation\n\n    print("Setting up RL training environment...")\n\n    # Create agent (in practice, this would connect to the Isaac Sim environment)\n    # observation_dim = 41  # Based on humanoid state space\n    # action_dim = 14       # Based on humanoid DOF\n    # agent = PPOAgent(observation_dim, action_dim)\n\n    # Create training pipeline\n    # pipeline = RLTrainingPipeline(agent, env)\n\n    # Run training\n    # pipeline.train()\n\n    print("RL training pipeline setup complete!")\n    print("Note: Full implementation requires Isaac Sim integration for actual simulation")\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,o.jsx)(e.h2,{id:"examples",children:"Examples"}),"\n",(0,o.jsx)(e.h3,{id:"example-1-humanoid-locomotion-training-with-ppo",children:"Example 1: Humanoid Locomotion Training with PPO"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n# humanoid_locomotion_ppo.py\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom typing import Tuple\n\nclass HumanoidLocomotionEnv:\n    """\n    Specialized environment for humanoid locomotion training\n    """\n\n    def __init__(self):\n        # Define locomotion-specific parameters\n        self.target_velocity = 1.0  # m/s\n        self.max_episode_length = 500\n        self.current_step = 0\n\n        # State space: [joint_pos, joint_vel, root_pos, root_vel, target_pos]\n        self.state_dim = 14 + 14 + 3 + 3 + 3  # Example dimensions\n\n        # Action space: joint position targets\n        self.action_dim = 14  # Example joint DOF\n\n        # Reward weights\n        self.velocity_weight = 1.0\n        self.energy_weight = -0.01\n        self.balance_weight = 0.5\n        self.forward_weight = 0.8\n\n    def reset(self) -> np.ndarray:\n        """Reset environment for locomotion task"""\n        self.current_step = 0\n\n        # Initialize robot in standing position\n        state = self.get_state()\n        return state\n\n    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, dict]:\n        """Execute one step of locomotion task"""\n        # Apply action to robot\n        self.apply_action(action)\n\n        # Get new state\n        next_state = self.get_state()\n\n        # Calculate locomotion-specific reward\n        reward = self.calculate_locomotion_reward()\n\n        # Check termination\n        done = self.current_step >= self.max_episode_length\n        self.current_step += 1\n\n        info = {\n            \'velocity\': self.get_forward_velocity(),\n            \'balance\': self.get_balance_score()\n        }\n\n        return next_state, reward, done, info\n\n    def calculate_locomotion_reward(self) -> float:\n        """Calculate reward for locomotion task"""\n        # Get current state\n        state = self.get_state()\n\n        # Forward velocity reward\n        forward_vel = self.get_forward_velocity()\n        velocity_reward = self.velocity_weight * abs(forward_vel - self.target_velocity)\n\n        # Energy efficiency penalty\n        joint_velocities = state[14:28]  # Assuming joint velocities are in this range\n        energy_penalty = self.energy_weight * np.sum(np.square(joint_velocities))\n\n        # Balance reward (keep torso upright)\n        balance_reward = self.balance_weight * self.get_balance_score()\n\n        # Forward progress reward\n        forward_progress = self.forward_weight * forward_vel\n\n        total_reward = velocity_reward + energy_penalty + balance_reward + forward_progress\n        return total_reward\n\n    def get_forward_velocity(self) -> float:\n        """Get forward velocity of humanoid"""\n        # This would interface with Isaac Sim to get actual velocity\n        return 0.0  # Placeholder\n\n    def get_balance_score(self) -> float:\n        """Get balance score (0-1, 1 being perfectly balanced)"""\n        # This would interface with Isaac Sim to get actual balance\n        return 0.0  # Placeholder\n\n    def get_state(self) -> np.ndarray:\n        """Get current state from Isaac Sim"""\n        # This would interface with Isaac Sim to get actual state\n        return np.zeros(self.state_dim)  # Placeholder\n\n    def apply_action(self, action: np.ndarray):\n        """Apply action to Isaac Sim robot"""\n        # This would interface with Isaac Sim to apply actions\n        pass\n\nclass LocomotionPPOAgent(nn.Module):\n    """\n    Specialized PPO agent for humanoid locomotion\n    """\n\n    def __init__(self, state_dim: int, action_dim: int):\n        super(LocomotionPPOAgent, self).__init__()\n\n        # Shared feature extractor\n        self.feature_extractor = nn.Sequential(\n            nn.Linear(state_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 256),\n            nn.ReLU()\n        )\n\n        # Actor network\n        self.actor_mean = nn.Linear(256, action_dim)\n        self.actor_std = nn.Parameter(torch.ones(action_dim) * 0.1)\n\n        # Critic network\n        self.critic = nn.Linear(256, 1)\n\n    def forward(self, state):\n        """Forward pass through networks"""\n        features = self.feature_extractor(state)\n\n        # Actor output\n        action_mean = torch.tanh(self.actor_mean(features))  # Keep actions bounded\n\n        # Critic output\n        value = self.critic(features)\n\n        return action_mean, self.actor_std, value\n\ndef train_locomotion():\n    """Train locomotion policy"""\n    env = HumanoidLocomotionEnv()\n    agent = LocomotionPPOAgent(env.state_dim, env.action_dim)\n    optimizer = optim.Adam(agent.parameters(), lr=3e-4)\n\n    # Training loop would go here\n    # This is a simplified example of the training process\n\n    print("Locomotion training setup complete")\n\nif __name__ == "__main__":\n    train_locomotion()\n'})}),"\n",(0,o.jsx)(e.h3,{id:"example-2-rl-policy-validation-and-transfer",children:"Example 2: RL Policy Validation and Transfer"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n# rl_policy_validation.py\n\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom typing import List, Dict, Tuple\nimport json\n\nclass PolicyValidator:\n    """\n    Validate trained RL policies for safety and performance\n    """\n\n    def __init__(self, agent, env):\n        self.agent = agent\n        self.env = env\n        self.metrics = {\n            \'success_rate\': [],\n            \'episode_length\': [],\n            \'cumulative_reward\': [],\n            \'safety_violations\': [],\n            \'energy_efficiency\': []\n        }\n\n    def validate_policy(self, num_episodes: int = 100) -> Dict[str, float]:\n        """Validate policy across multiple episodes"""\n        total_success = 0\n        total_reward = 0\n        total_safety_violations = 0\n        total_energy = 0\n\n        for episode in range(num_episodes):\n            state = self.env.reset()\n            episode_reward = 0\n            safety_violations = 0\n            energy_consumed = 0\n            done = False\n\n            while not done:\n                # Get action from trained policy\n                with torch.no_grad():\n                    action_mean, _, _ = self.agent.forward(torch.FloatTensor(state).unsqueeze(0))\n                    action = action_mean.squeeze(0).numpy()\n\n                # Execute action\n                next_state, reward, done, info = self.env.step(action)\n\n                # Track metrics\n                episode_reward += reward\n                energy_consumed += self.calculate_energy(action)\n\n                # Check for safety violations\n                if self.check_safety_violation(next_state):\n                    safety_violations += 1\n\n                state = next_state\n\n            # Update statistics\n            if self.check_task_success(info):\n                total_success += 1\n\n            total_reward += episode_reward\n            total_safety_violations += safety_violations\n            total_energy += energy_consumed\n\n        # Calculate final metrics\n        results = {\n            \'success_rate\': total_success / num_episodes,\n            \'average_reward\': total_reward / num_episodes,\n            \'average_safety_violations\': total_safety_violations / num_episodes,\n            \'average_energy_efficiency\': total_energy / num_episodes\n        }\n\n        return results\n\n    def calculate_energy(self, action: np.ndarray) -> float:\n        """Calculate energy consumption based on action"""\n        return np.sum(np.abs(action))  # Simplified energy calculation\n\n    def check_safety_violation(self, state: np.ndarray) -> bool:\n        """Check if state violates safety constraints"""\n        # Example: check if joint angles are within safe limits\n        joint_positions = state[:14]  # Assuming first 14 elements are joint positions\n        joint_limits = np.array([1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5])  # Example limits\n\n        return np.any(np.abs(joint_positions) > joint_limits)\n\n    def check_task_success(self, info: Dict) -> bool:\n        """Check if task was completed successfully"""\n        # This would depend on the specific task\n        # For example, reaching a target position\n        return info.get(\'distance_to_target\', float(\'inf\')) < 0.1  # Within 10cm of target\n\n    def visualize_validation_results(self, results: Dict[str, float]):\n        """Visualize validation results"""\n        metrics = list(results.keys())\n        values = list(results.values())\n\n        plt.figure(figsize=(10, 6))\n        bars = plt.bar(metrics, values)\n        plt.title(\'RL Policy Validation Results\')\n        plt.ylabel(\'Value\')\n        plt.xticks(rotation=45)\n\n        # Add value labels on bars\n        for bar, value in zip(bars, values):\n            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n                    f\'{value:.3f}\', ha=\'center\', va=\'bottom\')\n\n        plt.tight_layout()\n        plt.show()\n\nclass SimToRealTransfer:\n    """\n    Handle transfer of policies from simulation to real robots\n    """\n\n    def __init__(self, sim_agent, real_robot_interface):\n        self.sim_agent = sim_agent\n        self.real_robot = real_robot_interface\n        self.domain_randomization_params = {}\n        self.system_identification_params = {}\n\n    def domain_randomization(self):\n        """Apply domain randomization to improve sim-to-real transfer"""\n        # Randomize simulation parameters\n        self.domain_randomization_params = {\n            \'mass_variance\': np.random.uniform(0.8, 1.2),      # \xb120% mass variation\n            \'friction_variance\': np.random.uniform(0.5, 1.5),  # 0.5x to 1.5x friction\n            \'actuator_noise\': np.random.uniform(0.01, 0.05),   # 1-5% actuator noise\n            \'sensor_noise\': np.random.uniform(0.001, 0.01),    # sensor noise levels\n            \'dynamics_randomization\': np.random.uniform(0.9, 1.1)  # dynamics scaling\n        }\n\n    def system_identification(self) -> Dict[str, float]:\n        """Identify system parameters for real robot"""\n        # This would involve exciting the real robot and identifying parameters\n        # For this example, we\'ll return placeholder values\n        return {\n            \'mass\': 50.0,  # kg\n            \'inertia\': [1.0, 1.0, 1.0],  # kg*m^2\n            \'com_offset\': [0.0, 0.0, 0.0],  # m\n            \'actuator_gains\': [1.0, 1.0, 1.0]  # unitless\n        }\n\n    def adapt_policy(self, real_params: Dict[str, float]) -> torch.nn.Module:\n        """Adapt simulation policy for real robot"""\n        # Create adapted policy that accounts for real robot differences\n        adapted_agent = self.sim_agent  # In practice, this would involve parameter adjustment\n\n        # Update agent with real robot parameters\n        # This might involve fine-tuning or parameter adjustment\n\n        return adapted_agent\n\n    def validate_transfer(self, adapted_agent) -> Dict[str, float]:\n        """Validate policy transfer on real robot"""\n        # This would run the adapted policy on the real robot\n        # and measure performance\n        return {\n            \'success_rate\': 0.7,  # Example success rate\n            \'safety_violations\': 0.05,  # 5% safety violation rate\n            \'performance_drop\': 0.15  # 15% performance drop from sim\n        }\n\ndef main():\n    """Main function for RL validation and transfer"""\n    print("Starting RL policy validation and transfer...")\n\n    # Example validation process\n    # In practice, this would connect to actual trained agents and environments\n    print("Policy validation and transfer framework initialized")\n    print("Note: Full implementation requires trained agents and robot interfaces")\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,o.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(e.p,{children:"Reinforcement learning provides a powerful framework for training humanoid robots to perform complex tasks through trial and error. Key components include:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Environment Design"}),": Creating appropriate state, action, and reward spaces for the specific task"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Algorithm Selection"}),": Choosing suitable RL algorithms (PPO, SAC, TD3) based on task requirements"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Training Pipeline"}),": Implementing efficient training loops with proper data collection and updates"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Validation"}),": Ensuring trained policies are safe and performant before deployment"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Sim-to-Real Transfer"}),": Techniques to bridge the reality gap between simulation and real robots"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"exercises",children:"Exercises"}),"\n",(0,o.jsx)(e.h3,{id:"conceptual",children:"Conceptual"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Compare and contrast different RL algorithms (PPO, SAC, TD3) for humanoid robot control. What are the trade-offs between sample efficiency, stability, and performance?"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"logical",children:"Logical"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Analyze the reward function design for a humanoid walking task. How would you balance competing objectives like forward velocity, energy efficiency, and balance stability?"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"implementation-1",children:"Implementation"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Implement a complete PPO training pipeline for a simple humanoid reaching task in Isaac Sim, including proper reward shaping, curriculum learning, and policy validation."}),"\n"]})]})}function p(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},8453(n,e,t){t.d(e,{R:()=>r,x:()=>s});var i=t(6540);const o={},a=i.createContext(o);function r(n){const e=i.useContext(a);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:r(n.components),i.createElement(a.Provider,{value:e},n.children)}}}]);