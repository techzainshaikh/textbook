"use strict";(globalThis.webpackChunkphysical_ai_book_humanoid=globalThis.webpackChunkphysical_ai_book_humanoid||[]).push([[5566],{8453(e,n,t){t.d(n,{R:()=>a,x:()=>r});var s=t(6540);const i={},o=s.createContext(i);function a(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),s.createElement(o.Provider,{value:n},e.children)}},9632(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-4-vla/chapter-5-multimodal-perception","title":"Multimodal Perception for Vision-Language-Action Systems","description":"Implementing multimodal perception systems that integrate vision, language, and sensor data for humanoid robotics applications","source":"@site/docs/module-4-vla/chapter-5-multimodal-perception.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/chapter-5-multimodal-perception","permalink":"/textbook/docs/module-4-vla/chapter-5-multimodal-perception","draft":false,"unlisted":false,"editUrl":"https://github.com/Panaversity/physical_ai_book_humanoid/tree/main/docs/module-4-vla/chapter-5-multimodal-perception.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"title":"Multimodal Perception for Vision-Language-Action Systems","sidebar_position":6,"description":"Implementing multimodal perception systems that integrate vision, language, and sensor data for humanoid robotics applications","keywords":["multimodal perception","computer vision","sensor fusion","humanoid robotics","vision-language models","multimodal AI"]},"sidebar":"tutorialSidebar","previous":{"title":"ROS 2 Actions for VLA Systems","permalink":"/textbook/docs/module-4-vla/chapter-4-ros2-actions"},"next":{"title":"Module 4 Summary - Vision-Language-Action Systems","permalink":"/textbook/docs/module-4-vla/module-summary"}}');var i=t(4848),o=t(8453);const a={title:"Multimodal Perception for Vision-Language-Action Systems",sidebar_position:6,description:"Implementing multimodal perception systems that integrate vision, language, and sensor data for humanoid robotics applications",keywords:["multimodal perception","computer vision","sensor fusion","humanoid robotics","vision-language models","multimodal AI"]},r="Chapter 5: Multimodal Perception for Vision-Language-Action Systems",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Core Concepts",id:"core-concepts",level:2},{value:"Multimodal Architecture",id:"multimodal-architecture",level:3},{value:"Vision-Language Models",id:"vision-language-models",level:3},{value:"Implementation",id:"implementation",level:2},{value:"Vision-Language Integration Framework",id:"vision-language-integration-framework",level:3},{value:"Sensor Fusion for Multimodal Perception",id:"sensor-fusion-for-multimodal-perception",level:3},{value:"Examples",id:"examples",level:2},{value:"Example 1: Object Recognition and Manipulation Planning",id:"example-1-object-recognition-and-manipulation-planning",level:3},{value:"Example 2: Scene Understanding and Navigation Planning",id:"example-2-scene-understanding-and-navigation-planning",level:3},{value:"Summary",id:"summary",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Conceptual",id:"conceptual",level:3},{value:"Logical",id:"logical",level:3},{value:"Implementation",id:"implementation-1",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"chapter-5-multimodal-perception-for-vision-language-action-systems",children:"Chapter 5: Multimodal Perception for Vision-Language-Action Systems"})}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(n.p,{children:"By the end of this chapter, students will be able to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Implement multimodal perception systems that integrate vision, language, and sensor data"}),"\n",(0,i.jsx)(n.li,{children:"Design sensor fusion architectures for comprehensive environmental understanding"}),"\n",(0,i.jsx)(n.li,{children:"Integrate vision-language models for object recognition and scene understanding"}),"\n",(0,i.jsx)(n.li,{children:"Create multimodal embeddings that combine visual and textual information"}),"\n",(0,i.jsx)(n.li,{children:"Validate and test multimodal perception systems for humanoid robotics applications"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsx)(n.p,{children:"Students should have:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Understanding of computer vision fundamentals (covered in Module 2)"}),"\n",(0,i.jsx)(n.li,{children:"Knowledge of sensor fusion concepts (covered in Module 1)"}),"\n",(0,i.jsx)(n.li,{children:"Familiarity with neural networks and deep learning"}),"\n",(0,i.jsx)(n.li,{children:"Experience with Python programming and PyTorch/TensorFlow"}),"\n",(0,i.jsx)(n.li,{children:"Basic understanding of natural language processing concepts"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"core-concepts",children:"Core Concepts"}),"\n",(0,i.jsx)(n.p,{children:"Multimodal perception systems combine multiple sensory inputs to create a comprehensive understanding of the environment. These systems are crucial for humanoid robots to navigate and interact effectively with complex, dynamic environments."}),"\n",(0,i.jsx)(n.h3,{id:"multimodal-architecture",children:"Multimodal Architecture"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Vision Processing:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Object Detection"}),": Identify and locate objects in visual scenes"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Pose Estimation"}),": Determine object poses and spatial relationships"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Scene Understanding"}),": Comprehend spatial layouts and semantic relationships"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Activity Recognition"}),": Recognize human activities and behaviors"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Language Integration:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Visual Grounding"}),": Link language descriptions to visual elements"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Caption Generation"}),": Describe scenes in natural language"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Query Understanding"}),": Interpret language queries about visual content"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Instruction Following"}),": Execute language-based commands in visual contexts"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Sensor Fusion:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Multi-modal Integration"}),": Combine visual, auditory, tactile, and proprioceptive data"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Temporal Consistency"}),": Maintain coherent understanding across time"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Uncertainty Management"}),": Handle noisy and incomplete sensor data"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Real-time Processing"}),": Achieve timely responses for interactive applications"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"vision-language-models",children:"Vision-Language Models"}),"\n",(0,i.jsx)(n.p,{children:"Modern vision-language models enable sophisticated understanding by combining visual and textual modalities:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"CLIP"}),": Contrastive Language-Image Pretraining for zero-shot recognition"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"BLIP"}),": Bootstrapping Language-Image Pretraining for unified vision-language understanding"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ViLT"}),": Vision-and-Language Transformer for efficient multimodal processing"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Florence"}),": Foundation model for comprehensive vision-language tasks"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"implementation",children:"Implementation"}),"\n",(0,i.jsx)(n.p,{children:"Let's implement multimodal perception systems for humanoid robotics:"}),"\n",(0,i.jsx)(n.h3,{id:"vision-language-integration-framework",children:"Vision-Language Integration Framework"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# vision_language_integration.py\n\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom transformers import CLIPProcessor, CLIPModel, BlipProcessor, BlipForConditionalGeneration\nimport numpy as np\nimport cv2\nfrom typing import Dict, Any, List, Optional, Tuple\nfrom dataclasses import dataclass\nimport logging\n\n@dataclass\nclass PerceptionOutput:\n    """Output from the multimodal perception system"""\n    objects: List[Dict[str, Any]]  # Detected objects with properties\n    scene_description: str         # Natural language description of scene\n    spatial_relationships: List[Tuple[str, str, str]]  # Object relationships\n    embeddings: Dict[str, torch.Tensor]  # Multimodal embeddings\n    confidence_scores: Dict[str, float]  # Confidence scores for detections\n    timestamp: float               # Processing timestamp\n\nclass VisionLanguageModel(nn.Module):\n    """\n    Vision-language model for multimodal perception\n    """\n\n    def __init__(self, model_type: str = "clip"):\n        super().__init__()\n        self.model_type = model_type\n\n        if model_type == "clip":\n            # Load pre-trained CLIP model\n            self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")\n            self.processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")\n        elif model_type == "blip":\n            # Load BLIP model for image captioning\n            self.blip_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")\n            self.blip_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")\n        else:\n            # Simple CNN-based approach\n            self.visual_encoder = self._create_visual_encoder()\n            self.text_encoder = self._create_text_encoder()\n\n    def _create_visual_encoder(self):\n        """Create a simple CNN-based visual encoder"""\n        return nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d((1, 1)),\n            nn.Flatten(),\n            nn.Linear(256, 512)\n        )\n\n    def _create_text_encoder(self):\n        """Create a simple text encoder"""\n        vocab_size = 10000\n        embed_dim = 512\n        return nn.Embedding(vocab_size, embed_dim)\n\n    def encode_image(self, images: torch.Tensor) -> torch.Tensor:\n        """Encode images into visual features"""\n        if self.model_type == "clip":\n            return self.clip_model.get_image_features(pixel_values=images)\n        elif self.model_type == "blip":\n            # For BLIP, we need to handle differently\n            return self.blip_model.get_text_features(input_ids=images)  # This is a simplification\n        else:\n            return self.visual_encoder(images)\n\n    def encode_text(self, texts: List[str]) -> torch.Tensor:\n        """Encode text into language features"""\n        if self.model_type == "clip":\n            inputs = self.processor(text=texts, return_tensors="pt", padding=True, truncation=True)\n            return self.clip_model.get_text_features(input_ids=inputs.input_ids)\n        else:\n            # Simple approach: convert text to tokens and embed\n            # This is a simplified version - in practice, you\'d use proper tokenization\n            tokenized_texts = [[ord(c) % 10000 for c in text[:100]] for text in texts]  # Simplified tokenization\n            max_len = max(len(tokens) for tokens in tokenized_texts)\n            padded_texts = [tokens + [0] * (max_len - len(tokens)) for tokens in tokenized_texts]\n            text_tensor = torch.tensor(padded_texts)\n            embedded = self.text_encoder(text_tensor)\n            return embedded.mean(dim=1)  # Average pooling\n\n    def forward(self, images: torch.Tensor, texts: List[str]) -> Dict[str, torch.Tensor]:\n        """Forward pass through the vision-language model"""\n        image_features = self.encode_image(images)\n        text_features = self.encode_text(texts)\n\n        # Normalize features\n        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n\n        return {\n            "image_features": image_features,\n            "text_features": text_features,\n            "similarity": torch.matmul(image_features, text_features.t())\n        }\n\nclass ObjectDetector(nn.Module):\n    """\n    Object detection module for multimodal perception\n    """\n\n    def __init__(self, model_type: str = "yolo"):\n        super().__init__()\n        self.model_type = model_type\n\n        if model_type == "yolo":\n            # In practice, you\'d load a YOLO model\n            # For this example, we\'ll create a simple detector\n            self.detector = self._create_simple_detector()\n        else:\n            # Use torchvision models\n            from torchvision.models.detection import fasterrcnn_resnet50_fpn\n            self.detector = fasterrcnn_resnet50_fpn(pretrained=True)\n            self.detector.eval()\n\n    def _create_simple_detector(self):\n        """Create a simple object detector for demonstration"""\n        return nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d((1, 1)),\n            nn.Flatten(),\n            nn.Linear(128, 256)\n        )\n\n    def forward(self, images: torch.Tensor) -> List[Dict[str, Any]]:\n        """Detect objects in images"""\n        if self.model_type == "yolo":\n            # For demonstration, return mock detections\n            batch_size = images.size(0)\n            detections = []\n\n            for i in range(batch_size):\n                # Create mock detections\n                mock_detections = [\n                    {\n                        "label": "person",\n                        "confidence": 0.95,\n                        "bbox": [0.1, 0.1, 0.3, 0.4],  # x, y, width, height (normalized)\n                        "embedding": torch.randn(512)\n                    },\n                    {\n                        "label": "chair",\n                        "confidence": 0.89,\n                        "bbox": [0.4, 0.3, 0.2, 0.3],\n                        "embedding": torch.randn(512)\n                    }\n                ]\n                detections.append(mock_detections)\n\n            return detections\n        else:\n            # For FasterRCNN, use the actual model\n            # This is a simplified version - in practice, you\'d handle the full pipeline\n            return []\n\n    def detect_objects(self, image: torch.Tensor) -> List[Dict[str, Any]]:\n        """Detect objects in a single image"""\n        detections = self(image.unsqueeze(0))\n        return detections[0] if detections else []\n\nclass SpatialReasoner:\n    """\n    Spatial reasoning module for understanding object relationships\n    """\n\n    def __init__(self):\n        self.spatial_relations = [\n            "left of", "right of", "above", "below", "next to",\n            "behind", "in front of", "inside", "outside", "on top of"\n        ]\n\n    def compute_spatial_relationships(self, objects: List[Dict[str, Any]],\n                                    image_shape: Tuple[int, int]) -> List[Tuple[str, str, str]]:\n        """Compute spatial relationships between objects"""\n        relationships = []\n\n        for i, obj1 in enumerate(objects):\n            for j, obj2 in enumerate(objects):\n                if i != j:\n                    rel = self._compute_relationship(obj1, obj2, image_shape)\n                    if rel:\n                        relationships.append(rel)\n\n        return relationships\n\n    def _compute_relationship(self, obj1: Dict[str, Any], obj2: Dict[str, Any],\n                            image_shape: Tuple[int, int]) -> Optional[Tuple[str, str, str]]:\n        """Compute the spatial relationship between two objects"""\n        bbox1 = obj1["bbox"]  # [x, y, width, height] - normalized\n        bbox2 = obj2["bbox"]\n\n        # Convert normalized coordinates to pixel coordinates\n        h, w = image_shape\n        x1, y1 = bbox1[0] * w, bbox1[1] * h\n        w1, h1 = bbox1[2] * w, bbox1[3] * h\n        cx1, cy1 = x1 + w1/2, y1 + h1/2  # Center of object 1\n\n        x2, y2 = bbox2[0] * w, bbox2[1] * h\n        w2, h2 = bbox2[2] * w, bbox2[3] * h\n        cx2, cy2 = x2 + w2/2, y2 + h2/2  # Center of object 2\n\n        # Compute spatial relationship\n        dx = cx2 - cx1\n        dy = cy2 - cy1\n\n        # Determine primary direction\n        if abs(dx) > abs(dy):\n            # Horizontal relationship\n            if dx > 0:\n                return (obj1["label"], "right of", obj2["label"])\n            else:\n                return (obj1["label"], "left of", obj2["label"])\n        else:\n            # Vertical relationship\n            if dy > 0:\n                return (obj1["label"], "below", obj2["label"])\n            else:\n                return (obj1["label"], "above", obj2["label"])\n\nclass MultimodalPerceptionSystem:\n    """\n    Complete multimodal perception system for humanoid robots\n    """\n\n    def __init__(self, vl_model_type: str = "clip", detector_type: str = "yolo"):\n        self.vision_language_model = VisionLanguageModel(vl_model_type)\n        self.object_detector = ObjectDetector(detector_type)\n        self.spatial_reasoner = SpatialReasoner()\n\n        # Preprocessing transforms\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Resize((224, 224)),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n        self.logger = logging.getLogger(__name__)\n\n    def process_image(self, image: np.ndarray, text_queries: List[str] = None) -> PerceptionOutput:\n        """Process an image with optional text queries"""\n        # Preprocess image\n        image_tensor = self.transform(image).unsqueeze(0)\n\n        # Detect objects\n        objects = self.object_detector.detect_objects(image_tensor)\n\n        # Generate scene description if no text queries provided\n        if not text_queries:\n            scene_description = self._generate_scene_description(objects)\n        else:\n            # Use text queries to guide perception\n            scene_description = self._answer_queries(image_tensor, text_queries)\n\n        # Compute spatial relationships\n        image_h, image_w = image.shape[:2]\n        spatial_relationships = self.spatial_reasoner.compute_spatial_relationships(\n            objects, (image_h, image_w)\n        )\n\n        # Generate multimodal embeddings\n        embeddings = self._generate_embeddings(image_tensor, objects, text_queries or [scene_description])\n\n        # Compute confidence scores\n        confidence_scores = self._compute_confidence_scores(objects)\n\n        return PerceptionOutput(\n            objects=objects,\n            scene_description=scene_description,\n            spatial_relationships=spatial_relationships,\n            embeddings=embeddings,\n            confidence_scores=confidence_scores,\n            timestamp=time.time()\n        )\n\n    def _generate_scene_description(self, objects: List[Dict[str, Any]]) -> str:\n        """Generate a natural language description of the scene"""\n        if not objects:\n            return "The scene appears to be empty."\n\n        # Count objects by category\n        obj_counts = {}\n        for obj in objects:\n            label = obj["label"]\n            obj_counts[label] = obj_counts.get(label, 0) + 1\n\n        # Create description\n        parts = []\n        for label, count in obj_counts.items():\n            if count == 1:\n                parts.append(f"a {label}")\n            else:\n                parts.append(f"{count} {label}s")\n\n        if len(parts) == 1:\n            return f"The scene contains {parts[0]}."\n        elif len(parts) == 2:\n            return f"The scene contains {parts[0]} and {parts[1]}."\n        else:\n            return f"The scene contains {\', \'.join(parts[:-1])}, and {parts[-1]}."\n\n    def _answer_queries(self, image_tensor: torch.Tensor, queries: List[str]) -> str:\n        """Answer text queries about the image"""\n        # For simplicity, return a mock response\n        # In practice, you\'d use a more sophisticated approach\n        if len(queries) == 1:\n            return f"Based on the image, I can see objects that might relate to: {queries[0]}"\n        else:\n            return f"I can see objects that might relate to: {\', \'.join(queries)}"\n\n    def _generate_embeddings(self, image_tensor: torch.Tensor,\n                           objects: List[Dict[str, Any]],\n                           texts: List[str]) -> Dict[str, torch.Tensor]:\n        """Generate multimodal embeddings"""\n        embeddings = {}\n\n        # Image embedding\n        with torch.no_grad():\n            image_features = self.vision_language_model.encode_image(image_tensor)\n            embeddings["image"] = image_features\n\n        # Object embeddings\n        for i, obj in enumerate(objects):\n            embeddings[f"object_{i}"] = obj["embedding"]\n\n        # Text embeddings\n        if texts:\n            text_features = self.vision_language_model.encode_text(texts)\n            embeddings["text"] = text_features\n\n        return embeddings\n\n    def _compute_confidence_scores(self, objects: List[Dict[str, Any]]) -> Dict[str, float]:\n        """Compute confidence scores for detections"""\n        scores = {}\n        for i, obj in enumerate(objects):\n            scores[f"object_{i}_{obj[\'label\']}"] = obj["confidence"]\n        return scores\n\n    def find_objects_by_description(self, image: np.ndarray, description: str) -> List[Dict[str, Any]]:\n        """Find objects in image that match a text description"""\n        # Process the image\n        perception_output = self.process_image(image, [description])\n\n        # Use the vision-language model to match description to objects\n        image_tensor = self.transform(image).unsqueeze(0)\n        text_features = self.vision_language_model.encode_text([description])\n\n        matched_objects = []\n        for obj in perception_output.objects:\n            # Compute similarity between object embedding and text description\n            obj_embedding = obj["embedding"].unsqueeze(0)\n            similarity = torch.cosine_similarity(text_features, obj_embedding, dim=1)\n\n            if similarity.item() > 0.3:  # Threshold for matching\n                obj_copy = obj.copy()\n                obj_copy["match_score"] = similarity.item()\n                matched_objects.append(obj_copy)\n\n        return matched_objects\n\ndef create_multimodal_perception_system(model_type: str = "clip") -> MultimodalPerceptionSystem:\n    """Factory function to create a multimodal perception system"""\n    return MultimodalPerceptionSystem(model_type)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"sensor-fusion-for-multimodal-perception",children:"Sensor Fusion for Multimodal Perception"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# sensor_fusion.py\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom typing import Dict, Any, List, Optional, Tuple\nfrom dataclasses import dataclass\nimport time\n\n@dataclass\nclass SensorReading:\n    """A single sensor reading"""\n    sensor_type: str\n    data: torch.Tensor\n    timestamp: float\n    confidence: float\n\n@dataclass\nclass FusedPerceptionOutput:\n    """Output from the sensor fusion system"""\n    fused_features: torch.Tensor\n    certainty_scores: Dict[str, float]\n    temporal_consistency: float\n    sensor_contributions: Dict[str, float]\n    anomalies: List[Dict[str, Any]]\n\nclass KalmanFilter:\n    """\n    Kalman filter for sensor fusion and state estimation\n    """\n\n    def __init__(self, state_dim: int, measurement_dim: int):\n        self.state_dim = state_dim\n        self.measurement_dim = measurement_dim\n\n        # Initialize state and covariance matrices\n        self.state = np.zeros(state_dim)\n        self.covariance = np.eye(state_dim) * 1000  # High initial uncertainty\n\n        # Process and measurement noise\n        self.Q = np.eye(state_dim) * 0.1  # Process noise\n        self.R = np.eye(measurement_dim) * 1.0  # Measurement noise\n\n        # Identity matrix\n        self.I = np.eye(state_dim)\n\n    def predict(self, F: np.ndarray, B: np.ndarray = None, u: np.ndarray = None):\n        """Predict next state"""\n        # State transition\n        if B is not None and u is not None:\n            self.state = F @ self.state + B @ u\n        else:\n            self.state = F @ self.state\n\n        # Covariance prediction\n        self.covariance = F @ self.covariance @ F.T + self.Q\n\n    def update(self, measurement: np.ndarray, H: np.ndarray):\n        """Update state with measurement"""\n        # Innovation\n        innovation = measurement - H @ self.state\n        innovation_covariance = H @ self.covariance @ H.T + self.R\n\n        # Kalman gain\n        kalman_gain = self.covariance @ H.T @ np.linalg.inv(innovation_covariance)\n\n        # Update state and covariance\n        self.state = self.state + kalman_gain @ innovation\n        self.covariance = (self.I - kalman_gain @ H) @ self.covariance\n\nclass ParticleFilter:\n    """\n    Particle filter for non-linear, non-Gaussian state estimation\n    """\n\n    def __init__(self, state_dim: int, num_particles: int = 1000):\n        self.state_dim = state_dim\n        self.num_particles = num_particles\n\n        # Initialize particles randomly\n        self.particles = np.random.randn(num_particles, state_dim) * 10\n        self.weights = np.ones(num_particles) / num_particles\n\n    def predict(self, process_noise: float = 0.1):\n        """Predict particle states"""\n        noise = np.random.randn(self.num_particles, self.state_dim) * process_noise\n        self.particles += noise\n\n    def update(self, measurement: np.ndarray, measurement_function, measurement_noise: float = 1.0):\n        """Update particle weights based on measurement"""\n        # Compute likelihood of each particle given measurement\n        for i in range(self.num_particles):\n            predicted_measurement = measurement_function(self.particles[i])\n            likelihood = self._gaussian_likelihood(measurement, predicted_measurement, measurement_noise)\n            self.weights[i] *= likelihood\n\n        # Normalize weights\n        self.weights += 1e-300  # Prevent numerical issues\n        self.weights /= np.sum(self.weights)\n\n        # Resample if effective sample size is low\n        effective_samples = 1.0 / np.sum(self.weights ** 2)\n        if effective_samples < self.num_particles / 2:\n            self._resample()\n\n    def _gaussian_likelihood(self, measurement: np.ndarray, predicted: np.ndarray, noise: float):\n        """Compute Gaussian likelihood"""\n        diff = measurement - predicted\n        return np.exp(-0.5 * np.sum(diff ** 2) / noise ** 2)\n\n    def _resample(self):\n        """Resample particles based on weights"""\n        indices = np.random.choice(\n            self.num_particles,\n            size=self.num_particles,\n            p=self.weights\n        )\n        self.particles = self.particles[indices]\n        self.weights.fill(1.0 / self.num_particles)\n\n    def estimate(self) -> np.ndarray:\n        """Estimate state as weighted average of particles"""\n        return np.average(self.particles, axis=0, weights=self.weights)\n\nclass SensorFusionNetwork(nn.Module):\n    """\n    Neural network for sensor fusion\n    """\n\n    def __init__(self, sensor_configs: Dict[str, int]):\n        super().__init__()\n        self.sensor_configs = sensor_configs\n        self.num_sensors = len(sensor_configs)\n\n        # Sensor-specific encoders\n        self.encoders = nn.ModuleDict()\n        for sensor_type, input_dim in sensor_configs.items():\n            self.encoders[sensor_type] = nn.Sequential(\n                nn.Linear(input_dim, 128),\n                nn.ReLU(),\n                nn.Linear(128, 64),\n                nn.ReLU(),\n                nn.Linear(64, 32)\n            )\n\n        # Fusion layer\n        self.fusion_layer = nn.Sequential(\n            nn.Linear(32 * self.num_sensors, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64)\n        )\n\n        # Uncertainty estimation\n        self.uncertainty_head = nn.Linear(64, 1)\n\n    def forward(self, sensor_inputs: Dict[str, torch.Tensor]) -> torch.Tensor:\n        """Fuse sensor inputs"""\n        encoded_features = []\n\n        for sensor_type, encoder in self.encoders.items():\n            if sensor_type in sensor_inputs:\n                encoded = encoder(sensor_inputs[sensor_type])\n                encoded_features.append(encoded)\n            else:\n                # Use zero tensor if sensor data is missing\n                batch_size = next(iter(sensor_inputs.values())).size(0)\n                encoded_features.append(torch.zeros(batch_size, 32))\n\n        # Concatenate all encoded features\n        concat_features = torch.cat(encoded_features, dim=1)\n\n        # Fuse features\n        fused_features = self.fusion_layer(concat_features)\n\n        return fused_features\n\nclass MultimodalSensorFusion:\n    """\n    Multimodal sensor fusion system for humanoid robotics\n    """\n\n    def __init__(self, sensor_configs: Dict[str, int]):\n        self.sensor_configs = sensor_configs\n        self.fusion_network = SensorFusionNetwork(sensor_configs)\n\n        # Temporal consistency tracking\n        self.temporal_buffer = {}\n        self.buffer_size = 10\n\n        # Anomaly detection\n        self.anomaly_threshold = 0.8\n\n        # State estimators\n        self.kalman_filters = {}\n        self.particle_filters = {}\n\n    def register_kalman_filter(self, sensor_type: str, state_dim: int, measurement_dim: int):\n        """Register a Kalman filter for a sensor"""\n        self.kalman_filters[sensor_type] = KalmanFilter(state_dim, measurement_dim)\n\n    def register_particle_filter(self, sensor_type: str, state_dim: int, num_particles: int = 1000):\n        """Register a particle filter for a sensor"""\n        self.particle_filters[sensor_type] = ParticleFilter(state_dim, num_particles)\n\n    def process_sensor_readings(self, readings: List[SensorReading]) -> FusedPerceptionOutput:\n        """Process multiple sensor readings and fuse them"""\n        # Organize readings by sensor type\n        sensor_data = {}\n        for reading in readings:\n            if reading.sensor_type not in sensor_data:\n                sensor_data[reading.sensor_type] = []\n            sensor_data[reading.sensor_type].append(reading)\n\n        # Prepare input for fusion network\n        network_inputs = {}\n        for sensor_type, readings_list in sensor_data.items():\n            # Take the most recent reading for each sensor type\n            latest_reading = max(readings_list, key=lambda r: r.timestamp)\n\n            if sensor_type in self.sensor_configs:\n                network_inputs[sensor_type] = latest_reading.data\n\n        # Fuse using neural network\n        fused_features = self.fusion_network(network_inputs)\n\n        # Compute certainty scores\n        certainty_scores = self._compute_certainty_scores(network_inputs)\n\n        # Update temporal consistency\n        temporal_consistency = self._update_temporal_consistency(network_inputs)\n\n        # Compute sensor contributions\n        sensor_contributions = self._compute_sensor_contributions(network_inputs)\n\n        # Detect anomalies\n        anomalies = self._detect_anomalies(network_inputs)\n\n        return FusedPerceptionOutput(\n            fused_features=fused_features,\n            certainty_scores=certainty_scores,\n            temporal_consistency=temporal_consistency,\n            sensor_contributions=sensor_contributions,\n            anomalies=anomalies\n        )\n\n    def _compute_certainty_scores(self, sensor_inputs: Dict[str, torch.Tensor]) -> Dict[str, float]:\n        """Compute certainty scores for each sensor input"""\n        scores = {}\n        for sensor_type, data in sensor_inputs.items():\n            # Compute a simple variance-based certainty score\n            variance = torch.var(data).item()\n            # Lower variance means higher certainty\n            certainty = 1.0 / (1.0 + variance)\n            scores[sensor_type] = certainty\n        return scores\n\n    def _update_temporal_consistency(self, sensor_inputs: Dict[str, torch.Tensor]) -> float:\n        """Update and compute temporal consistency"""\n        current_time = time.time()\n\n        for sensor_type, data in sensor_inputs.items():\n            if sensor_type not in self.temporal_buffer:\n                self.temporal_buffer[sensor_type] = []\n\n            # Add current data to buffer\n            self.temporal_buffer[sensor_type].append({\n                \'data\': data.clone(),\n                \'timestamp\': current_time\n            })\n\n            # Keep only recent data\n            self.temporal_buffer[sensor_type] = [\n                item for item in self.temporal_buffer[sensor_type]\n                if current_time - item[\'timestamp\'] < 5.0  # Keep last 5 seconds\n            ]\n\n        # Compute temporal consistency as average correlation over time\n        consistency_sum = 0.0\n        consistency_count = 0\n\n        for sensor_type, buffer in self.temporal_buffer.items():\n            if len(buffer) > 1:\n                # Compute correlation between consecutive readings\n                for i in range(1, len(buffer)):\n                    prev_data = buffer[i-1][\'data\']\n                    curr_data = buffer[i][\'data\']\n\n                    # Compute cosine similarity\n                    cos_sim = torch.cosine_similarity(prev_data.flatten(), curr_data.flatten(), dim=0)\n                    consistency_sum += cos_sim.item()\n                    consistency_count += 1\n\n        return consistency_sum / consistency_count if consistency_count > 0 else 1.0\n\n    def _compute_sensor_contributions(self, sensor_inputs: Dict[str, torch.Tensor]) -> Dict[str, float]:\n        """Compute the contribution of each sensor to the fused output"""\n        contributions = {}\n\n        # For each sensor, temporarily remove it and see how much the output changes\n        with torch.no_grad():\n            # Get baseline fused output\n            baseline_output = self.fusion_network(sensor_inputs)\n\n            for sensor_type in sensor_inputs.keys():\n                # Remove this sensor and get new output\n                temp_inputs = {k: v for k, v in sensor_inputs.items() if k != sensor_type}\n\n                if temp_inputs:  # Only if there are other sensors\n                    modified_output = self.fusion_network(temp_inputs)\n\n                    # Compute difference as contribution measure\n                    diff = torch.norm(baseline_output - modified_output)\n                    contributions[sensor_type] = diff.item()\n                else:\n                    # If removing this sensor leaves no inputs, assign high contribution\n                    contributions[sensor_type] = 1.0\n\n        # Normalize contributions\n        total = sum(contributions.values())\n        if total > 0:\n            for sensor_type in contributions:\n                contributions[sensor_type] /= total\n\n        return contributions\n\n    def _detect_anomalies(self, sensor_inputs: Dict[str, torch.Tensor]) -> List[Dict[str, Any]]:\n        """Detect anomalous sensor readings"""\n        anomalies = []\n\n        for sensor_type, data in sensor_inputs.items():\n            # Check for extreme values\n            mean_val = torch.mean(data)\n            std_val = torch.std(data)\n            threshold = mean_val + 3 * std_val  # 3-sigma rule\n\n            if torch.any(torch.abs(data) > threshold):\n                anomalies.append({\n                    \'sensor_type\': sensor_type,\n                    \'anomaly_type\': \'outlier\',\n                    \'timestamp\': time.time(),\n                    \'severity\': float(torch.max(torch.abs(data) / threshold))\n                })\n\n        return anomalies\n\ndef create_sensor_fusion_system(sensor_configs: Dict[str, int]) -> MultimodalSensorFusion:\n    """Factory function to create a sensor fusion system"""\n    return MultimodalSensorFusion(sensor_configs)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"examples",children:"Examples"}),"\n",(0,i.jsx)(n.h3,{id:"example-1-object-recognition-and-manipulation-planning",children:"Example 1: Object Recognition and Manipulation Planning"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# object_recognition_manipulation.py\n\nimport numpy as np\nimport torch\nimport cv2\nfrom typing import Dict, Any, List\nimport time\n\nclass ObjectRecognitionManipulationSystem:\n    """\n    System that combines object recognition with manipulation planning\n    """\n\n    def __init__(self):\n        self.perception_system = create_multimodal_perception_system("clip")\n        self.fusion_system = create_sensor_fusion_system({\n            "camera": 3 * 224 * 224,  # RGB image\n            "depth": 224 * 224,      # Depth map\n            "imu": 6,                # Accelerometer + gyroscope\n            "force": 6               # Force/torque sensors\n        })\n\n        # Register filters for temporal consistency\n        self.fusion_system.register_kalman_filter("camera", 10, 10)  # Simplified state\n        self.fusion_system.register_particle_filter("force", 6)\n\n    def process_scene_for_manipulation(self, rgb_image: np.ndarray,\n                                     depth_map: np.ndarray,\n                                     object_query: str) -> Dict[str, Any]:\n        """Process scene to identify objects for manipulation"""\n\n        # Use multimodal perception to find target object\n        target_objects = self.perception_system.find_objects_by_description(\n            rgb_image, object_query\n        )\n\n        if not target_objects:\n            return {\n                "success": False,\n                "message": f"No objects matching \'{object_query}\' found",\n                "candidate_objects": [],\n                "manipulation_plan": None\n            }\n\n        # Get depth information for 3D positioning\n        depth_info = self._extract_depth_info(depth_map, target_objects)\n\n        # Generate manipulation plan\n        manipulation_plan = self._generate_manipulation_plan(target_objects, depth_info)\n\n        return {\n            "success": True,\n            "message": f"Found {len(target_objects)} objects matching \'{object_query}\'",\n            "candidate_objects": target_objects,\n            "manipulation_plan": manipulation_plan,\n            "depth_info": depth_info\n        }\n\n    def _extract_depth_info(self, depth_map: np.ndarray,\n                          objects: List[Dict[str, Any]]) -> Dict[str, Any]:\n        """Extract 3D information from depth map for detected objects"""\n        depth_info = {}\n\n        for obj in objects:\n            bbox = obj["bbox"]  # [x, y, width, height] - normalized\n\n            # Convert to pixel coordinates\n            h, w = depth_map.shape\n            x1 = int(bbox[0] * w)\n            y1 = int(bbox[1] * h)\n            x2 = int((bbox[0] + bbox[2]) * w)\n            y2 = int((bbox[1] + bbox[3]) * h)\n\n            # Ensure bounds are within image\n            x1, x2 = max(0, x1), min(w, x2)\n            y1, y2 = max(0, y1), min(h, y2)\n\n            if x2 > x1 and y2 > y1:\n                # Extract depth region\n                obj_depth_region = depth_map[y1:y2, x1:x2]\n\n                # Compute average depth (distance to object)\n                avg_depth = np.nanmedian(obj_depth_region[~np.isnan(obj_depth_region)])\n\n                # Compute 3D center point\n                center_x = x1 + (x2 - x1) / 2\n                center_y = y1 + (y2 - y1) / 2\n\n                depth_info[obj["label"]] = {\n                    "distance": avg_depth,\n                    "center_2d": (center_x, center_y),\n                    "bbox_3d": [x1, y1, x2, y2],\n                    "confidence": obj["confidence"]\n                }\n\n        return depth_info\n\n    def _generate_manipulation_plan(self, objects: List[Dict[str, Any]],\n                                  depth_info: Dict[str, Any]) -> List[Dict[str, Any]]:\n        """Generate a manipulation plan for the detected objects"""\n        plan = []\n\n        for obj in objects:\n            obj_label = obj["label"]\n            if obj_label in depth_info:\n                depth_data = depth_info[obj_label]\n\n                # Generate approach plan\n                approach_action = {\n                    "action": "approach_object",\n                    "target_object": obj_label,\n                    "position_3d": self._pixel_to_3d(depth_data["center_2d"], depth_data["distance"]),\n                    "grasp_strategy": self._select_grasp_strategy(obj_label),\n                    "estimated_time": 2.0  # seconds\n                }\n\n                # Generate grasp plan\n                grasp_action = {\n                    "action": "grasp_object",\n                    "target_object": obj_label,\n                    "gripper_width": self._estimate_gripper_width(obj_label),\n                    "force_limit": self._estimate_force_limit(obj_label),\n                    "estimated_time": 1.5\n                }\n\n                # Generate lift plan\n                lift_action = {\n                    "action": "lift_object",\n                    "height_offset": 0.1,  # Lift 10cm\n                    "estimated_time": 1.0\n                }\n\n                plan.extend([approach_action, grasp_action, lift_action])\n\n        return plan\n\n    def _pixel_to_3d(self, pixel_coords: Tuple[float, float], depth: float) -> List[float]:\n        """Convert 2D pixel coordinates to 3D world coordinates (simplified)"""\n        # This is a simplified transformation\n        # In practice, you\'d use camera intrinsic parameters\n        px, py = pixel_coords\n        fx, fy = 500.0, 500.0  # Focal lengths (typical values)\n        cx, cy = 320.0, 240.0  # Principal point (typical for 640x480)\n\n        # Convert to 3D assuming pinhole camera model\n        x = (px - cx) * depth / fx\n        y = (py - cy) * depth / fy\n        z = depth\n\n        return [x, y, z]\n\n    def _select_grasp_strategy(self, object_label: str) -> str:\n        """Select appropriate grasp strategy based on object type"""\n        grasp_strategies = {\n            "cup": "top_grasp",\n            "bottle": "side_grasp",\n            "box": "corner_grasp",\n            "sphere": "enclosing_grasp",\n            "cylinder": "side_grasp"\n        }\n\n        return grasp_strategies.get(object_label, "power_grasp")\n\n    def _estimate_gripper_width(self, object_label: str) -> float:\n        """Estimate appropriate gripper width for object"""\n        width_estimates = {\n            "cup": 0.08,      # 8cm\n            "bottle": 0.06,   # 6cm\n            "box": 0.10,      # 10cm\n            "sphere": 0.07,   # 7cm\n            "cylinder": 0.05  # 5cm\n        }\n\n        return width_estimates.get(object_label, 0.05)  # Default 5cm\n\n    def _estimate_force_limit(self, object_label: str) -> float:\n        """Estimate appropriate force limit for grasping"""\n        force_limits = {\n            "cup": 10.0,      # Light object\n            "bottle": 15.0,   # Medium weight\n            "box": 20.0,      # Heavier\n            "sphere": 8.0,    # Fragile\n            "cylinder": 12.0  # Medium weight\n        }\n\n        return force_limits.get(object_label, 10.0)  # Default 10N\n\n    def integrate_sensor_data(self, sensor_readings: List[SensorReading]) -> Dict[str, Any]:\n        """Integrate multiple sensor readings for robust perception"""\n        fusion_output = self.fusion_system.process_sensor_readings(sensor_readings)\n\n        # Combine with visual perception\n        result = {\n            "fused_features": fusion_output.fused_features,\n            "certainty_scores": fusion_output.certainty_scores,\n            "temporal_consistency": fusion_output.temporal_consistency,\n            "sensor_contributions": fusion_output.sensor_contributions,\n            "anomalies": fusion_output.anomalies\n        }\n\n        return result\n\ndef simulate_sensor_readings() -> List[SensorReading]:\n    """Simulate sensor readings for testing"""\n    readings = []\n\n    # Simulate camera reading\n    camera_data = torch.randn(3, 224, 224)  # RGB image\n    readings.append(SensorReading(\n        sensor_type="camera",\n        data=camera_data,\n        timestamp=time.time(),\n        confidence=0.9\n    ))\n\n    # Simulate depth reading\n    depth_data = torch.randn(224, 224)  # Depth map\n    readings.append(SensorReading(\n        sensor_type="depth",\n        data=depth_data,\n        timestamp=time.time(),\n        confidence=0.85\n    ))\n\n    # Simulate IMU reading\n    imu_data = torch.randn(6)  # Accelerometer + gyroscope\n    readings.append(SensorReading(\n        sensor_type="imu",\n        data=imu_data,\n        timestamp=time.time(),\n        confidence=0.95\n    ))\n\n    # Simulate force sensor reading\n    force_data = torch.randn(6)  # Force/torque\n    readings.append(SensorReading(\n        sensor_type="force",\n        data=force_data,\n        timestamp=time.time(),\n        confidence=0.9\n    ))\n\n    return readings\n\ndef main():\n    """Main function to demonstrate multimodal perception"""\n    print("Initializing Multimodal Perception System...")\n\n    # Create the system\n    system = ObjectRecognitionManipulationSystem()\n\n    # Simulate an RGB image (in practice, this would come from a camera)\n    dummy_rgb_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\n\n    # Simulate a depth map (in practice, this would come from a depth sensor)\n    dummy_depth_map = np.random.rand(480, 640).astype(np.float32) * 3.0  # 0-3 meters\n\n    # Process a scene to find a specific object\n    query = "red cup"\n    result = system.process_scene_for_manipulation(dummy_rgb_image, dummy_depth_map, query)\n\n    print(f"Object recognition result: {result[\'success\']}")\n    print(f"Message: {result[\'message\']}")\n    print(f"Number of candidate objects: {len(result[\'candidate_objects\'])}")\n    print(f"Manipulation plan steps: {len(result[\'manipulation_plan\']) if result[\'manipulation_plan\'] else 0}")\n\n    # Simulate sensor fusion\n    sensor_readings = simulate_sensor_readings()\n    fusion_result = system.integrate_sensor_data(sensor_readings)\n\n    print(f"Sensor fusion completed with {len(fusion_result[\'certainty_scores\'])} sensor types")\n    print(f"Temporal consistency: {fusion_result[\'temporal_consistency\']:.3f}")\n    print(f"Detected anomalies: {len(fusion_result[\'anomalies\'])}")\n\n    print("\\nMultimodal perception system demonstration completed!")\n    print("In a real implementation, this would connect to actual robot sensors and perception systems.")\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"example-2-scene-understanding-and-navigation-planning",children:"Example 2: Scene Understanding and Navigation Planning"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# scene_understanding_navigation.py\n\nimport numpy as np\nimport torch\nfrom typing import Dict, Any, List, Tuple\nimport time\n\nclass SceneUnderstandingNavigationSystem:\n    """\n    System that combines scene understanding with navigation planning\n    """\n\n    def __init__(self):\n        self.perception_system = create_multimodal_perception_system("clip")\n        self.fusion_system = create_sensor_fusion_system({\n            "camera": 3 * 224 * 224,\n            "lidar": 360,  # Simplified LIDAR data\n            "imu": 6,\n            "odometry": 3\n        })\n\n    def analyze_scene_for_navigation(self, rgb_image: np.ndarray,\n                                   lidar_scan: np.ndarray,\n                                   navigation_goal: str) -> Dict[str, Any]:\n        """Analyze scene to plan navigation to a goal location"""\n\n        # Use multimodal perception to understand the scene\n        perception_output = self.perception_system.process_image(rgb_image, [navigation_goal])\n\n        # Integrate with LIDAR data for obstacle detection\n        obstacles = self._process_lidar_data(lidar_scan)\n\n        # Combine visual and LIDAR information\n        combined_analysis = self._combine_visual_lidar(perception_output, obstacles)\n\n        # Generate navigation plan\n        navigation_plan = self._generate_navigation_plan(combined_analysis, navigation_goal)\n\n        return {\n            "success": True,\n            "scene_analysis": combined_analysis,\n            "navigation_plan": navigation_plan,\n            "obstacles": obstacles,\n            "spatial_relationships": perception_output.spatial_relationships\n        }\n\n    def _process_lidar_data(self, lidar_scan: np.ndarray) -> List[Dict[str, Any]]:\n        """Process LIDAR scan to detect obstacles"""\n        obstacles = []\n\n        # Simple threshold-based obstacle detection\n        distance_threshold = 1.0  # meters\n        min_points = 3  # Minimum points to consider as obstacle\n\n        current_cluster = []\n        for i, distance in enumerate(lidar_scan):\n            if distance < distance_threshold and not np.isnan(distance):\n                # Convert polar to Cartesian coordinates\n                angle = (i / len(lidar_scan)) * 2 * np.pi\n                x = distance * np.cos(angle)\n                y = distance * np.sin(angle)\n\n                current_cluster.append((x, y))\n            else:\n                if len(current_cluster) >= min_points:\n                    # Calculate cluster center and size\n                    cluster_array = np.array(current_cluster)\n                    center_x = np.mean(cluster_array[:, 0])\n                    center_y = np.mean(cluster_array[:, 1])\n                    size = np.std(cluster_array)\n\n                    obstacles.append({\n                        "type": "obstacle",\n                        "center": (center_x, center_y),\n                        "size": size,\n                        "points": current_cluster.copy()\n                    })\n\n                current_cluster = []\n\n        # Handle the last cluster if it exists\n        if len(current_cluster) >= min_points:\n            cluster_array = np.array(current_cluster)\n            center_x = np.mean(cluster_array[:, 0])\n            center_y = np.mean(cluster_array[:, 1])\n            size = np.std(cluster_array)\n\n            obstacles.append({\n                "type": "obstacle",\n                "center": (center_x, center_y),\n                "size": size,\n                "points": current_cluster\n            })\n\n        return obstacles\n\n    def _combine_visual_lidar(self, perception_output: PerceptionOutput,\n                            lidar_obstacles: List[Dict[str, Any]]) -> Dict[str, Any]:\n        """Combine visual perception with LIDAR data"""\n        combined = {\n            "visual_objects": perception_output.objects,\n            "lidar_obstacles": lidar_obstacles,\n            "spatial_relationships": perception_output.spatial_relationships,\n            "scene_description": perception_output.scene_description,\n            "combined_map": self._create_combined_map(perception_output, lidar_obstacles)\n        }\n\n        return combined\n\n    def _create_combined_map(self, perception_output: PerceptionOutput,\n                           lidar_obstacles: List[Dict[str, Any]]) -> Dict[str, Any]:\n        """Create a combined occupancy map from visual and LIDAR data"""\n        # This would typically create a 2D or 3D occupancy grid\n        # For this example, we\'ll return a simplified representation\n\n        map_data = {\n            "free_space": [],\n            "occupied_space": [],\n            "object_locations": {},\n            "obstacle_locations": []\n        }\n\n        # Add object locations from visual perception\n        for obj in perception_output.objects:\n            # This would convert from image coordinates to world coordinates\n            # For now, we\'ll just store the label and confidence\n            map_data["object_locations"][obj["label"]] = {\n                "confidence": obj["confidence"],\n                "bbox": obj["bbox"]\n            }\n\n        # Add obstacle locations from LIDAR\n        for obstacle in lidar_obstacles:\n            map_data["obstacle_locations"].append(obstacle["center"])\n\n        return map_data\n\n    def _generate_navigation_plan(self, scene_analysis: Dict[str, Any],\n                                goal_description: str) -> List[Dict[str, Any]]:\n        """Generate a navigation plan based on scene analysis"""\n        plan = []\n\n        # Analyze the goal description to find relevant objects/locations\n        relevant_objects = self._find_relevant_objects(scene_analysis, goal_description)\n\n        if relevant_objects:\n            # Create navigation steps to approach relevant objects\n            for obj_info in relevant_objects:\n                approach_step = {\n                    "action": "navigate_to_object",\n                    "target": obj_info["label"],\n                    "position": self._estimate_object_position(obj_info),\n                    "estimated_distance": obj_info.get("estimated_distance", 1.0),\n                    "estimated_time": 5.0  # seconds\n                }\n                plan.append(approach_step)\n\n                # Add inspection step\n                inspect_step = {\n                    "action": "inspect_object",\n                    "target": obj_info["label"],\n                    "estimated_time": 2.0\n                }\n                plan.append(inspect_step)\n        else:\n            # If no specific objects found, create a general exploration plan\n            exploration_step = {\n                "action": "explore_towards_goal",\n                "goal_description": goal_description,\n                "estimated_time": 10.0\n            }\n            plan.append(exploration_step)\n\n        # Add safety checks\n        safety_check = {\n            "action": "check_surroundings",\n            "estimated_time": 1.0\n        }\n        plan.append(safety_check)\n\n        return plan\n\n    def _find_relevant_objects(self, scene_analysis: Dict[str, Any],\n                             goal_description: str) -> List[Dict[str, Any]]:\n        """Find objects that are relevant to the navigation goal"""\n        relevant_objects = []\n\n        # Simple keyword matching approach\n        goal_lower = goal_description.lower()\n\n        for obj in scene_analysis["visual_objects"]:\n            obj_label = obj["label"].lower()\n            if obj_label in goal_lower or any(keyword in goal_lower for keyword in [obj_label, "near", "by", "at"]):\n                relevant_objects.append({\n                    "label": obj["label"],\n                    "confidence": obj["confidence"],\n                    "bbox": obj["bbox"]\n                })\n\n        return relevant_objects\n\n    def _estimate_object_position(self, obj_info: Dict[str, Any]) -> List[float]:\n        """Estimate 3D position of an object (simplified)"""\n        # This would normally use depth information\n        # For now, return a placeholder position\n        return [1.0, 0.5, 0.0]  # x, y, z in meters\n\n    def update_navigation_map(self, sensor_readings: List[SensorReading]) -> Dict[str, Any]:\n        """Update navigation map with new sensor data"""\n        fusion_output = self.fusion_system.process_sensor_readings(sensor_readings)\n\n        # Update the navigation map based on fused sensor data\n        updated_map = {\n            "timestamp": time.time(),\n            "fused_features": fusion_output.fused_features,\n            "updated_obstacles": self._detect_new_obstacles(fusion_output),\n            "certainty_map": fusion_output.certainty_scores,\n            "temporal_consistency": fusion_output.temporal_consistency\n        }\n\n        return updated_map\n\n    def _detect_new_obstacles(self, fusion_output: FusedPerceptionOutput) -> List[Dict[str, Any]]:\n        """Detect new obstacles from fused sensor data"""\n        # This would analyze the fused features to identify new obstacles\n        # For this example, we\'ll return mock data\n        return [\n            {"position": [2.0, 1.5, 0.0], "size": 0.3, "confidence": 0.9},\n            {"position": [0.5, 2.0, 0.0], "size": 0.5, "confidence": 0.8}\n        ]\n\ndef simulate_navigation_scenario():\n    """Simulate a navigation scenario"""\n    system = SceneUnderstandingNavigationSystem()\n\n    # Simulate sensor data\n    dummy_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\n    dummy_lidar = np.random.rand(360).astype(np.float32) * 5.0  # 0-5 meters\n\n    # Analyze scene for navigation to a specific goal\n    goal = "navigate to the kitchen counter"\n    result = system.analyze_scene_for_navigation(dummy_image, dummy_lidar, goal)\n\n    print(f"Navigation analysis completed:")\n    print(f"- Objects detected: {len(result[\'scene_analysis\'][\'visual_objects\'])}")\n    print(f"- LIDAR obstacles: {len(result[\'obstacles\'])}")\n    print(f"- Navigation plan steps: {len(result[\'navigation_plan\'])}")\n    print(f"- Spatial relationships: {len(result[\'spatial_relationships\'])}")\n\n    # Simulate updating navigation map with new sensor readings\n    sensor_readings = simulate_sensor_readings()  # From previous example\n    map_update = system.update_navigation_map(sensor_readings)\n\n    print(f"\\nNavigation map updated:")\n    print(f"- New obstacles detected: {len(map_update[\'updated_obstacles\'])}")\n    print(f"- Certainty scores: {list(map_update[\'certainty_map\'].keys())}")\n    print(f"- Temporal consistency: {map_update[\'temporal_consistency\']:.3f}")\n\ndef main():\n    """Main function for scene understanding and navigation"""\n    print("Initializing Scene Understanding and Navigation System...")\n\n    simulate_navigation_scenario()\n\n    print("\\nScene understanding and navigation system demonstrated!")\n    print("This system can analyze scenes and generate navigation plans based on multimodal perception.")\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"Multimodal perception systems integrate multiple sensory inputs to create a comprehensive understanding of the environment for humanoid robots. Key components include:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Vision-Language Integration"}),": Combining visual and textual information for object recognition and scene understanding"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sensor Fusion"}),": Integrating data from multiple sensors (cameras, LIDAR, IMU, etc.) for robust perception"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Spatial Reasoning"}),": Understanding spatial relationships between objects and the environment"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Temporal Consistency"}),": Maintaining coherent understanding across time"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Anomaly Detection"}),": Identifying unexpected or anomalous sensor readings"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"The effectiveness of these systems depends on proper integration of different modalities and robust handling of sensor noise and uncertainty."}),"\n",(0,i.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,i.jsx)(n.h3,{id:"conceptual",children:"Conceptual"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Compare and contrast different sensor fusion approaches (Kalman filtering, particle filtering, neural network fusion). What are the trade-offs of each approach for humanoid robotics?"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"logical",children:"Logical"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Design a multimodal perception system that can handle sensor failures gracefully. How would your system maintain functionality when one or more sensors become unavailable?"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"implementation-1",children:"Implementation"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Implement a complete multimodal perception pipeline that integrates vision, LIDAR, and IMU data for object recognition and navigation planning in a humanoid robot simulation."}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);