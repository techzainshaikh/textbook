"use strict";(globalThis.webpackChunkphysical_ai_book_humanoid=globalThis.webpackChunkphysical_ai_book_humanoid||[]).push([[4459],{5328(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>m,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-3-ai-brain/chapter-3-perception-pipelines","title":"Perception Pipelines for Robotics AI","description":"Building AI-powered perception systems for humanoid robots using Isaac ROS","source":"@site/docs/module-3-ai-brain/chapter-3-perception-pipelines.md","sourceDirName":"module-3-ai-brain","slug":"/module-3-ai-brain/chapter-3-perception-pipelines","permalink":"/textbook/docs/module-3-ai-brain/chapter-3-perception-pipelines","draft":false,"unlisted":false,"editUrl":"https://github.com/Panaversity/physical_ai_book_humanoid/tree/main/docs/module-3-ai-brain/chapter-3-perception-pipelines.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Perception Pipelines for Robotics AI","sidebar_position":4,"description":"Building AI-powered perception systems for humanoid robots using Isaac ROS","keywords":["perception","computer vision","ai","robotics","Isaac ROS","sensor fusion"]},"sidebar":"tutorialSidebar","previous":{"title":"Synthetic Data Generation for AI Training","permalink":"/textbook/docs/module-3-ai-brain/chapter-2-synthetic-data"},"next":{"title":"Navigation and Path Planning with Isaac ROS","permalink":"/textbook/docs/module-3-ai-brain/chapter-4-nav2-planning"}}');var s=t(4848),o=t(8453);const r={title:"Perception Pipelines for Robotics AI",sidebar_position:4,description:"Building AI-powered perception systems for humanoid robots using Isaac ROS",keywords:["perception","computer vision","ai","robotics","Isaac ROS","sensor fusion"]},a="Chapter 3: Perception Pipelines for Robotics AI",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Core Concepts",id:"core-concepts",level:2},{value:"Perception Pipeline Architecture",id:"perception-pipeline-architecture",level:3},{value:"Isaac ROS Perception Framework",id:"isaac-ros-perception-framework",level:3},{value:"Implementation",id:"implementation",level:2},{value:"Isaac ROS Perception Pipeline Setup",id:"isaac-ros-perception-pipeline-setup",level:3},{value:"Isaac ROS Sensor Fusion Implementation",id:"isaac-ros-sensor-fusion-implementation",level:3},{value:"Examples",id:"examples",level:2},{value:"Example 1: Isaac ROS Visual SLAM Integration",id:"example-1-isaac-ros-visual-slam-integration",level:3},{value:"Example 2: Deep Learning Perception Pipeline",id:"example-2-deep-learning-perception-pipeline",level:3},{value:"Summary",id:"summary",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Conceptual",id:"conceptual",level:3},{value:"Logical",id:"logical",level:3},{value:"Implementation",id:"implementation-1",level:3}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"chapter-3-perception-pipelines-for-robotics-ai",children:"Chapter 3: Perception Pipelines for Robotics AI"})}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"By the end of this chapter, students will be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Design and implement perception pipelines using Isaac ROS for humanoid robots"}),"\n",(0,s.jsx)(n.li,{children:"Integrate multiple sensor modalities for robust perception"}),"\n",(0,s.jsx)(n.li,{children:"Implement sensor fusion algorithms for enhanced environment understanding"}),"\n",(0,s.jsx)(n.li,{children:"Apply deep learning models for object detection and scene understanding"}),"\n",(0,s.jsx)(n.li,{children:"Validate perception system performance in simulation and real-world scenarios"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsx)(n.p,{children:"Students should have:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understanding of computer vision fundamentals (covered in Module 2)"}),"\n",(0,s.jsx)(n.li,{children:"Knowledge of sensor simulation (covered in Module 2, Chapter 2)"}),"\n",(0,s.jsx)(n.li,{children:"Experience with ROS 2 messaging (covered in Module 1)"}),"\n",(0,s.jsx)(n.li,{children:"Basic knowledge of deep learning and neural networks"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"core-concepts",children:"Core Concepts"}),"\n",(0,s.jsx)(n.p,{children:"Perception pipelines form the sensory foundation of AI-powered humanoid robots, enabling them to understand and interact with their environment. Isaac ROS provides GPU-accelerated perception capabilities that leverage NVIDIA hardware for real-time performance."}),"\n",(0,s.jsx)(n.h3,{id:"perception-pipeline-architecture",children:"Perception Pipeline Architecture"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Sensor Input Layer:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Multiple sensor modalities (LiDAR, cameras, IMU, GPS, etc.)"}),"\n",(0,s.jsx)(n.li,{children:"Synchronized data acquisition and timestamp management"}),"\n",(0,s.jsx)(n.li,{children:"Raw data preprocessing and calibration"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Feature Extraction Layer:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"GPU-accelerated computer vision algorithms"}),"\n",(0,s.jsx)(n.li,{children:"Deep learning inference for object detection/segmentation"}),"\n",(0,s.jsx)(n.li,{children:"Geometric feature extraction (edges, corners, surfaces)"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Data Association Layer:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Feature matching and correspondence establishment"}),"\n",(0,s.jsx)(n.li,{children:"Temporal association for tracking"}),"\n",(0,s.jsx)(n.li,{children:"Multi-view geometric relationships"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"State Estimation Layer:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Filtering and prediction algorithms (Kalman, particle filters)"}),"\n",(0,s.jsx)(n.li,{children:"Sensor fusion for state estimation"}),"\n",(0,s.jsx)(n.li,{children:"Uncertainty quantification and propagation"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"isaac-ros-perception-framework",children:"Isaac ROS Perception Framework"}),"\n",(0,s.jsx)(n.p,{children:"Isaac ROS provides several key perception capabilities:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Isaac ROS Apriltag"}),": High-precision fiducial marker detection"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Isaac ROS Stereo Dense Reconstruction"}),": 3D scene reconstruction"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Isaac ROS Detection ROS"}),": Object detection with deep learning"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Isaac ROS Visual Slam"}),": Visual simultaneous localization and mapping"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Isaac ROS ISAAC"}),": Inertial sensor array conditioning"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"implementation",children:"Implementation"}),"\n",(0,s.jsx)(n.p,{children:"Let's implement a comprehensive perception pipeline using Isaac ROS:"}),"\n",(0,s.jsx)(n.h3,{id:"isaac-ros-perception-pipeline-setup",children:"Isaac ROS Perception Pipeline Setup"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n# perception_pipeline.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo, PointCloud2, Imu, LaserScan\nfrom geometry_msgs.msg import PoseStamped, Twist\nfrom std_msgs.msg import String\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport cv2\nimport message_filters\nfrom tf2_ros import TransformListener, Buffer\nimport tf2_geometry_msgs\n\nclass IsaacPerceptionPipeline(Node):\n    \"\"\"\n    Perception pipeline using Isaac ROS for humanoid robot\n    \"\"\"\n\n    def __init__(self):\n        super().__init__('isaac_perception_pipeline')\n\n        # Publishers for perception outputs\n        self.object_detections_pub = self.create_publisher(String, '/humanoid/perception/objects', 10)\n        self.tracked_objects_pub = self.create_publisher(String, '/humanoid/perception/tracked_objects', 10)\n        self.environment_map_pub = self.create_publisher(String, '/humanoid/perception/environment_map', 10)\n        self.status_pub = self.create_publisher(String, '/humanoid/perception/status', 10)\n\n        # Subscribers with message filters for synchronization\n        self.rgb_sub = message_filters.Subscriber(self, Image, '/humanoid/camera/rgb/image_raw')\n        self.depth_sub = message_filters.Subscriber(self, Image, '/humanoid/camera/depth/image_raw')\n        self.info_sub = message_filters.Subscriber(self, CameraInfo, '/humanoid/camera/rgb/camera_info')\n\n        # Synchronize RGB, depth, and camera info\n        self.sync = message_filters.ApproximateTimeSynchronizer(\n            [self.rgb_sub, self.depth_sub, self.info_sub],\n            queue_size=10,\n            slop=0.1\n        )\n        self.sync.registerCallback(self.camera_callback)\n\n        # LiDAR subscriber\n        self.lidar_sub = self.create_subscription(\n            LaserScan,\n            '/humanoid/lidar/scan',\n            self.lidar_callback,\n            10\n        )\n\n        # IMU subscriber\n        self.imu_sub = self.create_subscription(\n            Imu,\n            '/humanoid/imu/data',\n            self.imu_callback,\n            10\n        )\n\n        # TF buffer and listener\n        self.tf_buffer = Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n\n        # CV Bridge for image processing\n        self.cv_bridge = CvBridge()\n\n        # Perception state\n        self.objects_in_view = []\n        self.tracked_objects_history = {}\n        self.environment_map = {}\n        self.robot_pose = None\n\n        # Timers\n        self.perception_timer = self.create_timer(0.1, self.process_perception_data)  # 10 Hz\n        self.status_timer = self.create_timer(1.0, self.publish_status)\n\n        self.get_logger().info('Isaac Perception Pipeline initialized')\n\n    def camera_callback(self, rgb_msg, depth_msg, info_msg):\n        \"\"\"Process synchronized camera data\"\"\"\n        try:\n            # Convert ROS images to OpenCV format\n            rgb_image = self.cv_bridge.imgmsg_to_cv2(rgb_msg, 'rgb8')\n            depth_image = self.cv_bridge.imgmsg_to_cv2(depth_msg, '32FC1')\n\n            # Process perception pipeline\n            self.process_camera_perception(rgb_image, depth_image, info_msg)\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing camera data: {e}')\n\n    def lidar_callback(self, msg):\n        \"\"\"Process LiDAR data for environment mapping\"\"\"\n        try:\n            # Process LiDAR scan for obstacle detection and mapping\n            self.process_lidar_perception(msg)\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing LiDAR data: {e}')\n\n    def imu_callback(self, msg):\n        \"\"\"Process IMU data for state estimation\"\"\"\n        try:\n            # Process IMU data for orientation and motion estimation\n            self.process_imu_perception(msg)\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing IMU data: {e}')\n\n    def process_camera_perception(self, rgb_image, depth_image, camera_info):\n        \"\"\"Process camera perception pipeline\"\"\"\n        # Step 1: Object detection using Isaac ROS (simulated)\n        detected_objects = self.detect_objects_isaac(rgb_image)\n\n        # Step 2: Depth-based filtering and 3D positioning\n        positioned_objects = self.filter_by_depth(detected_objects, depth_image, camera_info)\n\n        # Step 3: Track objects temporally\n        tracked_objects = self.track_objects(positioned_objects)\n\n        # Step 4: Update environment map\n        self.update_environment_map(tracked_objects)\n\n        # Publish results\n        self.publish_object_detections(tracked_objects)\n\n    def detect_objects_isaac(self, image):\n        \"\"\"Simulate Isaac ROS object detection\"\"\"\n        # In real implementation, this would call Isaac ROS object detection nodes\n        # For this example, we'll simulate object detection using OpenCV\n\n        # Convert to grayscale\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n\n        # Simple blob detection as a proxy for object detection\n        detector = cv2.SimpleBlobDetector_create()\n        keypoints = detector.detect(gray)\n\n        # Convert to object format\n        objects = []\n        for kp in keypoints:\n            if kp.size > 10:  # Filter small detections\n                obj = {\n                    'center': (int(kp.pt[0]), int(kp.pt[1])),\n                    'size': kp.size,\n                    'confidence': min(kp.response * 10, 1.0)  # Normalize confidence\n                }\n                objects.append(obj)\n\n        return objects\n\n    def filter_by_depth(self, objects, depth_image, camera_info):\n        \"\"\"Filter objects by depth and estimate 3D positions\"\"\"\n        # Camera intrinsic parameters\n        fx = camera_info.K[0]  # Focal length x\n        fy = camera_info.K[4]  # Focal length y\n        cx = camera_info.K[2]  # Principal point x\n        cy = camera_info.K[5]  # Principal point y\n\n        positioned_objects = []\n\n        for obj in objects:\n            center_x, center_y = obj['center']\n\n            # Get depth at object center\n            if 0 <= center_y < depth_image.shape[0] and 0 <= center_x < depth_image.shape[1]:\n                depth = depth_image[center_y, center_x]\n\n                if depth > 0 and np.isfinite(depth):\n                    # Calculate 3D position from 2D pixel + depth\n                    x = (center_x - cx) * depth / fx\n                    y = (center_y - cy) * depth / fy\n                    z = depth\n\n                    positioned_obj = {\n                        'name': f'object_{len(positioned_objects)}',\n                        'position_3d': [x, y, z],\n                        'position_2d': obj['center'],\n                        'size': obj['size'],\n                        'confidence': obj['confidence'],\n                        'timestamp': self.get_clock().now().seconds_nanoseconds()\n                    }\n\n                    positioned_objects.append(positioned_obj)\n\n        return positioned_objects\n\n    def track_objects(self, positioned_objects):\n        \"\"\"Track objects across frames\"\"\"\n        current_time = self.get_clock().now()\n\n        for obj in positioned_objects:\n            obj_id = obj['name']\n\n            if obj_id in self.tracked_objects_history:\n                # Update existing track\n                prev_pos = self.tracked_objects_history[obj_id]['position_3d']\n                current_pos = obj['position_3d']\n\n                # Calculate velocity\n                dt = (current_time.nanoseconds - self.tracked_objects_history[obj_id]['timestamp'][1]) / 1e9\n                if dt > 0:\n                    velocity = [(cp - pp) / dt for cp, pp in zip(current_pos, prev_pos)]\n                    obj['velocity'] = velocity\n                else:\n                    obj['velocity'] = [0, 0, 0]\n\n                # Update history\n                self.tracked_objects_history[obj_id].update(obj)\n            else:\n                # Initialize new track\n                obj['velocity'] = [0, 0, 0]\n                obj['track_start_time'] = current_time\n                self.tracked_objects_history[obj_id] = obj\n\n        # Clean up old tracks\n        self.cleanup_old_tracks(current_time)\n\n        return list(self.tracked_objects_history.values())\n\n    def cleanup_old_tracks(self, current_time):\n        \"\"\"Remove old object tracks that haven't been updated\"\"\"\n        # Remove tracks older than 5 seconds\n        old_tracks = []\n        for obj_id, track in self.tracked_objects_history.items():\n            track_age = current_time.nanoseconds - track['timestamp'][1]\n            if track_age > 5e9:  # 5 seconds in nanoseconds\n                old_tracks.append(obj_id)\n\n        for obj_id in old_tracks:\n            del self.tracked_objects_history[obj_id]\n\n    def process_lidar_perception(self, lidar_msg):\n        \"\"\"Process LiDAR data for environment mapping\"\"\"\n        # Process LiDAR ranges to detect obstacles and map environment\n        ranges = np.array(lidar_msg.ranges)\n        angles = np.linspace(lidar_msg.angle_min, lidar_msg.angle_max, len(ranges))\n\n        # Filter valid ranges\n        valid_mask = np.isfinite(ranges) & (ranges > lidar_msg.range_min) & (ranges < lidar_msg.range_max)\n        valid_ranges = ranges[valid_mask]\n        valid_angles = angles[valid_mask]\n\n        # Convert to Cartesian coordinates\n        x_coords = valid_ranges * np.cos(valid_angles)\n        y_coords = valid_ranges * np.sin(valid_angles)\n\n        # Create occupancy grid or point cloud representation\n        environment_points = list(zip(x_coords, y_coords))\n\n        # Update environment map with LiDAR data\n        self.environment_map['lidar_points'] = environment_points\n        self.environment_map['timestamp'] = self.get_clock().now().seconds_nanoseconds()\n\n    def process_imu_perception(self, imu_msg):\n        \"\"\"Process IMU data for state estimation\"\"\"\n        # Extract orientation from IMU\n        orientation = [\n            imu_msg.orientation.x,\n            imu_msg.orientation.y,\n            imu_msg.orientation.z,\n            imu_msg.orientation.w\n        ]\n\n        # Extract angular velocity\n        angular_vel = [\n            imu_msg.angular_velocity.x,\n            imu_msg.angular_velocity.y,\n            imu_msg.angular_velocity.z\n        ]\n\n        # Extract linear acceleration\n        linear_acc = [\n            imu_msg.linear_acceleration.x,\n            imu_msg.linear_acceleration.y,\n            imu_msg.linear_acceleration.z\n        ]\n\n        # Update internal state\n        self.robot_state = {\n            'orientation': orientation,\n            'angular_velocity': angular_vel,\n            'linear_acceleration': linear_acc,\n            'timestamp': self.get_clock().now().seconds_nanoseconds()\n        }\n\n    def update_environment_map(self, tracked_objects):\n        \"\"\"Update environment map with perception results\"\"\"\n        # Combine object detections with LiDAR environment mapping\n        if 'lidar_points' in self.environment_map:\n            self.environment_map['objects'] = tracked_objects\n            self.environment_map['updated'] = self.get_clock().now().seconds_nanoseconds()\n\n    def publish_object_detections(self, tracked_objects):\n        \"\"\"Publish object detections to ROS topic\"\"\"\n        if not tracked_objects:\n            return\n\n        # Create JSON-like message with object information\n        detection_msg = String()\n        detection_data = {\n            'timestamp': self.get_clock().now().seconds_nanoseconds(),\n            'objects': [\n                {\n                    'id': obj['name'],\n                    'position_3d': obj['position_3d'],\n                    'position_2d': obj['position_2d'],\n                    'velocity': obj.get('velocity', [0, 0, 0]),\n                    'confidence': obj['confidence']\n                } for obj in tracked_objects if obj['confidence'] > 0.5  # Filter low-confidence detections\n            ]\n        }\n\n        detection_msg.data = str(detection_data)\n        self.object_detections_pub.publish(detection_msg)\n\n    def process_perception_data(self):\n        \"\"\"Main perception processing loop\"\"\"\n        # This would coordinate all perception modules\n        # In a real implementation, this would run more sophisticated processing\n        pass\n\n    def publish_status(self):\n        \"\"\"Publish perception system status\"\"\"\n        status_msg = String()\n        status_msg.data = f\"Perception system active - Objects detected: {len(self.tracked_objects_history)}, \" \\\n                         f\"Environment points: {len(self.environment_map.get('lidar_points', [])) if self.environment_map else 0}\"\n        self.status_pub.publish(status_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    perception_node = IsaacPerceptionPipeline()\n\n    try:\n        rclpy.spin(perception_node)\n    except KeyboardInterrupt:\n        perception_node.get_logger().info('Shutting down Isaac Perception Pipeline...')\n    finally:\n        perception_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h3,{id:"isaac-ros-sensor-fusion-implementation",children:"Isaac ROS Sensor Fusion Implementation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# sensor_fusion.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, PointCloud2, Imu, LaserScan\nfrom geometry_msgs.msg import PoseWithCovarianceStamped, TwistStamped\nfrom std_msgs.msg import Float32MultiArray\nfrom tf2_ros import TransformListener, Buffer\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\nimport message_filters\n\nclass IsaacSensorFusion(Node):\n    """\n    Sensor fusion system using Isaac ROS capabilities\n    """\n\n    def __init__(self):\n        super().__init__(\'isaac_sensor_fusion\')\n\n        # Publishers\n        self.fused_pose_pub = self.create_publisher(PoseWithCovarianceStamped, \'/humanoid/perception/fused_pose\', 10)\n        self.fused_twist_pub = self.create_publisher(TwistStamped, \'/humanoid/perception/fused_twist\', 10)\n        self.fusion_status_pub = self.create_publisher(Float32MultiArray, \'/humanoid/perception/fusion_status\', 10)\n\n        # Subscribers with synchronization\n        self.imu_sub = message_filters.Subscriber(self, Imu, \'/humanoid/imu/data_raw\')\n        self.lidar_sub = message_filters.Subscriber(self, LaserScan, \'/humanoid/lidar/scan\')\n        self.odom_sub = message_filters.Subscriber(self, Odometry, \'/humanoid/odom\')\n\n        # Synchronize sensors\n        self.sensor_sync = message_filters.ApproximateTimeSynchronizer(\n            [self.imu_sub, self.lidar_sub, self.odom_sub],\n            queue_size=10,\n            slop=0.1\n        )\n        self.sensor_sync.registerCallback(self.sensors_callback)\n\n        # TF listener\n        self.tf_buffer = Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n\n        # State estimation variables\n        self.position = np.array([0.0, 0.0, 0.0])\n        self.velocity = np.array([0.0, 0.0, 0.0])\n        self.orientation = np.array([0.0, 0.0, 0.0, 1.0])  # quaternion\n        self.angular_velocity = np.array([0.0, 0.0, 0.0])\n\n        # Covariance matrices\n        self.position_covariance = np.eye(3) * 0.1\n        self.orientation_covariance = np.eye(3) * 0.01\n\n        # Kalman filter parameters\n        self.process_noise = np.eye(6) * 0.01\n        self.measurement_noise = np.eye(6) * 0.1\n\n        # Timers\n        self.fusion_timer = self.create_timer(0.05, self.run_sensor_fusion)  # 20 Hz\n\n        self.get_logger().info(\'Isaac Sensor Fusion initialized\')\n\n    def sensors_callback(self, imu_msg, lidar_msg, odom_msg):\n        """Process synchronized sensor data"""\n        # Update internal state with new sensor readings\n        self.update_imu_state(imu_msg)\n        self.update_odom_state(odom_msg)\n        self.update_lidar_state(lidar_msg)\n\n    def update_imu_state(self, imu_msg):\n        """Update state with IMU data"""\n        # Update orientation from IMU\n        self.orientation = np.array([\n            imu_msg.orientation.x,\n            imu_msg.orientation.y,\n            imu_msg.orientation.z,\n            imu_msg.orientation.w\n        ])\n\n        # Update angular velocity\n        self.angular_velocity = np.array([\n            imu_msg.angular_velocity.x,\n            imu_msg.angular_velocity.y,\n            imu_msg.angular_velocity.z\n        ])\n\n        # Update linear acceleration (for state prediction)\n        linear_acc = np.array([\n            imu_msg.linear_acceleration.x,\n            imu_msg.linear_acceleration.y,\n            imu_msg.linear_acceleration.z\n        ])\n\n        # Integrate acceleration to update velocity and position\n        dt = 0.05  # Assuming 20Hz update rate\n        self.velocity += linear_acc * dt\n        self.position += self.velocity * dt\n\n    def update_odom_state(self, odom_msg):\n        """Update state with odometry data"""\n        # Extract position from odometry\n        pos = odom_msg.pose.pose.position\n        self.position = np.array([pos.x, pos.y, pos.z])\n\n        # Extract orientation from odometry\n        orient = odom_msg.pose.pose.orientation\n        self.orientation = np.array([orient.x, orient.y, orient.z, orient.w])\n\n        # Extract velocity from odometry\n        vel = odom_msg.twist.twist.linear\n        self.velocity = np.array([vel.x, vel.y, vel.z])\n\n    def update_lidar_state(self, lidar_msg):\n        """Update state with LiDAR-based position estimates"""\n        # Process LiDAR scan for position corrections\n        # This would typically involve scan matching or landmark detection\n        # For this example, we\'ll just use it to validate position estimates\n\n        # Example: Calculate distance to nearest obstacle\n        valid_ranges = [r for r in lidar_msg.ranges if np.isfinite(r)]\n        if valid_ranges:\n            min_range = min(valid_ranges)\n\n            # If we\'re getting too close to obstacles, this might indicate\n            # a problem with our position estimate\n            if min_range < 0.5:  # Less than 50cm to obstacle\n                self.get_logger().warn(f\'Close to obstacle: {min_range:.2f}m - check position estimate\')\n\n    def run_sensor_fusion(self):\n        """Execute sensor fusion algorithm"""\n        try:\n            # Apply Kalman filter to fuse sensor data\n            self.predict_state()\n            self.update_state_with_measurements()\n            self.publish_fused_state()\n\n        except Exception as e:\n            self.get_logger().error(f\'Error in sensor fusion: {e}\')\n\n    def predict_state(self):\n        """Predict state based on motion model"""\n        # Use IMU data for prediction\n        dt = 0.05  # Time step\n\n        # Predict position based on current velocity\n        self.position += self.velocity * dt\n\n        # Predict velocity based on IMU acceleration\n        # (would need to transform IMU acceleration to world frame)\n        # For simplicity, we\'ll just keep the current velocity with some decay\n\n    def update_state_with_measurements(self):\n        """Update state with sensor measurements"""\n        # This is where the Kalman update step would occur\n        # In a real implementation, we would fuse:\n        # - Visual-inertial odometry\n        # - LiDAR-based position estimates\n        # - Wheel odometry\n        # - IMU measurements\n\n        # For this example, we\'ll just apply simple averaging\n        # with different weights for different sensor types\n\n        # Weighted fusion would happen here\n        pass\n\n    def publish_fused_state(self):\n        """Publish fused state estimate"""\n        # Publish fused pose with covariance\n        pose_msg = PoseWithCovarianceStamped()\n        pose_msg.header.stamp = self.get_clock().now().to_msg()\n        pose_msg.header.frame_id = \'map\'\n\n        # Fill position\n        pose_msg.pose.pose.position.x = self.position[0]\n        pose_msg.pose.pose.position.y = self.position[1]\n        pose_msg.pose.pose.position.z = self.position[2]\n\n        # Fill orientation\n        pose_msg.pose.pose.orientation.x = self.orientation[0]\n        pose_msg.pose.pose.orientation.y = self.orientation[1]\n        pose_msg.pose.pose.orientation.z = self.orientation[2]\n        pose_msg.pose.pose.orientation.w = self.orientation[3]\n\n        # Fill covariance (flattened 6x6 matrix)\n        cov_matrix = np.zeros(36)\n        cov_matrix[0] = self.position_covariance[0, 0]  # xx\n        cov_matrix[7] = self.position_covariance[1, 1]  # yy\n        cov_matrix[14] = self.position_covariance[2, 2]  # zz\n        cov_matrix[21] = self.orientation_covariance[0, 0]  # rr\n        cov_matrix[28] = self.orientation_covariance[1, 1]  # pp\n        cov_matrix[35] = self.orientation_covariance[2, 2]  # yy\n\n        pose_msg.pose.covariance = cov_matrix.tolist()\n\n        self.fused_pose_pub.publish(pose_msg)\n\n        # Publish fused twist\n        twist_msg = TwistStamped()\n        twist_msg.header.stamp = self.get_clock().now().to_msg()\n        twist_msg.header.frame_id = \'base_link\'\n\n        twist_msg.twist.linear.x = self.velocity[0]\n        twist_msg.twist.linear.y = self.velocity[1]\n        twist_msg.twist.linear.z = self.velocity[2]\n\n        twist_msg.twist.angular.x = self.angular_velocity[0]\n        twist_msg.twist.angular.y = self.angular_velocity[1]\n        twist_msg.twist.angular.z = self.angular_velocity[2]\n\n        self.fused_twist_pub.publish(twist_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    fusion_node = IsaacSensorFusion()\n\n    try:\n        rclpy.spin(fusion_node)\n    except KeyboardInterrupt:\n        fusion_node.get_logger().info(\'Shutting down Isaac Sensor Fusion...\')\n    finally:\n        fusion_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"examples",children:"Examples"}),"\n",(0,s.jsx)(n.h3,{id:"example-1-isaac-ros-visual-slam-integration",children:"Example 1: Isaac ROS Visual SLAM Integration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# visual_slam_integration.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom geometry_msgs.msg import PoseStamped\nfrom nav_msgs.msg import Odometry\nfrom visualization_msgs.msg import Marker, MarkerArray\nimport numpy as np\nimport cv2\nfrom cv_bridge import CvBridge\nimport tf2_ros\n\nclass IsaacVisualSLAMIntegration(Node):\n    """\n    Integrate Isaac ROS Visual SLAM with the humanoid robot\n    """\n\n    def __init__(self):\n        super().__init__(\'isaac_visual_slam_integration\')\n\n        # Publishers\n        self.map_pub = self.create_publisher(MarkerArray, \'/humanoid/vslam/map\', 10)\n        self.pose_pub = self.create_publisher(PoseStamped, \'/humanoid/vslam/pose\', 10)\n        self.odom_pub = self.create_publisher(Odometry, \'/humanoid/vslam/odom\', 10)\n\n        # Subscribers\n        self.left_image_sub = self.create_subscription(\n            Image,\n            \'/humanoid/stereo/left/image_rect_color\',\n            self.left_image_callback,\n            10\n        )\n\n        self.right_image_sub = self.create_subscription(\n            Image,\n            \'/humanoid/stereo/right/image_rect_color\',\n            self.right_image_callback,\n            10\n        )\n\n        self.left_info_sub = self.create_subscription(\n            CameraInfo,\n            \'/humanoid/stereo/left/camera_info\',\n            self.camera_info_callback,\n            10\n        )\n\n        # CV Bridge\n        self.cv_bridge = CvBridge()\n\n        # SLAM state\n        self.left_image = None\n        self.right_image = None\n        self.camera_info = None\n        self.feature_points = []  # 3D points in the map\n        self.robot_trajectory = []  # Robot path\n\n        # Timers\n        self.processing_timer = self.create_timer(0.1, self.process_slam_step)  # 10 Hz\n        self.mapping_timer = self.create_timer(1.0, self.update_map_visualization)\n\n        # TF broadcaster\n        self.tf_broadcaster = tf2_ros.TransformBroadcaster(self)\n\n        self.get_logger().info(\'Isaac Visual SLAM Integration initialized\')\n\n    def left_image_callback(self, msg):\n        """Process left stereo camera image"""\n        try:\n            self.left_image = self.cv_bridge.imgmsg_to_cv2(msg, \'rgb8\')\n        except Exception as e:\n            self.get_logger().error(f\'Error processing left image: {e}\')\n\n    def right_image_callback(self, msg):\n        """Process right stereo camera image"""\n        try:\n            self.right_image = self.cv_bridge.imgmsg_to_cv2(msg, \'rgb8\')\n        except Exception as e:\n            self.get_logger().error(f\'Error processing right image: {e}\')\n\n    def camera_info_callback(self, msg):\n        """Process camera calibration information"""\n        self.camera_info = msg\n\n    def process_slam_step(self):\n        """Process one step of visual SLAM"""\n        if self.left_image is None or self.right_image is None or self.camera_info is None:\n            return\n\n        try:\n            # Step 1: Feature extraction\n            features_left = self.extract_features(self.left_image)\n            features_right = self.extract_features(self.right_image)\n\n            # Step 2: Stereo matching to get 3D points\n            points_3d = self.stereo_match(features_left, features_right)\n\n            # Step 3: Track features across frames\n            tracked_features = self.track_features(features_left)\n\n            # Step 4: Estimate motion using 3D-2D correspondences\n            motion_estimate = self.estimate_motion(tracked_features)\n\n            # Step 5: Update map and robot pose\n            self.update_map(points_3d)\n            self.update_robot_pose(motion_estimate)\n\n        except Exception as e:\n            self.get_logger().error(f\'Error in SLAM processing: {e}\')\n\n    def extract_features(self, image):\n        """Extract features from image (simplified implementation)"""\n        # In a real Isaac ROS implementation, this would use hardware-accelerated feature extraction\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n\n        # Use ORB for feature detection (would be SIFT/SURF in Isaac ROS)\n        orb = cv2.ORB_create(nfeatures=500)\n        keypoints, descriptors = orb.detectAndCompute(gray, None)\n\n        if keypoints is not None:\n            features = [{\'pt\': kp.pt, \'angle\': kp.angle, \'response\': kp.response} for kp in keypoints]\n        else:\n            features = []\n\n        return features\n\n    def stereo_match(self, left_features, right_features):\n        """Match features between stereo images to get 3D points"""\n        if len(left_features) < 2 or len(right_features) < 2:\n            return []\n\n        # Simple descriptor matching\n        left_pts = np.array([f[\'pt\'] for f in left_features])\n        right_pts = np.array([f[\'pt\'] for f in right_features])\n\n        # In a real implementation, we\'d use the camera calibration to triangulate\n        # For this example, we\'ll simulate 3D points based on disparity\n        points_3d = []\n\n        # Simple stereo matching (would be more sophisticated in Isaac ROS)\n        for left_pt in left_pts:\n            # Find corresponding point in right image (simplified)\n            disparities = np.abs(right_pts[:, 0] - left_pt[0])\n            if len(disparities) > 0:\n                closest_idx = np.argmin(disparities)\n                disparity = disparities[closest_idx]\n\n                if disparity > 0:  # Valid match\n                    # Simple triangulation (would use actual camera params in real implementation)\n                    z = 0.1 / disparity if disparity > 0.01 else 10.0  # Depth estimate\n                    x = left_pt[0] * z / 500  # Rough conversion to 3D\n                    y = left_pt[1] * z / 500  # Rough conversion to 3D\n\n                    points_3d.append([x, y, z])\n\n        return points_3d\n\n    def track_features(self, current_features):\n        """Track features across frames"""\n        # In a real implementation, this would use optical flow or feature tracking\n        # to maintain correspondences between frames\n        return current_features\n\n    def estimate_motion(self, features):\n        """Estimate camera/robot motion using features"""\n        # This would use algorithms like PnP or iterative closest point\n        # to estimate motion based on tracked features\n        # For this example, we\'ll return a simple motion estimate\n        return {\n            \'translation\': [0.01, 0.00, 0.00],  # Small forward motion\n            \'rotation\': [0.0, 0.0, 0.0, 1.0]   # No rotation\n        }\n\n    def update_map(self, points_3d):\n        """Update the map with new 3D points"""\n        for pt in points_3d:\n            # Add point to map if it\'s not already there (simplified)\n            self.feature_points.append(pt)\n\n        # Keep only recent points to manage memory\n        if len(self.feature_points) > 10000:\n            self.feature_points = self.feature_points[-5000:]\n\n    def update_robot_pose(self, motion_estimate):\n        """Update robot pose based on motion estimate"""\n        # Integrate motion to get new pose\n        translation = motion_estimate[\'translation\']\n        rotation = motion_estimate[\'rotation\']\n\n        # Update pose (simplified integration)\n        if self.robot_trajectory:\n            last_pose = self.robot_trajectory[-1]\n            new_pose = [\n                last_pose[0] + translation[0],\n                last_pose[1] + translation[1],\n                last_pose[2] + translation[2]\n            ]\n        else:\n            new_pose = [0.0, 0.0, 0.0]\n\n        self.robot_trajectory.append(new_pose)\n\n        # Publish pose estimate\n        pose_msg = PoseStamped()\n        pose_msg.header.stamp = self.get_clock().now().to_msg()\n        pose_msg.header.frame_id = \'map\'\n        pose_msg.pose.position.x = new_pose[0]\n        pose_msg.pose.position.y = new_pose[1]\n        pose_msg.pose.position.z = new_pose[2]\n        pose_msg.pose.orientation.x = rotation[0]\n        pose_msg.pose.orientation.y = rotation[1]\n        pose_msg.pose.orientation.z = rotation[2]\n        pose_msg.pose.orientation.w = rotation[3]\n\n        self.pose_pub.publish(pose_msg)\n\n        # Publish odometry\n        odom_msg = Odometry()\n        odom_msg.header.stamp = pose_msg.header.stamp\n        odom_msg.header.frame_id = \'map\'\n        odom_msg.child_frame_id = \'base_link\'\n        odom_msg.pose.pose = pose_msg.pose\n\n        self.odom_pub.publish(odom_msg)\n\n    def update_map_visualization(self):\n        """Update map visualization markers"""\n        marker_array = MarkerArray()\n\n        # Create markers for feature points\n        for i, point in enumerate(self.feature_points[-100:]):  # Show last 100 points\n            marker = Marker()\n            marker.header.stamp = self.get_clock().now().to_msg()\n            marker.header.frame_id = \'map\'\n            marker.ns = \'vslam_map\'\n            marker.id = i\n            marker.type = Marker.SPHERE\n            marker.action = Marker.ADD\n\n            marker.pose.position.x = point[0]\n            marker.pose.position.y = point[1]\n            marker.pose.position.z = point[2]\n            marker.pose.orientation.w = 1.0\n\n            marker.scale.x = 0.05  # 5cm spheres\n            marker.scale.y = 0.05\n            marker.scale.z = 0.05\n\n            marker.color.r = 0.0  # Blue points\n            marker.color.g = 0.0\n            marker.color.b = 1.0\n            marker.color.a = 0.8\n\n            marker_array.markers.append(marker)\n\n        # Create markers for robot trajectory\n        for i, pose in enumerate(self.robot_trajectory[-50:]):  # Show last 50 poses\n            marker = Marker()\n            marker.header.stamp = self.get_clock().now().to_msg()\n            marker.header.frame_id = \'map\'\n            marker.ns = \'robot_trajectory\'\n            marker.id = i + 1000  # Offset ID to avoid conflict\n            marker.type = Marker.CUBE\n            marker.action = Marker.ADD\n\n            marker.pose.position.x = pose[0]\n            marker.pose.position.y = pose[1]\n            marker.pose.position.z = pose[2]\n            marker.pose.orientation.w = 1.0\n\n            marker.scale.x = 0.1  # 10cm cubes\n            marker.scale.y = 0.1\n            marker.scale.z = 0.1\n\n            marker.color.r = 1.0  # Red trajectory\n            marker.color.g = 0.0\n            marker.color.b = 0.0\n            marker.color.a = 0.8\n\n            marker_array.markers.append(marker)\n\n        self.map_pub.publish(marker_array)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    slam_node = IsaacVisualSLAMIntegration()\n\n    try:\n        rclpy.spin(slam_node)\n    except KeyboardInterrupt:\n        slam_node.get_logger().info(\'Shutting down Isaac Visual SLAM Integration...\')\n    finally:\n        slam_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"example-2-deep-learning-perception-pipeline",children:"Example 2: Deep Learning Perception Pipeline"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# dl_perception_pipeline.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom std_msgs.msg import String\nfrom vision_msgs.msg import Detection2DArray, ObjectHypothesisWithPose\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport torch\nimport torchvision.transforms as transforms\nfrom PIL import Image as PILImage\n\nclass IsaacDLPerceptionPipeline(Node):\n    """\n    Deep learning-based perception pipeline using Isaac ROS\n    """\n\n    def __init__(self):\n        super().__init__(\'isaac_dl_perception_pipeline\')\n\n        # Publishers\n        self.detections_pub = self.create_publisher(Detection2DArray, \'/humanoid/perception/detections\', 10)\n        self.classification_pub = self.create_publisher(String, \'/humanoid/perception/classification\', 10)\n\n        # Subscribers\n        self.image_sub = self.create_subscription(\n            Image,\n            \'/humanoid/camera/rgb/image_raw\',\n            self.image_callback,\n            10\n        )\n\n        # CV Bridge\n        self.cv_bridge = CvBridge()\n\n        # Load pre-trained model (simulated - in real implementation would use Isaac ROS DNN nodes)\n        self.model = self.load_perception_model()\n        self.transform = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n        # Timers\n        self.processing_timer = self.create_timer(0.1, self.process_detections)  # 10 Hz\n\n        self.get_logger().info(\'Isaac Deep Learning Perception Pipeline initialized\')\n\n    def load_perception_model(self):\n        """Load perception model (simulated for this example)"""\n        # In a real Isaac ROS implementation, this would load a TensorRT-optimized model\n        # or use Isaac ROS DNN inference nodes\n        # For this example, we\'ll simulate the model\n        class MockModel(torch.nn.Module):\n            def forward(self, x):\n                # Simulate object detection output\n                batch_size = x.shape[0]\n                # Return mock detections: [batch, num_detections, 6] where 6 = [x, y, w, h, conf, class]\n                detections = torch.tensor([[[100, 100, 50, 50, 0.9, 1],  # [x, y, width, height, confidence, class]\n                                          [200, 150, 30, 30, 0.8, 2],\n                                          [0, 0, 0, 0, 0, 0]]], dtype=torch.float32).repeat(batch_size, 1, 1)\n                return detections\n\n        return MockModel()\n\n    def image_callback(self, msg):\n        """Process incoming image for deep learning inference"""\n        try:\n            # Convert ROS image to OpenCV format\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, \'rgb8\')\n\n            # Store for processing\n            self.current_image = cv_image\n            self.current_image_header = msg.header\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing image: {e}\')\n\n    def process_detections(self):\n        """Process current image with deep learning model"""\n        if not hasattr(self, \'current_image\'):\n            return\n\n        try:\n            # Convert OpenCV image to PIL for processing\n            pil_image = PILImage.fromarray(cv2.cvtColor(self.current_image, cv2.COLOR_BGR2RGB))\n\n            # Apply transforms\n            input_tensor = self.transform(pil_image).unsqueeze(0)  # Add batch dimension\n\n            # Run inference\n            with torch.no_grad():\n                detections = self.model(input_tensor)\n\n            # Process detections\n            self.publish_detections(detections[0], self.current_image_header)\n\n        except Exception as e:\n            self.get_logger().error(f\'Error in DL processing: {e}\')\n\n    def publish_detections(self, detections, header):\n        """Publish detection results"""\n        detection_array = Detection2DArray()\n        detection_array.header = header\n\n        # Process each detection\n        for det in detections:\n            x, y, w, h, conf, class_id = det\n\n            # Skip invalid detections\n            if w == 0 and h == 0:\n                continue\n\n            if float(conf) < 0.5:  # Confidence threshold\n                continue\n\n            detection = Detection2D()\n            detection.header = header\n\n            # Set bounding box\n            detection.bbox.center.x = float(x + w/2)\n            detection.bbox.center.y = float(y + h/2)\n            detection.bbox.size_x = float(w)\n            detection.bbox.size_y = float(h)\n\n            # Set classification\n            hypothesis = ObjectHypothesisWithPose()\n            hypothesis.id = int(float(class_id))\n            hypothesis.score = float(conf)\n\n            # Map class IDs to names (simplified)\n            class_names = {1: "person", 2: "chair", 3: "table", 4: "robot"}\n            class_name = class_names.get(int(float(class_id)), f"unknown_{int(float(class_id))}")\n\n            # Create classification result\n            classification_msg = String()\n            classification_msg.data = f"Detected {class_name} with confidence {float(conf):.2f} at position ({float(x+w/2):.1f}, {float(y+h/2):.1f})"\n            self.classification_pub.publish(classification_msg)\n\n            detection.results.append(hypothesis)\n            detection_array.detections.append(detection)\n\n        self.detections_pub.publish(detection_array)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    perception_node = IsaacDLPerceptionPipeline()\n\n    try:\n        rclpy.spin(perception_node)\n    except KeyboardInterrupt:\n        perception_node.get_logger().info(\'Shutting down Isaac DL Perception Pipeline...\')\n    finally:\n        perception_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"Perception pipelines form the sensory foundation of AI-powered humanoid robots. Isaac ROS provides GPU-accelerated perception capabilities that enable real-time processing of multiple sensor modalities. The key components include:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multi-Sensor Integration"}),": Combining data from cameras, LiDAR, IMU, and other sensors"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Deep Learning Inference"}),": Running neural networks for object detection, segmentation, and classification"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"State Estimation"}),": Using filtering and fusion algorithms to estimate robot state"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Environment Mapping"}),": Creating representations of the robot's surroundings"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-Time Performance"}),": Leveraging GPU acceleration for low-latency processing"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,s.jsx)(n.h3,{id:"conceptual",children:"Conceptual"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Explain the advantages of using Isaac ROS perception pipelines over traditional CPU-based computer vision approaches for humanoid robotics."}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"logical",children:"Logical"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Analyze the trade-offs between perception accuracy and computational performance in real-time robotics applications. When would you prioritize one over the other?"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"implementation-1",children:"Implementation"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Implement a complete perception pipeline that integrates stereo vision, LiDAR, and IMU data for robust environment understanding in your humanoid robot simulation."}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(p,{...e})}):p(e)}},8453(e,n,t){t.d(n,{R:()=>r,x:()=>a});var i=t(6540);const s={},o=i.createContext(s);function r(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);