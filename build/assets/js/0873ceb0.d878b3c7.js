"use strict";(globalThis.webpackChunkphysical_ai_book_humanoid=globalThis.webpackChunkphysical_ai_book_humanoid||[]).push([[9051],{6844(e,n,t){t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>p,frontMatter:()=>i,metadata:()=>a,toc:()=>l});const a=JSON.parse('{"id":"module-4-vla/chapter-4-ros2-actions","title":"ROS 2 Actions for VLA Systems","description":"Implementing ROS 2 action servers and clients for complex task execution in Vision-Language-Action systems","source":"@site/docs/module-4-vla/chapter-4-ros2-actions.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/chapter-4-ros2-actions","permalink":"/textbook/docs/module-4-vla/chapter-4-ros2-actions","draft":false,"unlisted":false,"editUrl":"https://github.com/Panaversity/physical_ai_book_humanoid/tree/main/docs/module-4-vla/chapter-4-ros2-actions.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"title":"ROS 2 Actions for VLA Systems","sidebar_position":5,"description":"Implementing ROS 2 action servers and clients for complex task execution in Vision-Language-Action systems","keywords":["ROS 2","actions","task execution","VLA","humanoid robotics","action servers","action clients"]},"sidebar":"tutorialSidebar","previous":{"title":"LLM-Based Task Planning for Robotics","permalink":"/textbook/docs/module-4-vla/chapter-3-llm-planning"},"next":{"title":"Multimodal Perception for Vision-Language-Action Systems","permalink":"/textbook/docs/module-4-vla/chapter-5-multimodal-perception"}}');var s=t(4848),o=t(8453);const i={title:"ROS 2 Actions for VLA Systems",sidebar_position:5,description:"Implementing ROS 2 action servers and clients for complex task execution in Vision-Language-Action systems",keywords:["ROS 2","actions","task execution","VLA","humanoid robotics","action servers","action clients"]},r="Chapter 4: ROS 2 Actions for VLA Systems",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Core Concepts",id:"core-concepts",level:2},{value:"Action Architecture",id:"action-architecture",level:3},{value:"Action Patterns in VLA Systems",id:"action-patterns-in-vla-systems",level:3},{value:"Implementation",id:"implementation",level:2},{value:"Custom Action Message Definitions",id:"custom-action-message-definitions",level:3},{value:"Action Server Implementation",id:"action-server-implementation",level:3},{value:"Action Client Implementation",id:"action-client-implementation",level:3},{value:"Examples",id:"examples",level:2},{value:"Example 1: VLA Task Coordination System",id:"example-1-vla-task-coordination-system",level:3},{value:"Example 2: Integration with LLM Planning System",id:"example-2-integration-with-llm-planning-system",level:3},{value:"Summary",id:"summary",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Conceptual",id:"conceptual",level:3},{value:"Logical",id:"logical",level:3},{value:"Implementation",id:"implementation-1",level:3}];function _(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"chapter-4-ros-2-actions-for-vla-systems",children:"Chapter 4: ROS 2 Actions for VLA Systems"})}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"By the end of this chapter, students will be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Implement ROS 2 action servers for complex robot task execution"}),"\n",(0,s.jsx)(n.li,{children:"Create action clients that can interact with VLA system components"}),"\n",(0,s.jsx)(n.li,{children:"Design custom action messages for Vision-Language-Action workflows"}),"\n",(0,s.jsx)(n.li,{children:"Handle action feedback, goals, and results in complex robotic tasks"}),"\n",(0,s.jsx)(n.li,{children:"Integrate ROS 2 actions with LLM planning and perception systems"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsx)(n.p,{children:"Students should have:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understanding of ROS 2 fundamentals (covered in Module 1)"}),"\n",(0,s.jsx)(n.li,{children:"Knowledge of ROS 2 services and topics (covered in Module 1)"}),"\n",(0,s.jsx)(n.li,{children:"Familiarity with action-based task execution concepts"}),"\n",(0,s.jsx)(n.li,{children:"Experience with Python programming for ROS 2"}),"\n",(0,s.jsx)(n.li,{children:"Basic understanding of task planning (covered in Chapter 3)"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"core-concepts",children:"Core Concepts"}),"\n",(0,s.jsx)(n.p,{children:"ROS 2 actions provide a way to execute long-running tasks with feedback and status updates. Unlike services, which are synchronous, or topics, which are asynchronous, actions combine both approaches to handle complex tasks that may take significant time to complete."}),"\n",(0,s.jsx)(n.h3,{id:"action-architecture",children:"Action Architecture"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Action Components:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Goal"}),": Request to initiate a long-running task with parameters"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Feedback"}),": Continuous updates during task execution"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Result"}),": Final outcome of the completed task"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Status"}),": Current state of the action (active, succeeded, cancelled, aborted)"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Action Messages:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action Goal"}),": Defines the input parameters for the action"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action Result"}),": Defines the output parameters upon completion"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action Feedback"}),": Defines the ongoing status updates during execution"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"action-patterns-in-vla-systems",children:"Action Patterns in VLA Systems"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Navigation Actions"}),": Moving to specific locations with obstacle avoidance\n",(0,s.jsx)(n.strong,{children:"Manipulation Actions"}),": Grasping, lifting, and placing objects\n",(0,s.jsx)(n.strong,{children:"Perception Actions"}),": Object detection and scene understanding\n",(0,s.jsx)(n.strong,{children:"Composite Actions"}),": Multi-step tasks combining navigation and manipulation"]}),"\n",(0,s.jsx)(n.h2,{id:"implementation",children:"Implementation"}),"\n",(0,s.jsx)(n.p,{children:"Let's implement ROS 2 actions for Vision-Language-Action systems:"}),"\n",(0,s.jsx)(n.h3,{id:"custom-action-message-definitions",children:"Custom Action Message Definitions"}),"\n",(0,s.jsx)(n.p,{children:"First, let's define custom action messages for VLA tasks:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# In your ROS 2 package, create the action directory and files:\nmkdir -p action/{Navigation,Manipulation,Perception,Composite}\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Navigation.action:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"# Goal: Navigate to a target location\ngeometry_msgs/PoseStamped target_pose\nfloat32 max_speed\nbool avoid_obstacles\n\n---\n# Result: Navigation outcome\nbool success\nstring message\nfloat32 execution_time\ngeometry_msgs/PoseStamped final_pose\n\n---\n# Feedback: Current navigation status\nstring status\nfloat32 progress_percentage\ngeometry_msgs/PoseStamped current_pose\nfloat32 distance_remaining\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Manipulation.action:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'# Goal: Manipulate an object\nstring object_name\ngeometry_msgs/PoseStamped target_pose\nstring operation  # "pick", "place", "grasp", "release"\nfloat32 grip_force\n\n---\n# Result: Manipulation outcome\nbool success\nstring message\nfloat32 execution_time\ngeometry_msgs/PoseStamped final_pose\n\n---\n# Feedback: Current manipulation status\nstring status\nfloat32 progress_percentage\nstring current_operation\ngeometry_msgs/PoseStamped current_pose\nbool object_detected\nbool object_grasped\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Perception.action:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'# Goal: Perform perception task\nstring task_type  # "detect_objects", "recognize_scene", "track_object"\nstring[] target_objects\ngeometry_msgs/Point search_center\nfloat32 search_radius\n\n---\n# Result: Perception outcome\nbool success\nstring message\nfloat32 execution_time\nobject_recognition_msgs/RecognizedObjectArray objects\ngeometry_msgs/PoseArray poses\n\n---\n# Feedback: Current perception status\nstring status\nfloat32 progress_percentage\nint32 objects_detected\ngeometry_msgs/PoseArray candidate_poses\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Composite.action:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"# Goal: Execute composite task\nVLAAction[] sub_tasks  # Array of sub-actions to execute\nbool continue_on_failure\n\n---\n# Result: Composite task outcome\nbool success\nstring message\nfloat32 execution_time\nActionResult[] sub_results\n\n---\n# Feedback: Current composite status\nstring status\nfloat32 progress_percentage\nint32 current_task_index\nstring current_task_description\nbool[] task_completed\n"})}),"\n",(0,s.jsx)(n.h3,{id:"action-server-implementation",children:"Action Server Implementation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# vla_action_servers.py\n\nimport rclpy\nfrom rclpy.action import ActionServer, CancelResponse, GoalResponse\nfrom rclpy.node import Node\nfrom rclpy.callback_groups import ReentrantCallbackGroup\nfrom rclpy.executors import MultiThreadedExecutor\n\n# Import custom action messages (assuming they\'re in your package)\nfrom your_vla_package.action import Navigation, Manipulation, Perception, Composite\nfrom geometry_msgs.msg import PoseStamped, Pose, Point\nfrom std_msgs.msg import Header\nimport time\nimport threading\nfrom typing import Optional\nimport math\n\nclass NavigationActionServer(Node):\n    """\n    Action server for navigation tasks in VLA systems\n    """\n\n    def __init__(self):\n        super().__init__(\'vla_navigation_server\')\n\n        # Create action server\n        self._action_server = ActionServer(\n            self,\n            Navigation,\n            \'navigate_to_pose\',\n            execute_callback=self.execute_callback,\n            callback_group=ReentrantCallbackGroup(),\n            goal_callback=self.goal_callback,\n            cancel_callback=self.cancel_callback\n        )\n\n        # Publishers and subscribers for navigation\n        self.nav_status_pub = self.create_publisher(\n            Navigation.Feedback, \'navigation_status\', 10\n        )\n\n        self.get_logger().info(\'Navigation action server initialized\')\n\n    def destroy_node(self):\n        self._action_server.destroy()\n        super().destroy_node()\n\n    def goal_callback(self, goal_request):\n        """Accept or reject a goal"""\n        self.get_logger().info(f\'Received navigation goal: {goal_request.target_pose}\')\n\n        # Validate goal\n        if self._validate_navigation_goal(goal_request):\n            return GoalResponse.ACCEPT\n        else:\n            return GoalResponse.REJECT\n\n    def cancel_callback(self, goal_handle):\n        """Accept or reject a cancellation request"""\n        self.get_logger().info(\'Received cancel request\')\n        return CancelResponse.ACCEPT\n\n    def _validate_navigation_goal(self, goal_request) -> bool:\n        """Validate the navigation goal"""\n        # Check if target pose is valid\n        target = goal_request.target_pose.pose\n        if math.isnan(target.position.x) or math.isnan(target.position.y):\n            return False\n\n        # Check if target is within workspace bounds\n        workspace_bounds = {\n            \'x\': [-2.0, 2.0],\n            \'y\': [-2.0, 2.0],\n            \'z\': [0.0, 1.0]\n        }\n\n        if (not workspace_bounds[\'x\'][0] <= target.position.x <= workspace_bounds[\'x\'][1] or\n            not workspace_bounds[\'y\'][0] <= target.position.y <= workspace_bounds[\'y\'][1] or\n            not workspace_bounds[\'z\'][0] <= target.position.z <= workspace_bounds[\'z\'][1]):\n            return False\n\n        return True\n\n    async def execute_callback(self, goal_handle):\n        """Execute the navigation goal"""\n        self.get_logger().info(\'Executing navigation goal\')\n\n        feedback_msg = Navigation.Feedback()\n        result = Navigation.Result()\n\n        target_pose = goal_handle.request.target_pose\n        max_speed = goal_handle.request.max_speed\n        avoid_obstacles = goal_handle.request.avoid_obstacles\n\n        # Initialize feedback\n        feedback_msg.status = "Initializing navigation"\n        feedback_msg.progress_percentage = 0.0\n        feedback_msg.current_pose = self._get_current_pose()\n        feedback_msg.distance_remaining = self._calculate_distance(\n            feedback_msg.current_pose, target_pose\n        )\n\n        start_time = time.time()\n\n        try:\n            # Simulate navigation execution\n            current_pose = feedback_msg.current_pose\n            total_distance = feedback_msg.distance_remaining\n            traveled_distance = 0.0\n\n            while traveled_distance < total_distance and not goal_handle.is_cancel_requested:\n                # Update current pose (simulated movement)\n                current_pose = self._update_current_pose(current_pose, target_pose, max_speed)\n\n                # Calculate progress\n                distance_remaining = self._calculate_distance(current_pose, target_pose)\n                traveled_distance = total_distance - distance_remaining\n                progress = min(100.0, (traveled_distance / total_distance) * 100.0) if total_distance > 0 else 100.0\n\n                # Update feedback\n                feedback_msg.status = "Navigating to target"\n                feedback_msg.progress_percentage = progress\n                feedback_msg.current_pose = current_pose\n                feedback_msg.distance_remaining = distance_remaining\n\n                # Publish feedback\n                goal_handle.publish_feedback(feedback_msg)\n\n                # Sleep to simulate real navigation\n                time.sleep(0.1)\n\n                # Check for obstacles if avoidance is enabled\n                if avoid_obstacles and self._detect_obstacle(current_pose, target_pose):\n                    feedback_msg.status = "Avoiding obstacle"\n                    # Simulate obstacle avoidance maneuver\n                    time.sleep(0.5)\n\n            if goal_handle.is_cancel_requested:\n                goal_handle.canceled()\n                result.success = False\n                result.message = "Goal canceled"\n                return result\n\n            # Check if we reached the target\n            final_distance = self._calculate_distance(current_pose, target_pose)\n            if final_distance < 0.1:  # 10cm tolerance\n                goal_handle.succeed()\n                result.success = True\n                result.message = "Successfully reached target pose"\n            else:\n                goal_handle.abort()\n                result.success = False\n                result.message = f"Failed to reach target, distance: {final_distance:.2f}m"\n\n            result.execution_time = time.time() - start_time\n            result.final_pose = current_pose\n\n        except Exception as e:\n            goal_handle.abort()\n            result.success = False\n            result.message = f"Execution failed: {str(e)}"\n\n        return result\n\n    def _get_current_pose(self) -> PoseStamped:\n        """Get current robot pose (simulated)"""\n        pose = PoseStamped()\n        pose.header = Header()\n        pose.header.stamp = self.get_clock().now().to_msg()\n        pose.header.frame_id = "map"\n        pose.pose.position.x = 0.0\n        pose.pose.position.y = 0.0\n        pose.pose.position.z = 0.0\n        pose.pose.orientation.w = 1.0\n        return pose\n\n    def _calculate_distance(self, pose1: PoseStamped, pose2: PoseStamped) -> float:\n        """Calculate Euclidean distance between two poses"""\n        dx = pose2.pose.position.x - pose1.pose.position.x\n        dy = pose2.pose.position.y - pose1.pose.position.y\n        dz = pose2.pose.position.z - pose1.pose.position.z\n        return math.sqrt(dx*dx + dy*dy + dz*dz)\n\n    def _update_current_pose(self, current_pose: PoseStamped, target_pose: PoseStamped, speed: float) -> PoseStamped:\n        """Update current pose based on movement toward target"""\n        # Calculate direction vector\n        dx = target_pose.pose.position.x - current_pose.pose.position.x\n        dy = target_pose.pose.position.y - current_pose.pose.position.y\n        dz = target_pose.pose.position.z - current_pose.pose.position.z\n        distance = math.sqrt(dx*dx + dy*dy + dz*dz)\n\n        if distance > 0.01:  # Avoid division by zero\n            # Move towards target at specified speed\n            scale = min(speed * 0.1 / distance, 1.0)  # 0.1s time step\n            new_pose = PoseStamped()\n            new_pose.header = current_pose.header\n            new_pose.pose.position.x = current_pose.pose.position.x + dx * scale\n            new_pose.pose.position.y = current_pose.pose.position.y + dy * scale\n            new_pose.pose.position.z = current_pose.pose.position.z + dz * scale\n            new_pose.pose.orientation = current_pose.pose.orientation\n            return new_pose\n\n        return current_pose\n\n    def _detect_obstacle(self, current_pose: PoseStamped, target_pose: PoseStamped) -> bool:\n        """Detect obstacles in the path (simulated)"""\n        # Simple simulation: 20% chance of obstacle detection\n        import random\n        return random.random() < 0.2\n\nclass ManipulationActionServer(Node):\n    """\n    Action server for manipulation tasks in VLA systems\n    """\n\n    def __init__(self):\n        super().__init__(\'vla_manipulation_server\')\n\n        # Create action server\n        self._action_server = ActionServer(\n            self,\n            Manipulation,\n            \'manipulate_object\',\n            execute_callback=self.execute_callback,\n            callback_group=ReentrantCallbackGroup(),\n            goal_callback=self.goal_callback,\n            cancel_callback=self.cancel_callback\n        )\n\n        self.get_logger().info(\'Manipulation action server initialized\')\n\n    def destroy_node(self):\n        self._action_server.destroy()\n        super().destroy_node()\n\n    def goal_callback(self, goal_request):\n        """Accept or reject a goal"""\n        self.get_logger().info(f\'Received manipulation goal: {goal_request.operation}\')\n        return GoalResponse.ACCEPT\n\n    def cancel_callback(self, goal_handle):\n        """Accept or reject a cancellation request"""\n        self.get_logger().info(\'Received cancel request\')\n        return CancelResponse.ACCEPT\n\n    async def execute_callback(self, goal_handle):\n        """Execute the manipulation goal"""\n        self.get_logger().info(f\'Executing {goal_handle.request.operation} operation\')\n\n        feedback_msg = Manipulation.Feedback()\n        result = Manipulation.Result()\n\n        object_name = goal_handle.request.object_name\n        target_pose = goal_handle.request.target_pose\n        operation = goal_handle.request.operation\n        grip_force = goal_handle.request.grip_force\n\n        # Initialize feedback\n        feedback_msg.status = f"Starting {operation} operation"\n        feedback_msg.progress_percentage = 0.0\n        feedback_msg.current_operation = operation\n        feedback_msg.current_pose = target_pose\n        feedback_msg.object_detected = True  # Simulated\n        feedback_msg.object_grasped = False\n\n        start_time = time.time()\n\n        try:\n            if operation == "pick":\n                success = await self._execute_pick_operation(\n                    goal_handle, feedback_msg, object_name, target_pose, grip_force\n                )\n            elif operation == "place":\n                success = await self._execute_place_operation(\n                    goal_handle, feedback_msg, target_pose\n                )\n            elif operation == "grasp":\n                success = await self._execute_grasp_operation(\n                    goal_handle, feedback_msg, target_pose, grip_force\n                )\n            elif operation == "release":\n                success = await self._execute_release_operation(\n                    goal_handle, feedback_msg\n                )\n            else:\n                success = False\n                result.message = f"Unknown operation: {operation}"\n\n            if success:\n                goal_handle.succeed()\n                result.success = True\n                result.message = f"Successfully completed {operation} operation"\n            else:\n                goal_handle.abort()\n                result.success = False\n                result.message = f"Failed to complete {operation} operation"\n\n            result.execution_time = time.time() - start_time\n            result.final_pose = target_pose\n\n        except Exception as e:\n            goal_handle.abort()\n            result.success = False\n            result.message = f"Execution failed: {str(e)}"\n\n        return result\n\n    async def _execute_pick_operation(self, goal_handle, feedback_msg, object_name, target_pose, grip_force):\n        """Execute pick operation"""\n        # Approach object\n        feedback_msg.status = f"Approaching {object_name}"\n        feedback_msg.progress_percentage = 25.0\n        feedback_msg.current_operation = "approach"\n        goal_handle.publish_feedback(feedback_msg)\n        await self._simulate_action_duration(1.0)\n\n        # Grasp object\n        feedback_msg.status = f"Grasping {object_name}"\n        feedback_msg.progress_percentage = 50.0\n        feedback_msg.current_operation = "grasp"\n        feedback_msg.object_grasped = True\n        goal_handle.publish_feedback(feedback_msg)\n        await self._simulate_action_duration(1.0)\n\n        # Lift object\n        feedback_msg.status = f"Lifting {object_name}"\n        feedback_msg.progress_percentage = 75.0\n        feedback_msg.current_operation = "lift"\n        goal_handle.publish_feedback(feedback_msg)\n        await self._simulate_action_duration(0.5)\n\n        # Verify grasp\n        feedback_msg.status = f"Verifying grasp of {object_name}"\n        feedback_msg.progress_percentage = 100.0\n        feedback_msg.current_operation = "verify"\n        goal_handle.publish_feedback(feedback_msg)\n        await self._simulate_action_duration(0.5)\n\n        return True\n\n    async def _execute_place_operation(self, goal_handle, feedback_msg, target_pose):\n        """Execute place operation"""\n        # Approach target location\n        feedback_msg.status = "Approaching placement location"\n        feedback_msg.progress_percentage = 33.0\n        feedback_msg.current_operation = "approach"\n        goal_handle.publish_feedback(feedback_msg)\n        await self._simulate_action_duration(1.0)\n\n        # Lower object\n        feedback_msg.status = "Lowering object to placement location"\n        feedback_msg.progress_percentage = 66.0\n        feedback_msg.current_operation = "lower"\n        goal_handle.publish_feedback(feedback_msg)\n        await self._simulate_action_duration(0.5)\n\n        # Release object\n        feedback_msg.status = "Releasing object"\n        feedback_msg.progress_percentage = 100.0\n        feedback_msg.current_operation = "release"\n        feedback_msg.object_grasped = False\n        goal_handle.publish_feedback(feedback_msg)\n        await self._simulate_action_duration(0.5)\n\n        return True\n\n    async def _execute_grasp_operation(self, goal_handle, feedback_msg, target_pose, grip_force):\n        """Execute grasp operation"""\n        feedback_msg.status = "Moving to grasp position"\n        feedback_msg.progress_percentage = 50.0\n        feedback_msg.current_operation = "position"\n        goal_handle.publish_feedback(feedback_msg)\n        await self._simulate_action_duration(1.0)\n\n        feedback_msg.status = "Closing gripper"\n        feedback_msg.progress_percentage = 100.0\n        feedback_msg.current_operation = "grasp"\n        feedback_msg.object_grasped = True\n        goal_handle.publish_feedback(feedback_msg)\n        await self._simulate_action_duration(0.5)\n\n        return True\n\n    async def _execute_release_operation(self, goal_handle, feedback_msg):\n        """Execute release operation"""\n        feedback_msg.status = "Opening gripper"\n        feedback_msg.progress_percentage = 100.0\n        feedback_msg.current_operation = "release"\n        feedback_msg.object_grasped = False\n        goal_handle.publish_feedback(feedback_msg)\n        await self._simulate_action_duration(0.5)\n\n        return True\n\n    async def _simulate_action_duration(self, duration: float):\n        """Simulate action execution time"""\n        start = time.time()\n        while time.time() - start < duration:\n            await asyncio.sleep(0.01)\n\nclass PerceptionActionServer(Node):\n    """\n    Action server for perception tasks in VLA systems\n    """\n\n    def __init__(self):\n        super().__init__(\'vla_perception_server\')\n\n        # Create action server\n        self._action_server = ActionServer(\n            self,\n            Perception,\n            \'perform_perception\',\n            execute_callback=self.execute_callback,\n            callback_group=ReentrantCallbackGroup(),\n            goal_callback=self.goal_callback,\n            cancel_callback=self.cancel_callback\n        )\n\n        self.get_logger().info(\'Perception action server initialized\')\n\n    def destroy_node(self):\n        self._action_server.destroy()\n        super().destroy_node()\n\n    def goal_callback(self, goal_request):\n        """Accept or reject a goal"""\n        self.get_logger().info(f\'Received perception goal: {goal_request.task_type}\')\n        return GoalResponse.ACCEPT\n\n    def cancel_callback(self, goal_handle):\n        """Accept or reject a cancellation request"""\n        self.get_logger().info(\'Received cancel request\')\n        return CancelResponse.ACCEPT\n\n    async def execute_callback(self, goal_handle):\n        """Execute the perception goal"""\n        self.get_logger().info(\'Executing perception goal\')\n\n        feedback_msg = Perception.Feedback()\n        result = Perception.Result()\n\n        task_type = goal_handle.request.task_type\n        target_objects = goal_handle.request.target_objects\n        search_center = goal_request.search_center\n        search_radius = goal_request.search_radius\n\n        # Initialize feedback\n        feedback_msg.status = f"Starting {task_type} task"\n        feedback_msg.progress_percentage = 0.0\n        feedback_msg.objects_detected = 0\n\n        start_time = time.time()\n\n        try:\n            if task_type == "detect_objects":\n                result = await self._execute_detect_objects(\n                    goal_handle, feedback_msg, target_objects, search_center, search_radius\n                )\n            elif task_type == "recognize_scene":\n                result = await self._execute_recognize_scene(\n                    goal_handle, feedback_msg\n                )\n            elif task_type == "track_object":\n                result = await self._execute_track_object(\n                    goal_handle, feedback_msg, target_objects[0] if target_objects else None\n                )\n            else:\n                goal_handle.abort()\n                result.success = False\n                result.message = f"Unknown task type: {task_type}"\n                return result\n\n            if result.success:\n                goal_handle.succeed()\n            else:\n                goal_handle.abort()\n\n            result.execution_time = time.time() - start_time\n\n        except Exception as e:\n            goal_handle.abort()\n            result.success = False\n            result.message = f"Execution failed: {str(e)}"\n\n        return result\n\n    async def _execute_detect_objects(self, goal_handle, feedback_msg, target_objects, search_center, search_radius):\n        """Execute object detection task"""\n        import random\n\n        # Simulate scanning process\n        for i in range(10):  # Simulate 10 scan steps\n            feedback_msg.status = f"Scanning environment ({i+1}/10)"\n            feedback_msg.progress_percentage = (i + 1) * 10.0\n            feedback_msg.objects_detected = random.randint(0, len(target_objects) + 2)\n\n            goal_handle.publish_feedback(feedback_msg)\n            await self._simulate_action_duration(0.2)\n\n        # Create mock detection results\n        result = Perception.Result()\n        result.success = True\n        result.message = f"Detected {feedback_msg.objects_detected} objects"\n\n        # Create mock recognized objects\n        # In a real implementation, this would populate with actual detection data\n        result.objects = []  # Would be populated with recognized objects\n        result.poses = []    # Would be populated with object poses\n\n        return result\n\n    async def _execute_recognize_scene(self, goal_handle, feedback_msg):\n        """Execute scene recognition task"""\n        # Simulate scene analysis\n        for i in range(5):\n            feedback_msg.status = f"Analyzing scene - Step {i+1}/5"\n            feedback_msg.progress_percentage = (i + 1) * 20.0\n            feedback_msg.objects_detected = i + 1  # Simulated increasing detection\n\n            goal_handle.publish_feedback(feedback_msg)\n            await self._simulate_action_duration(0.5)\n\n        result = Perception.Result()\n        result.success = True\n        result.message = "Scene analysis completed"\n        result.objects = []  # Would be populated with scene understanding\n        result.poses = []    # Would be populated with spatial relationships\n\n        return result\n\n    async def _execute_track_object(self, goal_handle, feedback_msg, target_object):\n        """Execute object tracking task"""\n        if not target_object:\n            result = Perception.Result()\n            result.success = False\n            result.message = "No target object specified for tracking"\n            return result\n\n        # Simulate continuous tracking\n        for i in range(20):  # Simulate 20 tracking steps\n            feedback_msg.status = f"Tracking {target_object} - Frame {i+1}/20"\n            feedback_msg.progress_percentage = min(100.0, (i + 1) * 5.0)  # Up to 100%\n            feedback_msg.objects_detected = 1  # One object being tracked\n\n            goal_handle.publish_feedback(feedback_msg)\n            await self._simulate_action_duration(0.1)\n\n        result = Perception.Result()\n        result.success = True\n        result.message = f"Successfully tracked {target_object} for 2 seconds"\n        result.objects = []  # Would contain tracking results\n        result.poses = []    # Would contain trajectory information\n\n        return result\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    # Create nodes\n    nav_server = NavigationActionServer()\n    manip_server = ManipulationActionServer()\n    percep_server = PerceptionActionServer()\n\n    # Use MultiThreadedExecutor to handle multiple action servers\n    executor = MultiThreadedExecutor()\n    executor.add_node(nav_server)\n    executor.add_node(manip_server)\n    executor.add_node(percep_server)\n\n    try:\n        print("VLA Action Servers starting...")\n        executor.spin()\n    except KeyboardInterrupt:\n        print("Shutting down VLA Action Servers...")\n    finally:\n        nav_server.destroy_node()\n        manip_server.destroy_node()\n        percep_server.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"action-client-implementation",children:"Action Client Implementation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# vla_action_clients.py\n\nimport rclpy\nfrom rclpy.action import ActionClient\nfrom rclpy.node import Node\nfrom rclpy.duration import Duration\n\n# Import custom action messages\nfrom your_vla_package.action import Navigation, Manipulation, Perception, Composite\nfrom geometry_msgs.msg import PoseStamped, Pose, Point\nfrom std_msgs.msg import Header\nimport time\nimport asyncio\nfrom typing import Optional, List, Dict, Any\n\nclass VLAActionClient(Node):\n    """\n    Client for interacting with VLA action servers\n    """\n\n    def __init__(self):\n        super().__init__(\'vla_action_client\')\n\n        # Create action clients\n        self.nav_client = ActionClient(self, Navigation, \'navigate_to_pose\')\n        self.manip_client = ActionClient(self, Manipulation, \'manipulate_object\')\n        self.percep_client = ActionClient(self, Perception, \'perform_perception\')\n\n        # Wait for action servers to be available\n        self.nav_client.wait_for_server(timeout_sec=5.0)\n        self.manip_client.wait_for_server(timeout_sec=5.0)\n        self.percep_client.wait_for_server(timeout_sec=5.0)\n\n        self.get_logger().info(\'VLA Action Client initialized\')\n\n    def create_navigation_goal(self, target_pose: PoseStamped, max_speed: float = 0.5,\n                              avoid_obstacles: bool = True) -> Navigation.Goal:\n        """Create a navigation goal"""\n        goal = Navigation.Goal()\n        goal.target_pose = target_pose\n        goal.max_speed = max_speed\n        goal.avoid_obstacles = avoid_obstacles\n        return goal\n\n    def create_manipulation_goal(self, object_name: str, target_pose: PoseStamped,\n                               operation: str, grip_force: float = 50.0) -> Manipulation.Goal:\n        """Create a manipulation goal"""\n        goal = Manipulation.Goal()\n        goal.object_name = object_name\n        goal.target_pose = target_pose\n        goal.operation = operation\n        goal.grip_force = grip_force\n        return goal\n\n    def create_perception_goal(self, task_type: str, target_objects: List[str] = None,\n                             search_center: Point = None, search_radius: float = 1.0) -> Perception.Goal:\n        """Create a perception goal"""\n        goal = Perception.Goal()\n        goal.task_type = task_type\n        goal.target_objects = target_objects or []\n        goal.search_center = search_center or Point(x=0.0, y=0.0, z=0.0)\n        goal.search_radius = search_radius\n        return goal\n\n    async def navigate_to_pose(self, target_pose: PoseStamped, max_speed: float = 0.5,\n                              avoid_obstacles: bool = True,\n                              timeout: float = 30.0) -> Optional[Navigation.Result]:\n        """Send navigation goal and wait for result"""\n        goal = self.create_navigation_goal(target_pose, max_speed, avoid_obstacles)\n\n        # Send goal\n        goal_future = await self.nav_client.send_goal_async(goal)\n\n        if not goal_future.accepted:\n            self.get_logger().error(\'Navigation goal rejected\')\n            return None\n\n        self.get_logger().info(\'Navigation goal accepted, waiting for result...\')\n\n        # Get result\n        result_future = goal_future.get_result_async()\n\n        # Wait for result with timeout\n        try:\n            result_response = await asyncio.wait_for(\n                result_future,\n                timeout=timeout\n            )\n            return result_response.result\n        except asyncio.TimeoutError:\n            self.get_logger().error(f\'Navigation goal timed out after {timeout}s\')\n            # Cancel the goal\n            cancel_future = goal_future.cancel_goal_async()\n            try:\n                await cancel_future\n            except:\n                pass\n            return None\n\n    async def manipulate_object(self, object_name: str, target_pose: PoseStamped,\n                               operation: str, grip_force: float = 50.0,\n                               timeout: float = 20.0) -> Optional[Manipulation.Result]:\n        """Send manipulation goal and wait for result"""\n        goal = self.create_manipulation_goal(object_name, target_pose, operation, grip_force)\n\n        # Send goal\n        goal_future = await self.manip_client.send_goal_async(goal)\n\n        if not goal_future.accepted:\n            self.get_logger().error(\'Manipulation goal rejected\')\n            return None\n\n        self.get_logger().info(\'Manipulation goal accepted, waiting for result...\')\n\n        # Get result\n        result_future = goal_future.get_result_async()\n\n        # Wait for result with timeout\n        try:\n            result_response = await asyncio.wait_for(\n                result_future,\n                timeout=timeout\n            )\n            return result_response.result\n        except asyncio.TimeoutError:\n            self.get_logger().error(f\'Manipulation goal timed out after {timeout}s\')\n            # Cancel the goal\n            cancel_future = goal_future.cancel_goal_async()\n            try:\n                await cancel_future\n            except:\n                pass\n            return None\n\n    async def perform_perception(self, task_type: str, target_objects: List[str] = None,\n                                search_center: Point = None, search_radius: float = 1.0,\n                                timeout: float = 15.0) -> Optional[Perception.Result]:\n        """Send perception goal and wait for result"""\n        goal = self.create_perception_goal(task_type, target_objects, search_center, search_radius)\n\n        # Send goal\n        goal_future = await self.percep_client.send_goal_async(goal)\n\n        if not goal_future.accepted:\n            self.get_logger().error(\'Perception goal rejected\')\n            return None\n\n        self.get_logger().info(\'Perception goal accepted, waiting for result...\')\n\n        # Get result\n        result_future = goal_future.get_result_async()\n\n        # Wait for result with timeout\n        try:\n            result_response = await asyncio.wait_for(\n                result_future,\n                timeout=timeout\n            )\n            return result_response.result\n        except asyncio.TimeoutError:\n            self.get_logger().error(f\'Perception goal timed out after {timeout}s\')\n            # Cancel the goal\n            cancel_future = goal_future.cancel_goal_async()\n            try:\n                await cancel_future\n            except:\n                pass\n            return None\n\n    def subscribe_to_navigation_feedback(self, feedback_callback):\n        """Subscribe to navigation feedback"""\n        return self.nav_client._send_goal_future.add_feedback_callback(feedback_callback)\n\n    def subscribe_to_manipulation_feedback(self, feedback_callback):\n        """Subscribe to manipulation feedback"""\n        return self.manip_client._send_goal_future.add_feedback_callback(feedback_callback)\n\n    def subscribe_to_perception_feedback(self, feedback_callback):\n        """Subscribe to perception feedback"""\n        return self.percep_client._send_goal_future.add_feedback_callback(feedback_callback)\n\nclass VLACompositeActionServer(Node):\n    """\n    Action server for composite tasks that coordinate multiple actions\n    """\n\n    def __init__(self):\n        super().__init__(\'vla_composite_server\')\n\n        # Create action server\n        self._action_server = ActionServer(\n            self,\n            Composite,\n            \'execute_composite_task\',\n            execute_callback=self.execute_callback,\n            callback_group=ReentrantCallbackGroup(),\n            goal_callback=self.goal_callback,\n            cancel_callback=self.cancel_callback\n        )\n\n        # Create action clients for coordinating sub-actions\n        self.nav_client = ActionClient(self, Navigation, \'navigate_to_pose\')\n        self.manip_client = ActionClient(self, Manipulation, \'manipulate_object\')\n        self.percep_client = ActionClient(self, Perception, \'perform_perception\')\n\n        # Wait for servers\n        self.nav_client.wait_for_server(timeout_sec=5.0)\n        self.manip_client.wait_for_server(timeout_sec=5.0)\n        self.percep_client.wait_for_server(timeout_sec=5.0)\n\n        self.get_logger().info(\'Composite action server initialized\')\n\n    def destroy_node(self):\n        self._action_server.destroy()\n        super().destroy_node()\n\n    def goal_callback(self, goal_request):\n        """Accept or reject a goal"""\n        self.get_logger().info(f\'Received composite task with {len(goal_request.sub_tasks)} sub-tasks\')\n        return GoalResponse.ACCEPT\n\n    def cancel_callback(self, goal_handle):\n        """Accept or reject a cancellation request"""\n        self.get_logger().info(\'Received cancel request for composite task\')\n        return CancelResponse.ACCEPT\n\n    async def execute_callback(self, goal_handle):\n        """Execute the composite goal"""\n        self.get_logger().info(\'Executing composite task\')\n\n        feedback_msg = Composite.Feedback()\n        result = Composite.Result()\n\n        sub_tasks = goal_handle.request.sub_tasks\n        continue_on_failure = goal_handle.request.continue_on_failure\n\n        # Initialize feedback\n        feedback_msg.status = "Initializing composite task"\n        feedback_msg.progress_percentage = 0.0\n        feedback_msg.current_task_index = 0\n        feedback_msg.current_task_description = "Starting composite execution"\n        feedback_msg.task_completed = [False] * len(sub_tasks)\n\n        start_time = time.time()\n\n        try:\n            results = []\n\n            for i, sub_task in enumerate(sub_tasks):\n                if goal_handle.is_cancel_requested:\n                    goal_handle.canceled()\n                    result.success = False\n                    result.message = "Composite task canceled"\n                    result.execution_time = time.time() - start_time\n                    return result\n\n                # Update feedback for current task\n                feedback_msg.current_task_index = i\n                feedback_msg.current_task_description = f"Executing task {i+1}/{len(sub_tasks)}"\n                feedback_msg.progress_percentage = (i / len(sub_tasks)) * 100.0\n\n                goal_handle.publish_feedback(feedback_msg)\n\n                # Execute the sub-task based on its type\n                sub_result = await self._execute_sub_task(sub_task, goal_handle)\n                results.append(sub_result)\n\n                # Mark task as completed\n                feedback_msg.task_completed[i] = True\n                feedback_msg.status = f"Completed task {i+1}/{len(sub_tasks)}"\n                feedback_msg.progress_percentage = ((i + 1) / len(sub_tasks)) * 100.0\n\n                goal_handle.publish_feedback(feedback_msg)\n\n                # Check if task failed and whether to continue\n                if not sub_result.success and not continue_on_failure:\n                    goal_handle.abort()\n                    result.success = False\n                    result.message = f"Sub-task {i} failed: {sub_result.message}"\n                    result.execution_time = time.time() - start_time\n                    result.sub_results = results\n                    return result\n\n            # All tasks completed successfully\n            goal_handle.succeed()\n            result.success = True\n            result.message = f"All {len(sub_tasks)} sub-tasks completed successfully"\n            result.execution_time = time.time() - start_time\n            result.sub_results = results\n\n        except Exception as e:\n            goal_handle.abort()\n            result.success = False\n            result.message = f"Composite execution failed: {str(e)}"\n            result.execution_time = time.time() - start_time\n\n        return result\n\n    async def _execute_sub_task(self, sub_task, goal_handle):\n        """Execute a single sub-task"""\n        # This is a simplified version - in practice, you\'d have more sophisticated task routing\n        if sub_task.type == "navigation":\n            # Create and send navigation goal\n            nav_goal = Navigation.Goal()\n            nav_goal.target_pose = sub_task.target_pose\n            nav_goal.max_speed = getattr(sub_task, \'max_speed\', 0.5)\n            nav_goal.avoid_obstacles = getattr(sub_task, \'avoid_obstacles\', True)\n\n            goal_future = await self.nav_client.send_goal_async(nav_goal)\n            if goal_future.accepted:\n                result_future = goal_future.get_result_async()\n                try:\n                    result_response = await result_future\n                    return result_response.result\n                except:\n                    # Return failure result\n                    nav_result = Navigation.Result()\n                    nav_result.success = False\n                    nav_result.message = "Navigation sub-task failed"\n                    return nav_result\n            else:\n                nav_result = Navigation.Result()\n                nav_result.success = False\n                nav_result.message = "Navigation goal rejected"\n                return nav_result\n\n        elif sub_task.type == "manipulation":\n            # Similar pattern for manipulation\n            manip_goal = Manipulation.Goal()\n            manip_goal.object_name = getattr(sub_task, \'object_name\', \'\')\n            manip_goal.target_pose = getattr(sub_task, \'target_pose\', PoseStamped())\n            manip_goal.operation = getattr(sub_task, \'operation\', \'grasp\')\n            manip_goal.grip_force = getattr(sub_task, \'grip_force\', 50.0)\n\n            goal_future = await self.manip_client.send_goal_async(manip_goal)\n            if goal_future.accepted:\n                result_future = goal_future.get_result_async()\n                try:\n                    result_response = await result_future\n                    return result_response.result\n                except:\n                    manip_result = Manipulation.Result()\n                    manip_result.success = False\n                    manip_result.message = "Manipulation sub-task failed"\n                    return manip_result\n            else:\n                manip_result = Manipulation.Result()\n                manip_result.success = False\n                manip_result.message = "Manipulation goal rejected"\n                return manip_result\n\n        else:\n            # Return failure for unknown task type\n            result = type(\'GenericResult\', (), {\'success\': False, \'message\': f\'Unknown task type: {sub_task.type}\'})()\n            return result\n\ndef create_vla_action_client() -> VLAActionClient:\n    """Factory function to create a VLA action client"""\n    return VLAActionClient()\n\ndef main():\n    """Main function for VLA action client example"""\n    rclpy.init()\n\n    client = VLAActionClient()\n\n    # Example usage\n    print("VLA Action Client initialized. Ready to send action requests.")\n\n    # Example: Create a target pose\n    target_pose = PoseStamped()\n    target_pose.header = Header()\n    target_pose.header.frame_id = "map"\n    target_pose.pose.position.x = 1.0\n    target_pose.pose.position.y = 1.0\n    target_pose.pose.position.z = 0.0\n    target_pose.pose.orientation.w = 1.0\n\n    print("Ready to send action requests. In a real implementation, this would connect to action servers.")\n\n    client.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"examples",children:"Examples"}),"\n",(0,s.jsx)(n.h3,{id:"example-1-vla-task-coordination-system",children:"Example 1: VLA Task Coordination System"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n# vla_task_coordination.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.action import ActionClient\nfrom rclpy.qos import QoSProfile\nimport asyncio\nfrom typing import Dict, Any, List, Optional\n\nclass VLATaskCoordinationSystem(Node):\n    \"\"\"\n    System that coordinates Vision-Language-Action tasks using ROS 2 actions\n    \"\"\"\n\n    def __init__(self):\n        super().__init__('vla_task_coordinator')\n\n        # Create action clients for all VLA components\n        self.action_client = VLAActionClient()\n\n        # Task queue for managing multiple requests\n        self.task_queue = asyncio.Queue()\n        self.is_running = False\n\n        self.get_logger().info('VLA Task Coordination System initialized')\n\n    async def process_vla_command(self, command: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"\n        Process a VLA command that may involve navigation, manipulation, and perception\n        \"\"\"\n        command_type = command.get('type', 'unknown')\n        target_object = command.get('target_object', 'unknown')\n        target_location = command.get('target_location', None)\n        action = command.get('action', 'unknown')\n\n        results = {}\n\n        try:\n            if command_type == 'navigation':\n                # Navigate to target location\n                if target_location:\n                    pose_stamped = self._dict_to_pose_stamped(target_location)\n                    nav_result = await self.action_client.navigate_to_pose(pose_stamped)\n                    results['navigation'] = nav_result\n\n            elif command_type == 'manipulation':\n                # Perform manipulation task\n                if target_object and target_location:\n                    pose_stamped = self._dict_to_pose_stamped(target_location)\n                    manip_result = await self.action_client.manipulate_object(\n                        target_object, pose_stamped, action\n                    )\n                    results['manipulation'] = manip_result\n                else:\n                    # First detect the object\n                    percep_result = await self.action_client.perform_perception(\n                        'detect_objects', [target_object]\n                    )\n                    results['perception'] = percep_result\n\n                    if percep_result and percep_result.success:\n                        # Navigate to object\n                        if percep_result.poses:\n                            nav_result = await self.action_client.navigate_to_pose(\n                                percep_result.poses[0]\n                            )\n                            results['navigation'] = nav_result\n\n                            # Perform manipulation\n                            manip_result = await self.action_client.manipulate_object(\n                                target_object, percep_result.poses[0], action\n                            )\n                            results['manipulation'] = manip_result\n\n            elif command_type == 'composite':\n                # Execute a sequence of actions\n                tasks = command.get('tasks', [])\n                for task in tasks:\n                    task_result = await self._execute_single_task(task)\n                    results[f\"task_{task.get('type', 'unknown')}\"] = task_result\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing VLA command: {e}')\n            return {'success': False, 'error': str(e)}\n\n        return {\n            'success': all(res.success for res in results.values() if hasattr(res, 'success')),\n            'results': results,\n            'command': command\n        }\n\n    def _dict_to_pose_stamped(self, pose_dict: Dict[str, float]) -> PoseStamped:\n        \"\"\"Convert dictionary to PoseStamped message\"\"\"\n        pose = PoseStamped()\n        pose.header = Header()\n        pose.header.stamp = self.get_clock().now().to_msg()\n        pose.header.frame_id = pose_dict.get('frame_id', 'map')\n        pose.pose.position.x = pose_dict.get('x', 0.0)\n        pose.pose.position.y = pose_dict.get('y', 0.0)\n        pose.pose.position.z = pose_dict.get('z', 0.0)\n\n        # Default orientation (identity quaternion)\n        pose.pose.orientation.w = 1.0\n        return pose\n\n    async def _execute_single_task(self, task: Dict[str, Any]) -> Any:\n        \"\"\"Execute a single task based on its type\"\"\"\n        task_type = task.get('type', 'unknown')\n\n        if task_type == 'navigation':\n            target_pose = self._dict_to_pose_stamped(task.get('target_pose', {}))\n            return await self.action_client.navigate_to_pose(target_pose)\n        elif task_type == 'manipulation':\n            target_pose = self._dict_to_pose_stamped(task.get('target_pose', {}))\n            return await self.action_client.manipulate_object(\n                task.get('object_name', 'unknown'),\n                target_pose,\n                task.get('operation', 'grasp')\n            )\n        elif task_type == 'perception':\n            return await self.action_client.perform_perception(\n                task.get('task_type', 'detect_objects'),\n                task.get('target_objects', [])\n            )\n        else:\n            # Return a generic failure result\n            result = type('GenericResult', (), {'success': False, 'message': f'Unknown task type: {task_type}'})()\n            return result\n\n    async def run_task_scheduler(self):\n        \"\"\"Run the task scheduler to process queued tasks\"\"\"\n        self.is_running = True\n\n        while self.is_running:\n            try:\n                # Get next task from queue\n                task = await self.task_queue.get()\n\n                if task is None:  # Sentinel to stop the scheduler\n                    break\n\n                # Process the task\n                result = await self.process_vla_command(task)\n\n                # Log the result\n                self.get_logger().info(f'Task completed: {result[\"success\"]}')\n\n                # Mark task as done\n                self.task_queue.task_done()\n\n            except Exception as e:\n                self.get_logger().error(f'Error in task scheduler: {e}')\n                continue\n\n    def add_task(self, command: Dict[str, Any]):\n        \"\"\"Add a task to the queue\"\"\"\n        self.task_queue.put_nowait(command)\n\n    def stop_scheduler(self):\n        \"\"\"Stop the task scheduler\"\"\"\n        self.is_running = False\n        # Add sentinel to stop the scheduler\n        self.task_queue.put_nowait(None)\n\nasync def main():\n    \"\"\"Main function to demonstrate VLA task coordination\"\"\"\n    rclpy.init()\n\n    coordinator = VLATaskCoordinationSystem()\n\n    # Example commands\n    commands = [\n        {\n            'type': 'navigation',\n            'target_location': {'x': 1.0, 'y': 1.0, 'z': 0.0, 'frame_id': 'map'}\n        },\n        {\n            'type': 'manipulation',\n            'target_object': 'red_cube',\n            'action': 'pick',\n            'target_location': {'x': 0.5, 'y': 0.5, 'z': 0.0}\n        },\n        {\n            'type': 'composite',\n            'tasks': [\n                {\n                    'type': 'perception',\n                    'task_type': 'detect_objects',\n                    'target_objects': ['blue_sphere']\n                },\n                {\n                    'type': 'navigation',\n                    'target_pose': {'x': 0.8, 'y': 0.8, 'z': 0.0}\n                },\n                {\n                    'type': 'manipulation',\n                    'object_name': 'blue_sphere',\n                    'operation': 'grasp',\n                    'target_pose': {'x': 0.8, 'y': 0.8, 'z': 0.1}\n                }\n            ]\n        }\n    ]\n\n    # Add tasks to queue\n    for cmd in commands:\n        coordinator.add_task(cmd)\n\n    # Run scheduler for a few seconds\n    import threading\n    scheduler_thread = threading.Thread(target=lambda: rclpy.spin(coordinator))\n    scheduler_thread.start()\n\n    try:\n        await coordinator.run_task_scheduler()\n    except KeyboardInterrupt:\n        print(\"Shutting down VLA Task Coordination System...\")\n    finally:\n        coordinator.stop_scheduler()\n        coordinator.destroy_node()\n        rclpy.shutdown()\n        scheduler_thread.join()\n\nif __name__ == '__main__':\n    asyncio.run(main())\n"})}),"\n",(0,s.jsx)(n.h3,{id:"example-2-integration-with-llm-planning-system",children:"Example 2: Integration with LLM Planning System"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n# vla_llm_integration.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.action import ActionClient\nfrom typing import Dict, Any, List\nimport json\n\nclass VLALLMIntegration(Node):\n    \"\"\"\n    Integration layer between LLM planning and ROS 2 action execution\n    \"\"\"\n\n    def __init__(self):\n        super().__init__('vla_llm_integration')\n\n        # Action clients\n        self.action_client = VLAActionClient()\n\n        # Mapping from LLM actions to ROS 2 actions\n        self.action_mapping = {\n            'navigate_to': self._execute_navigation,\n            'pick_object': self._execute_manipulation,\n            'place_object': self._execute_manipulation,\n            'grasp': self._execute_manipulation,\n            'release': self._execute_manipulation,\n            'inspect': self._execute_perception,\n            'detect_object': self._execute_perception,\n            'approach': self._execute_navigation\n        }\n\n        self.get_logger().info('VLA-LLM Integration initialized')\n\n    async def execute_llm_plan(self, plan: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"Execute a plan generated by an LLM\"\"\"\n        results = []\n        success = True\n        total_time = 0.0\n\n        for i, step in enumerate(plan):\n            action_name = step.get('action', '')\n            parameters = step.get('parameters', {})\n\n            self.get_logger().info(f'Executing step {i+1}/{len(plan)}: {action_name}')\n\n            # Map LLM action to ROS 2 action\n            if action_name in self.action_mapping:\n                try:\n                    start_time = self.get_clock().now().nanoseconds / 1e9\n                    result = await self.action_mapping[action_name](parameters)\n                    execution_time = (self.get_clock().now().nanoseconds / 1e9) - start_time\n\n                    step_result = {\n                        'step': i,\n                        'action': action_name,\n                        'parameters': parameters,\n                        'result': result,\n                        'execution_time': execution_time,\n                        'success': result.success if hasattr(result, 'success') else True\n                    }\n\n                    results.append(step_result)\n                    total_time += execution_time\n\n                    if not step_result['success']:\n                        success = False\n                        self.get_logger().error(f'Step {i} failed: {action_name}')\n\n                except Exception as e:\n                    self.get_logger().error(f'Error executing step {i}: {e}')\n                    step_result = {\n                        'step': i,\n                        'action': action_name,\n                        'parameters': parameters,\n                        'result': None,\n                        'execution_time': 0.0,\n                        'success': False,\n                        'error': str(e)\n                    }\n                    results.append(step_result)\n                    success = False\n            else:\n                self.get_logger().error(f'Unknown action: {action_name}')\n                step_result = {\n                    'step': i,\n                    'action': action_name,\n                    'parameters': parameters,\n                    'result': None,\n                    'execution_time': 0.0,\n                    'success': False,\n                    'error': f'Unknown action: {action_name}'\n                }\n                results.append(step_result)\n                success = False\n\n        return {\n            'success': success,\n            'results': results,\n            'total_execution_time': total_time,\n            'steps_completed': len([r for r in results if r['success']])\n        }\n\n    async def _execute_navigation(self, parameters: Dict[str, Any]):\n        \"\"\"Execute navigation action from LLM plan\"\"\"\n        target_pose_dict = parameters.get('target_pose', parameters.get('location', {}))\n        max_speed = parameters.get('max_speed', 0.5)\n        avoid_obstacles = parameters.get('avoid_obstacles', True)\n\n        target_pose = self._dict_to_pose_stamped(target_pose_dict)\n        return await self.action_client.navigate_to_pose(target_pose, max_speed, avoid_obstacles)\n\n    async def _execute_manipulation(self, parameters: Dict[str, Any]):\n        \"\"\"Execute manipulation action from LLM plan\"\"\"\n        object_name = parameters.get('object', parameters.get('target_object', 'unknown'))\n        target_pose_dict = parameters.get('target_pose', parameters.get('location', {}))\n        operation = parameters.get('operation', parameters.get('action', 'grasp'))\n        grip_force = parameters.get('grip_force', 50.0)\n\n        target_pose = self._dict_to_pose_stamped(target_pose_dict)\n        return await self.action_client.manipulate_object(object_name, target_pose, operation, grip_force)\n\n    async def _execute_perception(self, parameters: Dict[str, Any]):\n        \"\"\"Execute perception action from LLM plan\"\"\"\n        task_type = parameters.get('task_type', parameters.get('action', 'detect_objects'))\n        target_objects = parameters.get('target_objects', parameters.get('objects', []))\n        search_center = parameters.get('search_center', {'x': 0.0, 'y': 0.0, 'z': 0.0})\n        search_radius = parameters.get('search_radius', 1.0)\n\n        center_point = Point()\n        center_point.x = search_center.get('x', 0.0)\n        center_point.y = search_center.get('y', 0.0)\n        center_point.z = search_center.get('z', 0.0)\n\n        return await self.action_client.perform_perception(\n            task_type, target_objects, center_point, search_radius\n        )\n\n    def _dict_to_pose_stamped(self, pose_dict: Dict[str, float]) -> PoseStamped:\n        \"\"\"Convert dictionary to PoseStamped message\"\"\"\n        pose = PoseStamped()\n        pose.header = Header()\n        pose.header.stamp = self.get_clock().now().to_msg()\n        pose.header.frame_id = pose_dict.get('frame_id', 'map')\n        pose.pose.position.x = pose_dict.get('x', 0.0)\n        pose.pose.position.y = pose_dict.get('y', 0.0)\n        pose.pose.position.z = pose_dict.get('z', 0.0)\n\n        # Handle orientation - if not provided, use identity quaternion\n        if 'orientation' in pose_dict:\n            orient = pose_dict['orientation']\n            pose.pose.orientation.x = orient.get('x', 0.0)\n            pose.pose.orientation.y = orient.get('y', 0.0)\n            pose.pose.orientation.z = orient.get('z', 0.0)\n            pose.pose.orientation.w = orient.get('w', 1.0)\n        else:\n            pose.pose.orientation.w = 1.0  # Default to identity\n\n        return pose\n\ndef main():\n    \"\"\"Main function for VLA-LLM integration\"\"\"\n    rclpy.init()\n\n    integration = VLALLMIntegration()\n\n    print(\"VLA-LLM Integration system initialized.\")\n    print(\"This system can execute plans generated by LLMs using ROS 2 actions.\")\n    print(\"In a real implementation, this would connect to both LLM planning and robot action servers.\")\n\n    integration.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"ROS 2 actions provide a powerful framework for executing complex, long-running tasks in Vision-Language-Action systems. Key benefits include:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Asynchronous Execution"}),": Long-running tasks don't block other operations"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Progress Feedback"}),": Continuous updates during task execution"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cancellation Support"}),": Ability to interrupt running tasks"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Result Reporting"}),": Comprehensive outcome information"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Error Handling"}),": Robust error management and recovery"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The integration of actions with LLM planning systems enables high-level task decomposition to be executed reliably on robotic platforms."}),"\n",(0,s.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,s.jsx)(n.h3,{id:"conceptual",children:"Conceptual"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Compare and contrast ROS 2 actions, services, and topics. When would you choose each communication pattern for different VLA system components?"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"logical",children:"Logical"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Design an error handling strategy for a composite action that coordinates navigation, manipulation, and perception. How would your system handle partial failures and ensure safe robot operation?"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"implementation-1",children:"Implementation"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Implement a complete VLA action system with custom action messages, action servers for navigation/manipulation/perception, and a coordinating client that can execute complex tasks involving multiple sequential and parallel actions."}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(_,{...e})}):_(e)}},8453(e,n,t){t.d(n,{R:()=>i,x:()=>r});var a=t(6540);const s={},o=a.createContext(s);function i(e){const n=a.useContext(o);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),a.createElement(o.Provider,{value:n},e.children)}}}]);